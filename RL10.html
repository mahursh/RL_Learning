<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Reinforcement Learning Mastery & Generalization Roadmap</title>
    <style>
        /* Base styles from Deep RL Mastery */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            line-height: 1.6; /* Added for readability */
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header (from Deep RL Mastery) */
        .header {
            text-align: center;
            margin-bottom: 40px;
            background: rgba(255, 255, 255, 0.95);
            padding: 30px;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        .header h1 {
            font-size: 3em;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.2em;
            color: #666;
            margin-bottom: 20px;
        }

        .progress-overview {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }

        .progress-card {
            background: rgba(255, 255, 255, 0.9);
            padding: 20px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .progress-card:hover {
            transform: translateY(-5px);
        }

        .progress-number {
            font-size: 2.5em;
            font-weight: bold;
            color: #667eea;
        }

        /* Tabs (from Deep RL Mastery) */
        .tabs {
            display: flex;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 15px;
            padding: 5px;
            margin-bottom: 30px;
            backdrop-filter: blur(10px);
        }

        .tab {
            flex: 1;
            padding: 15px 20px;
            text-align: center;
            background: transparent;
            border: none;
            color: white;
            font-size: 1.1em;
            font-weight: 600;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .tab.active {
            background: rgba(255, 255, 255, 0.9);
            color: #333;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .content-section {
            display: none;
            animation: fadeIn 0.5s ease;
        }

        .content-section.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Phase Structure (from Deep RL Mastery, adapted for Generalization Roadmap style) */
        .phase {
            background: rgba(255, 255, 255, 0.95); /* Changed to white from previous to match new source */
            border-radius: 15px; /* Changed from 20px */
            margin-bottom: 30px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1); /* Adjusted shadow */
        }

        .phase-header {
            background: linear-gradient(135deg, #4c51bf, #667eea); /* Changed gradient */
            color: white;
            padding: 25px 30px; /* Adjusted padding */
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            flex-direction: column; /* Changed to column for text stack */
            align-items: flex-start; /* Aligned text to start */
        }
        .phase-header h2 { /* Adjust to align with new h2 styles in the phase header */
            font-size: 1.8rem;
            margin-bottom: 5px;
        }
        .phase-header p {
            font-size: 0.9em;
            margin-top: 5px;
            color: rgba(255, 255, 255, 0.8);
        }

        .phase-content {
            padding: 30px; /* Adjusted padding */
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }

        .phase.expanded .phase-content {
            max-height: 2000px; /* Large enough to show all content */
            transition: max-height 0.5s ease-in;
        }

        /* Week Section (from Generalization Roadmap) */
        .week-section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f8fafc;
            border-radius: 12px;
            border-left: 4px solid #38b2ac; /* New color */
            transition: all 0.3s ease; /* Added transition */
        }
        .week-section:hover { /* Added hover effect */
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }

        .week-section h4 {
            color: #2d3748;
            font-size: 1.3rem;
            margin-bottom: 15px;
        }

        .topics {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 15px;
        }

        .topic-tag {
            background: linear-gradient(135deg, #38b2ac, #319795); /* New color */
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
        }

        /* Resources (from Deep RL Mastery, combined with Generalization Roadmap styles) */
        .resources {
            margin-top: 15px;
        }

        .resources h5 {
            color: #4a5568; /* Adjusted color */
            margin-bottom: 10px;
            font-weight: 600; /* Adjusted font weight */
        }

        .resource-links {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }

        .resource-link {
            background: #667eea; /* Kept previous color */
            color: white;
            padding: 8px 15px;
            border-radius: 20px;
            text-decoration: none;
            font-size: 0.9em;
            transition: all 0.3s ease;
            display: inline-flex; /* Added for icon alignment */
            align-items: center; /* Added for icon alignment */
            gap: 8px; /* Spacing for icon */
        }

        .resource-link:hover {
            background: #5a6fd8;
            transform: translateY(-2px);
        }

        /* Quiz Section (from Deep RL Mastery) */
        .quiz-section {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
        }
        .quiz-section h3 { /* Style for quiz section headers */
            font-size: 1.6em;
            color: #667eea;
            margin-bottom: 20px;
            text-align: center;
        }

        .quiz-card {
            background: #f8f9ff;
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 20px;
            border-left: 4px solid #667eea;
        }

        .quiz-question {
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 15px;
            color: #333;
        }

        .quiz-options {
            display: grid;
            gap: 10px;
            margin-bottom: 15px;
        }

        .quiz-option {
            padding: 12px 20px;
            background: white;
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .quiz-option:hover {
            border-color: #667eea;
            background: #f0f4ff;
        }

        .quiz-option.selected {
            border-color: #667eea;
            background: #667eea;
            color: white;
        }

        .quiz-option.correct {
            border-color: #28a745;
            background: #28a745;
            color: white;
        }

        .quiz-option.incorrect {
            border-color: #dc3545;
            background: #dc3545;
            color: white;
        }

        .quiz-feedback {
            padding: 15px;
            border-radius: 10px;
            margin-top: 15px;
            display: none;
        }

        .quiz-feedback.correct {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }

        .quiz-feedback.incorrect {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }

        /* Schedule Section (from Deep RL Mastery) */
        .week-schedule {
            display: grid;
            gap: 20px;
        }

        .week-card {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            border-left: 5px solid #667eea;
        }

        .week-number {
            font-size: 1.2em;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 10px;
        }

        .week-focus {
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 10px;
            color: #333;
        }

        .week-goals {
            color: #666;
            line-height: 1.6;
        }

        .btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 12px 25px;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            font-weight: 600;
            transition: all 0.3s ease;
            margin-top: 15px; /* Added margin for button */
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
        }

        /* New elements from RL Generalization Roadmap */
        .hero { /* Changed from .header to .hero to avoid conflict */
            text-align: center;
            padding: 40px 0;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            margin-bottom: 30px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: white; /* Ensure text is visible on transparent background */
        }

        .hero h1 { /* Overriding header h1 for hero */
            font-size: 3rem;
            font-weight: 800;
            background: linear-gradient(45deg, #fff, #e0e7ff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 10px;
        }

        .hero p { /* Overriding header p for hero */
            color: #e0e7ff;
            font-size: 1.2rem;
            max-width: 600px;
            margin: 0 auto;
        }

        .overview-card { /* Renamed from .overview to prevent CSS conflict */
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .overview-card h2 { /* Targeting h2 within overview-card */
            color: #4c51bf;
            font-size: 2rem;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .competencies-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .competency-card {
            background: linear-gradient(135deg, #f8fafc, #e2e8f0);
            padding: 20px;
            border-radius: 12px;
            border-left: 4px solid #4c51bf;
            transition: all 0.3s ease; /* Added transition */
        }

        .competency-card:hover { /* Added hover */
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(76, 81, 191, 0.15);
        }

        .competency-card h4 {
            color: #2d3748;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }

        .competency-card ul {
            list-style: none;
            color: #4a5568;
        }

        .competency-card li {
            padding: 2px 0;
            position: relative;
            padding-left: 15px;
        }

        .competency-card li:before {
            content: "‚ñ∏";
            position: absolute;
            left: 0;
            color: #4c51bf;
        }

        .tools-section {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .tools-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }

        .tool-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 15px;
            background: linear-gradient(135deg, #f7fafc, #edf2f7);
            border-radius: 10px;
            border-left: 4px solid #ed8936;
            transition: all 0.3s ease; /* Added transition */
        }
        .tool-item:hover { /* Added hover */
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .tool-name {
            font-weight: 600;
            color: #2d3748;
        }

        .tool-desc {
            color: #4a5568;
            font-size: 0.9rem;
        }

        .progress-tracker {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .progress-bar {
            width: 100%;
            height: 20px;
            background: #e2e8f0;
            border-radius: 10px;
            overflow: hidden;
            margin: 20px 0;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #48bb78, #38a169);
            width: 0%;
            transition: width 0.5s ease;
            border-radius: 10px;
        }

        .checklist-items-container { /* New container for checklist items */
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }

        .checklist-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px;
            background: #f8fafc;
            border-radius: 8px;
            cursor: pointer;
            transition: background 0.3s ease;
        }

        .checklist-item:hover {
            background: #e2e8f0;
        }

        .checkbox {
            width: 20px;
            height: 20px;
            border: 2px solid #4c51bf;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: all 0.2s ease; /* Added transition */
        }
        .checkbox:hover { /* Added hover */
            transform: scale(1.1);
            border-color: #667eea;
        }

        .checkbox.checked {
            background: #4c51bf;
            color: white;
        }

        .floating-nav {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(10px);
            border-radius: 12px;
            padding: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
            z-index: 1000;
        }

        .nav-item {
            display: block;
            color: #4c51bf;
            text-decoration: none;
            padding: 8px 12px;
            border-radius: 8px;
            margin-bottom: 5px;
            font-size: 0.9rem;
            transition: background 0.3s ease;
        }

        .nav-item:hover {
            background: rgba(76, 81, 191, 0.1);
        }

        /* Floating Progress (from Deep RL Mastery) - keep for overall page progress */
        .floating-progress {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: rgba(255, 255, 255, 0.95);
            padding: 20px;
            border-radius: 50%;
            width: 80px;
            height: 80px;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
            font-weight: bold;
            color: #667eea;
            font-size: 1.1em;
        }

        /* Study Timer (New feature) */
        .study-timer-container {
            position: fixed;
            bottom: 20px;
            left: 20px;
            background: white;
            padding: 15px;
            border-radius: 12px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            z-index: 1000;
        }
        .study-timer-container .timer-title {
            font-weight: 600;
            color: #4c51bf;
            margin-bottom: 10px;
        }
        .study-timer-container #timerDisplay {
            font-size: 1.2rem;
            font-weight: 600;
            color: #2d3748;
            margin-bottom: 10px;
        }
        .study-timer-container button {
            border: none;
            padding: 8px 16px;
            border-radius: 6px;
            cursor: pointer;
            margin-right: 8px;
            font-weight: 500;
            transition: background 0.3s ease;
        }
        .study-timer-container #timerBtn {
            background: #4c51bf;
            color: white;
        }
        .study-timer-container #timerBtn:hover {
            background: #5a6fd8;
        }
        .study-timer-container #resetBtn {
            background: #e53e3e;
            color: white;
        }
        .study-timer-container #resetBtn:hover {
            background: #c53030;
        }

        /* Subtopic Summary Styles */
        .subtopic-button { /* Styled as a button */
            background: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.9em;
            border: 1px solid #e0e0e0;
            cursor: pointer;
            transition: all 0.2s ease;
            color: #333; /* Default text color */
        }
        .subtopic-button:hover {
            background: #e6e9f0;
            border-color: #667eea;
        }
        .subtopic-button.active-summary { /* Style for clicked button */
            background: #667eea;
            color: white;
            border-color: #667eea;
        }
        .subtopic-summary {
            background: #e6f0ff; /* Light blue background */
            border-left: 3px solid #4c51bf;
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
            font-size: 0.95em;
            color: #2d3748;
            display: none; /* Hidden by default */
            animation: fadeIn 0.3s ease-out; /* Simple fade in */
        }
        .subtopic-summary.active {
            display: block;
        }


        /* Responsive adjustments */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }

            .header h1 {
                font-size: 2em;
            }
            .hero h1 { /* Adjust hero font size for mobile */
                font-size: 2.2rem;
            }

            .tabs {
                flex-direction: column;
            }

            .progress-overview {
                grid-template-columns: 1fr 1fr;
            }

            .floating-progress {
                bottom: 20px;
                right: 20px;
                width: 60px;
                height: 60px;
                font-size: 0.9em;
            }
            .floating-nav { /* Hide floating nav on small screens */
                display: none;
            }
            .competencies-grid, .tools-grid, .checklist-items-container { /* Adjust grid columns for mobile */
                grid-template-columns: 1fr;
            }
            .study-timer-container {
                bottom: 10px;
                left: 10px;
                padding: 10px;
            }
            .study-timer-container #timerDisplay {
                font-size: 1rem;
            }
            .study-timer-container button {
                padding: 6px 12px;
                font-size: 0.8rem;
            }
        }

        /* CSS animations for new elements */
        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .resource-link {
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
        }

        .week-section {
            transition: all 0.3s ease;
        }

        .week-section:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }

        .phase {
            transition: all 0.3s ease;
        }

        .checkbox {
            transition: all 0.2s ease;
        }

        .checkbox:hover {
            transform: scale(1.1);
            border-color: #667eea;
        }

        .competency-card {
            transition: all 0.3s ease;
        }

        .competency-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(76, 81, 191, 0.15);
        }

        .tool-item {
            transition: all 0.3s ease;
        }

        .tool-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body>
<div class="container">
    <div class="hero">
        <h1>üß† RL Generalization Roadmap</h1>
        <p>Your comprehensive 6-month journey to mastering Reinforcement Learning Generalization</p>
    </div>

    <div class="header" style="background:none; box-shadow:none; backdrop-filter:none; padding:0; margin-bottom: 20px;">
        <div class="progress-overview">
            <div class="progress-card">
                <div class="progress-number" id="overall-progress">0%</div>
                <div>Overall Progress</div>
            </div>
            <div class="progress-card">
                <div class="progress-number" id="completed-topics">0</div>
                <div>Topics Completed</div>
            </div>
            <div class="progress-card">
                <div class="progress-number" id="current-week">Week 1</div>
                <div>Current Focus</div>
            </div>
            <div class="progress-card">
                <div class="progress-number" id="study-streak">0</div>
                <div>Study Streak</div>
            </div>
        </div>
    </div>

    <div class="tabs">
        <button class="tab active" onclick="showSection('roadmap')">üìö Roadmap</button>
        <button class="tab" onclick="showSection('overview')">üí° Overview</button>
        <button class="tab" onclick="showSection('quiz')">üß© Quizzes</button>
        <button class="tab" onclick="showSection('schedule')">üìÖ Schedule</button>
        <button class="tab" onclick="showSection('tools')">üîß Tools</button>
        <button class="tab" onclick="showSection('resources')">üîó Resources</button>
        <button class="tab" onclick="showSection('checklist')">üìã Checklist</button>
    </div>

    <div id="overview" class="content-section">
        <div class="overview-card">
            <h2>‚úÖ Overview: Core Competencies</h2>
            <p>These are the fundamental areas you'll master throughout this roadmap.</p>
            <div class="competencies-grid">
                <div class="competency-card">
                    <h4>üìä Mathematics</h4>
                    <ul>
                        <li>Probability & Statistics</li>
                        <li>Linear Algebra</li>
                        <li>Calculus</li>
                        <li>Optimization</li>
                    </ul>
                </div>
                <div class="competency-card">
                    <h4>ü§ñ Machine Learning</h4>
                    <ul>
                        <li>Classical ML algorithms</li>
                        <li>Bias-variance trade-off</li>
                        <li>Overfitting concepts</li>
                    </ul>
                </div>
                <div class="competency-card">
                    <h4>üß† Deep Learning</h4>
                    <ul>
                        <li>Neural Networks</li>
                        <li>CNNs, RNNs, Transformers</li>
                        <li>Representation Learning</li>
                    </ul>
                </div>
                <div class="competency-card">
                    <h4>üéÆ Reinforcement Learning</h4>
                    <ul>
                        <li>MDPs, Bellman equations</li>
                        <li>Value & Policy methods</li>
                        <li>Generalization in RL</li>
                    </ul>
                </div>
                <div class="competency-card">
                    <h4>üî¨ Research Skills</h4>
                    <ul>
                        <li>Paper reading</li>
                        <li>Algorithm implementation</li>
                        <li>Experiment design</li>
                    </ul>
                </div>
                <div class="competency-card">
                    <h4>üíª Software</h4>
                    <ul>
                        <li>Python, PyTorch, JAX</li>
                        <li>RL Environments</li>
                        <li>Experiment tracking</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>


    <div id="roadmap" class="content-section active">
        <div class="progress-tracker">
            <h2>üìà Overall Roadmap Progress</h2>
            <div class="progress-bar">
                <div class="progress-fill" id="roadmapProgressFill"></div>
            </div>
            <p id="roadmapProgressText">0% Complete - Let's get started!</p>
        </div>

        <div class="phase" id="phase1-math">
            <div class="phase-header" onclick="togglePhase(this)">
                <div>
                    <h2>üü¶ Phase 1: Mathematics Foundations</h2>
                    <p>Weeks 1-6 ‚Ä¢ Building the essential mathematical foundation for ML and RL.</p>
                </div>
            </div>
            <div class="phase-content">
                <div class="week-section">
                    <h4>üìÖ Week 1-2: Probability & Statistics</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="distributions">Distributions</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="expectation-variance">Expectation & Variance</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bayes-rule">Bayes Rule</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="kl-divergence">KL Divergence</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://www.khanacademy.org/math/statistics-probability" class="resource-link" target="_blank">üìä Khan Academy</a>
                            <a href="https://greenteapress.com/thinkstats/" class="resource-link" target="_blank">üìñ Think Stats</a>
                            <a href="https://www.youtube.com/playlist?list=PLblh5JKNUPxVjGyf6S3zFh_f4s-QYf6Pj" class="resource-link" target="_blank">üì∫ StatQuest (YouTube)</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 3: Linear Algebra</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="vectors-matrices">Vectors & Matrices</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="eigenvalues">Eigenvalues</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="svd-pca">SVD & PCA</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" class="resource-link" target="_blank">üé• 3Blue1Brown (YouTube)</a>
                            <a href="https://web.stanford.edu/class/cs229/section/cs229-linalg.pdf" class="resource-link" target="_blank">üìÑ Stanford CS229 Notes</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 4: Calculus</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="derivatives">Derivatives</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="chain-rule">Chain Rule</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="partial-derivatives">Partial Derivatives</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="jacobians">Jacobians</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://www.khanacademy.org/math/calculus-1" class="resource-link" target="_blank">üìä Khan Academy Calculus</a>
                            <a href="https://www.coursera.org/learn/machine-learning-linear-algebra" class="resource-link" target="_blank">üéì DeepLearning.ai Linear Algebra (Coursera)</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 5-6: Optimization</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="gradient-descent">Gradient Descent</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="sgd-adam">SGD & Adam</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="convex-optimization">Convex Optimization</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229 Lectures</a>
                            <a href="https://web.stanford.edu/~boyd/cvxbook/" class="resource-link" target="_blank">üìñ Convex Optimization (Boyd & Vandenberghe)</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="phase" id="phase2-ml-dl">
            <div class="phase-header" onclick="togglePhase(this)">
                <div>
                    <h2>üü¶ Phase 2: Machine Learning & Deep Learning</h2>
                    <p>Weeks 7-12 ‚Ä¢ Core ML algorithms and neural network foundations.</p>
                </div>
            </div>
            <div class="phase-content">
                <div class="week-section">
                    <h4>üìÖ Week 7-8: ML Fundamentals</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="supervised-learning">Supervised Learning</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bias-variance">Bias-Variance</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="cross-validation">Cross-validation</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ml-regularization">Regularization</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229</a>
                            <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book" class="resource-link" target="_blank">üìñ Bishop: Pattern Recognition and Machine Learning</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 9-10: Deep Learning Foundations</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="nn-basics">Neural Network Basics</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="backpropagation">Backpropagation</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="cnns-rnns">CNNs & RNNs</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="transformers-intro">Transformers Intro</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://cs231n.github.io/" class="resource-link" target="_blank">üéì CS231n Stanford (CNNs)</a>
                            <a href="https://www.coursera.org/specializations/deep-learning" class="resource-link" target="_blank">üéì DeepLearning.ai Specialization</a>
                            <a href="https://nnfs.io/" class="resource-link" target="_blank">üìñ Neural Networks from Scratch</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 11-12: Representation Learning</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="autoencoders">Autoencoders</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="vaes">Variational Autoencoders (VAEs)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="contrastive-learning">Contrastive Learning</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="self-supervised-learning">Self-supervised Learning</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://arxiv.org/abs/1312.6114" class="resource-link" target="_blank">üìÑ Auto-Encoding Variational Bayes (Paper)</a>
                            <a href="https://www.youtube.com/playlist?list=PLtk_G4NlB8j49yJ-K24Qp2WnI1uFp252P" class="resource-link" target="_blank">üì∫ Yannic Kilcher (YouTube - VAEs/GANs)</a>
                            <a href="https://lilianweng.github.io/posts/2021-01-29-self-supervised-learning/" class="resource-link" target="_blank">üìù Lil'Log Self-Supervised Learning</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="phase" id="phase3-rl">
            <div class="phase-header" onclick="togglePhase(this)">
                <div>
                    <h2>üü¶ Phase 3: Reinforcement Learning</h2>
                    <p>Weeks 13-20 ‚Ä¢ Deep dive into core RL algorithms and generalization concepts.</p>
                </div>
            </div>
            <div class="phase-content">
                <div class="week-section">
                    <h4>üìÖ Week 13-14: Tabular RL & MDPs</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="mdps">Markov Decision Processes (MDPs)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="value-functions">Value Functions ($V^\pi, Q^\pi$)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bellman-equations">Bellman Equations</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="value-iteration">Value Iteration</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-iteration">Policy Iteration</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="http://incompleteideas.net/book/the-book-2nd.html" class="resource-link" target="_blank">üìñ Sutton & Barto (Full Book)</a>
                            <a href="https://www.davidsilver.uk/teaching/" class="resource-link" target="_blank">üé• David Silver's RL Course</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 15-16: Deep RL Foundations (Value-Based)</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="q-learning-deep">Q-learning (deep)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="dqn">Deep Q-Networks (DQN)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="experience-replay">Experience Replay</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="target-networks">Target Networks</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="double-dueling-dqn">Double DQN, Dueling DQN</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://spinningup.openai.com/en/latest/" class="resource-link" target="_blank">üöÄ OpenAI Spinning Up</a>
                            <a href="https://github.com/vwxyzjn/cleanrl" class="resource-link" target="_blank">üíª CleanRL Implementations</a>
                            <a href="https://arxiv.org/abs/1312.5602" class="resource-link" target="_blank">üìÑ DQN Paper</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 17-18: Policy Gradient + Actor-Critic Methods</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-gradients">Policy Gradients (REINFORCE)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="actor-critic">Actor-Critic Methods</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="a2c-a3c">A2C/A3C</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ppo">Proximal Policy Optimization (PPO)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="sac">Soft Actor-Critic (SAC)</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Resources:</h5>
                        <div class="resource-links">
                            <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" class="resource-link" target="_blank">üìù Lil'Log: Policy Gradient Algorithms</a>
                            <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction" class="resource-link" target="_blank">ü§ó HuggingFace Deep RL Course</a>
                            <a href="https://arxiv.org/abs/1707.06347" class="resource-link" target="_blank">üìÑ PPO Paper</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 19-20: Generalization in RL</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="environment-randomization">Environment Randomization (Domain Randomization)</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="procgen-benchmark">Procgen Benchmark</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="crafter-environment">Crafter Environment</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="data-augmentation-rl">Data Augmentation for RL</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-embeddings-transfer">Policy Embeddings / Transfer Learning</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Key Papers & Environments:</h5>
                        <div class="resource-links">
                            <a href="https://arxiv.org/abs/2306.05483" class="resource-link" target="_blank">üìÑ Generalization via Adversarial Regularization (Paper)</a>
                            <a href="https://openai.com/research/procgen" class="resource-link" target="_blank">üéÆ OpenAI Procgen</a>
                            <a href="https://github.com/danijar/crafter" class="resource-link" target="_blank">üéÆ Crafter Environment</a>
                            <a href="https://arxiv.org/abs/1911.05722" class="resource-link" target="_blank">üìÑ CURL: Contrastive Unsupervised Representations for RL (Paper)</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="phase" id="phase4-research">
            <div class="phase-header" onclick="togglePhase(this)">
                <div>
                    <h2>üü¶ Phase 4: Research & Practice</h2>
                    <p>Weeks 21-24 ‚Ä¢ Hands-on research experience and project development.</p>
                </div>
            </div>
            <div class="phase-content">
                <div class="week-section">
                    <h4>üìÖ Week 21-22: Reproduce a Paper</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="paper-reading">Paper Reading & Understanding</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="algorithm-implementation">Algorithm Implementation from Scratch</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ablation-studies">Ablation Studies</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="benchmarking-hyperparameter-tuning">Benchmarking & Hyperparameter Tuning</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Tools & Platforms:</h5>
                        <div class="resource-links">
                            <a href="https://pytorch.org/" class="resource-link" target="_blank">üî• PyTorch</a>
                            <a href="https://jax.readthedocs.io/en/latest/" class="resource-link" target="_blank">üêç JAX</a>
                            <a href="https://hydra.cc/" class="resource-link" target="_blank">‚öôÔ∏è Hydra (Config Management)</a>
                            <a href="https://wandb.ai/" class="resource-link" target="_blank">üìä Weights & Biases (Experiment Tracking)</a>
                            <a href="https://paperswithcode.com/" class="resource-link" target="_blank">üî¨ Papers With Code</a>
                        </div>
                    </div>
                </div>

                <div class="week-section">
                    <h4>üìÖ Week 23-24: Mini Project & Portfolio Building</h4>
                    <div class="topics">
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="define-project-scope">Define Project Scope</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="custom-environment-design">Custom Environment Design</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="generalization-analysis-new-tasks">Generalization Analysis on New Tasks</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-clustering-visualization">Policy Clustering / Visualization of Embeddings</button>
                        <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="communication-results">Effective Communication of Results (Report/Blog/Video)</button>
                    </div>
                    <div class="subtopic-summary"></div>
                    <div class="resources">
                        <h5>üìö Environments & Project Ideas:</h5>
                        <div class="resource-links">
                            <a href="https://gym.openai.com/" class="resource-link" target="_blank">üèãÔ∏è OpenAI Gym (Basic environments)</a>
                            <a href="https://github.com/openai/procgen" class="resource-link" target="_blank">üéÆ Procgen (Procedural Generation)</a>
                            <a href="https://github.com/deepmind/dm_control" class="resource-link" target="_blank">ü§ñ DeepMind Control Suite</a>
                            <a href="https://www.kaggle.com/competitions?sortBy=date&search=reinforcement+learning" class="resource-link" target="_blank">üèÖ Kaggle RL Competitions</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div id="quiz" class="content-section">
        <h2>üß© Knowledge Quizzes</h2>
        <p>Test your understanding with these interactive quizzes, organized by phase!</p>

        <div class="quiz-section">
            <h3>üìê Math Foundations Quizzes</h3>

            <div class="quiz-card">
                <div class="quiz-question">1. (Set Theory) Which of the following best describes the union of two sets A and B?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) Elements common to both A and B.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) All elements that are in A, or in B, or in both.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) Elements in A but not in B.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) Elements not in A and not in B.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">2. (Probability) If P(A) = 0.5, P(B) = 0.4, and A and B are independent events, what is P(A and B)?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) 0.9</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) 0.2</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) 0.1</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) 0.0</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">3. (Linear Algebra) What is the result of multiplying a 2x3 matrix by a 3x2 matrix?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) A 3x3 matrix</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">B) A 2x3 matrix</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">C) A 2x2 matrix</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) A 3x2 matrix</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">4. (Calculus) What is the derivative of $f(x) = x^2 + 3x - 5$?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) $2x - 5$</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">B) $x + 3$</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">C) $2x + 3$</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) $x^3/3 + 3x^2/2 - 5x$</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">5. (Optimization) Gradient Descent aims to find the minimum of a function by iteratively moving in which direction?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) The direction of the steepest ascent</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) The direction opposite to the gradient</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) A random direction</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) The direction parallel to the gradient</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>
        </div>

        <div class="quiz-section">
            <h3>üß† Deep Learning Foundations Quizzes</h3>

            <div class="quiz-card">
                <div class="quiz-question">1. (Neural Network Basics) Which of the following is NOT a common activation function used in neural networks?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) ReLU</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">B) Sigmoid</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) Tanh</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">D) Fourier Transform</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">2. (Backpropagation) What is the primary role of backpropagation in neural network training?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) To perform the forward pass through the network.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) To compute the gradients of the loss function with respect to the network's weights.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) To initialize the weights of the network.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) To select the optimal network architecture.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">3. (Training Techniques) Which technique helps prevent overfitting by randomly setting a fraction of neurons to zero during training?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) Batch Normalization</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) Dropout</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) L1 Regularization</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) Early Stopping</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>
        </div>

        <div class="quiz-section">
            <h3>üéØ Core Reinforcement Learning Quizzes</h3>

            <div class="quiz-card">
                <div class="quiz-question">1. What is the Bellman equation used for in reinforcement learning?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) To calculate the gradient of the policy</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) To express the relationship between value functions at different time steps</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) To define the action space</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) To update neural network weights</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">2. What is the main difference between Q-learning and SARSA?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) Q-learning uses neural networks, SARSA doesn't</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) Q-learning is off-policy, SARSA is on-policy</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) Q-learning is for continuous actions, SARSA for discrete</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) There is no significant difference</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">3. What problem does the Deep Q-Network (DQN) address compared to traditional Q-learning?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) DQN works only with continuous action spaces</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) DQN can handle large state spaces through function approximation</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) DQN eliminates the need for exploration</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) DQN works faster than Q-learning</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">4. In the context of policy gradient methods, what does the gradient ascent update?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) The value function parameters</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) The policy parameters to maximize expected return</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) The environment dynamics</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) The reward function</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">5. What is the purpose of experience replay in DQN?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) To speed up training by using multiple GPUs</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) To break correlation between consecutive samples and improve sample efficiency</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) To generate new experiences from existing ones</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) To visualize the learning process</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>
        </div>

        <div class="quiz-section">
            <h3>üîÑ Representation & Generalization Quizzes</h3>

            <div class="quiz-card">
                <div class="quiz-question">1. (Contrastive Embeddings) What is the main goal of contrastive learning in the context of state representation?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) To learn a policy directly from raw pixels.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) To learn a representation where similar states are close and dissimilar states are far apart.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) To predict the next state given the current state and action.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) To minimize the number of actions required to reach a goal.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">2. (Distributional RL) Unlike traditional RL which estimates expected returns, what does distributional RL estimate?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) The optimal policy directly.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">B) The uncertainty in the environment.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">C) The full distribution of returns.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) The variance of the returns.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">3. (Offline RL) What is the primary characteristic of offline (or batch) reinforcement learning?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) The agent interacts with the environment in real-time.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) The agent learns from a fixed dataset of previously collected transitions without further environment interaction.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) The agent learns only from expert demonstrations.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) The agent is trained using only positive rewards.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>
        </div>

        <div class="quiz-section">
            <h3>üöÄ Projects & Portfolio Quizzes</h3>

            <div class="quiz-card">
                <div class="quiz-question">1. (Implementing Algorithms) When implementing DQN, why is a target network often used?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) To speed up the forward pass.</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) To stabilize the training process by providing a fixed Q-value target for a period.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) To generate more diverse experiences for replay.</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) To handle continuous action spaces.</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>

            <div class="quiz-card">
                <div class="quiz-question">2. (Embedding Visualizations) Which dimensionality reduction technique is commonly used to visualize high-dimensional embeddings in 2D or 3D?</div>
                <div class="quiz-options">
                    <div class="quiz-option" onclick="selectOption(this, false)">A) Principal Component Analysis (PCA)</div>
                    <div class="quiz-option" onclick="selectOption(this, true)">B) t-Distributed Stochastic Neighbor Embedding (t-SNE)</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">C) Linear Discriminant Analysis (LDA)</div>
                    <div class="quiz-option" onclick="selectOption(this, false)">D) Independent Component Analysis (ICA)</div>
                </div>
                <div class="quiz-feedback"></div>
            </div>
        </div>
    </div>

    <div id="schedule" class="content-section">
        <div class="week-schedule">
            <h2>üìÖ 15-Week Learning Schedule (Deep RL Core)</h2>
            <p>This schedule outlines a systematic approach to mastering core Deep RL concepts.</p>

            <div class="week-card">
                <div class="week-number">Week 1</div>
                <div class="week-focus">Set Theory & Logic</div>
                <div class="week-goals">Understand sets, functions, and logical operations. Practice with exercises from Khan Academy and complete basic proofs.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Weeks 2-3</div>
                <div class="week-focus">Probability & Statistics</div>
                <div class="week-goals">Learn distributions, expectation, variance. Complete Khan Academy statistics course and work through Think Stats problems.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Weeks 4-5</div>
                <div class="week-focus">Linear Algebra & Calculus</div>
                <div class="week-goals">Finish 3Blue1Brown & Khan courses; solve practice problems focusing on derivatives and chain rule.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 6</div>
                <div class="week-focus">Optimization Basics</div>
                <div class="week-goals">Cover gradient descent fundamentals, convexity concepts. Implement a simple optimizer from scratch.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 7</div>
                <div class="week-focus">Neural Network Basics</div>
                <div class="week-goals">Build perceptron from scratch, understand forward pass and common loss functions. Start with simple classification tasks.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 8</div>
                <div class="week-focus">Backpropagation & Gradient Descent</div>
                <div class="week-goals">Implement backpropagation algorithm step by step. Train small networks on toy datasets to solidify understanding.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 9</div>
                <div class="week-focus">Activation Functions & Architectures</div>
                <div class="week-goals">Explore different activation functions (ReLU, Sigmoid, Tanh). Introduction to CNNs with simple image classification.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 10</div>
                <div class="week-focus">Training Deep Networks</div>
                <div class="week-goals">Apply regularization techniques (dropout, batch norm). Train a complete network end-to-end on a real dataset.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 11</div>
                <div class="week-focus">Core RL - MDPs, Value Functions, Bellman Equations</div>
                <div class="week-goals">Study Sutton & Barto Chapters 1-3. Implement value iteration and policy iteration on grid world examples.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 12</div>
                <div class="week-focus">TD, Monte Carlo, Q-Learning, SARSA</div>
                <div class="week-goals">Implement tabular Q-Learning and SARSA agents. Test on simple environments like FrozenLake or Taxi.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 13</div>
                <div class="week-focus">Deep RL - DQN, Policy Gradients, Actor-Critic</div>
                <div class="week-goals">Implement DQN from scratch. Try OpenAI Gym examples with Atari games. Experiment with PPO using Stable-Baselines3.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 14</div>
                <div class="week-focus">Representation & Exploration</div>
                <div class="week-goals">Learn about contrastive embeddings and representation learning. Run experiments on Procgen environments.</div>
            </div>

            <div class="week-card">
                <div class="week-number">Week 15</div>
                <div class="week-focus">Projects & Research Portfolio</div>
                <div class="week-goals">Reproduce a research paper (PSE or EDE). Create visualizations of your results and build a portfolio showcasing your implementations.</div>
            </div>
        </div>
    </div>

    <div id="tools" class="content-section">
        <div class="tools-section">
            <h2>üîß Essential Tools & Libraries</h2>
            <p>These are key software and libraries you'll use throughout your Deep RL journey.</p>
            <div class="tools-grid">
                <div class="tool-item">
                    <div>
                        <div class="tool-name">Python</div>
                        <div class="tool-desc">Primary language for ML/RL development and research.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">PyTorch</div>
                        <div class="tool-desc">Flexible deep learning framework, widely used in academia and research.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">JAX</div>
                        <div class="tool-desc">High-performance numerical computing library, gaining popularity in Google Research.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">OpenAI Gym / Gymnasium</div>
                        <div class="tool-desc">Toolkit for developing and comparing reinforcement learning algorithms.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">DeepMind Control Suite</div>
                        <div class="tool-desc">Set of physics-based reinforcement learning environments, great for continuous control.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">Weights & Biases (W&B)</div>
                        <div class="tool-desc">Experiment tracking, visualization, and collaboration platform.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">Stable-Baselines3</div>
                        <div class="tool-desc">Set of reliable implementations of RL algorithms in PyTorch.</div>
                    </div>
                </div>
                <div class="tool-item">
                    <div>
                        <div class="tool-name">TensorFlow</div>
                        <div class="tool-desc">End-to-end open source machine learning platform.</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div id="resources" class="content-section">
        <div class="quiz-section">
            <h2>üîó Essential Resources</h2>
            <p>Curated collection of the best learning materials, now including key papers and environments.</p>

            <div class="quiz-card">
                <h3>üìö Books & Textbooks</h3>
                <div class="resource-links">
                    <a href="http://incompleteideas.net/book/the-book-2nd.html" class="resource-link" target="_blank">üìñ Sutton & Barto - Reinforcement Learning: An Introduction</a>
                    <a href="https://www.deeplearningbook.org/" class="resource-link" target="_blank">üìñ Deep Learning by Ian Goodfellow et al.</a>
                    <a href="https://nnfs.io/" class="resource-link" target="_blank">üìñ Neural Networks from Scratch</a>
                    <a href="https://think-bayes.github.io/think-bayes/" class="resource-link" target="_blank">üìñ Think Bayes</a>
                    <a href="https://web.stanford.edu/~boyd/cvxbook/" class="resource-link" target="_blank">üìñ Convex Optimization (Boyd & Vandenberghe)</a>
                    <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book" class="resource-link" target="_blank">üìñ Pattern Recognition and Machine Learning (Bishop)</a>
                </div>
            </div>

            <div class="quiz-card">
                <h3>üéì Online Courses & Lectures</h3>
                <div class="resource-links">
                    <a href="https://www.deeplearning.ai/" class="resource-link" target="_blank">üéì DeepLearning.AI Specialization</a>
                    <a href="https://course.fast.ai/" class="resource-link" target="_blank">üéì Fast.ai Practical Deep Learning</a>
                    <a href="https://www.davidsilver.uk/teaching/" class="resource-link" target="_blank">üé• David Silver's RL Course</a>
                    <a href="https://spinningup.openai.com/" class="resource-link" target="_blank">üöÄ OpenAI Spinning Up</a>
                    <a href="https://cs231n.github.io/" class="resource-link" target="_blank">üéì CS231n Stanford (CNNs)</a>
                    <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229 (Machine Learning)</a>
                    <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction" class="resource-link" target="_blank">ü§ó HuggingFace Deep RL Course</a>
                    <a href="https://www.khanacademy.org/math/statistics-probability" class="resource-link" target="_blank">üìä Khan Academy (Math Foundations)</a>
                    <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" class="resource-link" target="_blank">üé• 3Blue1Brown (Linear Algebra)</a>
                </div>
            </div>

            <div class="quiz-card">
                <h3>üìù Key Papers & Articles</h3>
                <div class="resource-links">
                    <a href="https://arxiv.org/" class="resource-link" target="_blank">üî¨ ArXiv (Preprint server)</a>
                    <a href="https://paperswithcode.com/" class="resource-link" target="_blank">üìä Papers with Code</a>
                    <a href="https://www.assemblyai.com/blog/rl-newsletter/" class="resource-link" target="_blank">‚úâÔ∏è The Reinforcement Learning Newsletter</a>
                    <a href="https://arxiv.org/abs/1312.5602" class="resource-link" target="_blank">üìÑ DQN Paper</a>
                    <a href="https://arxiv.org/abs/1707.06347" class="resource-link" target="_blank">üìÑ PPO Paper</a>
                    <a href="https://arxiv.org/abs/1312.6114" class="resource-link" target="_blank">üìÑ Auto-Encoding Variational Bayes (VAE)</a>
                    <a href="https://arxiv.org/abs/1911.05722" class="resource-link" target="_blank">üìÑ CURL: Contrastive Unsupervised Representations for RL</a>
                    <a href="https://arxiv.org/abs/2306.05483" class="resource-link" target="_blank">üìÑ Generalization via Adversarial Regularization</a>
                    <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" class="resource-link" target="_blank">üìù Lil'Log: Policy Gradient Algorithms</a>
                    <a href="https://lilianweng.github.io/posts/2021-01-29-self-supervised-learning/" class="resource-link" target="_blank">üìù Lil'Log: Self-Supervised Learning</a>
                </div>
            </div>

            <div class="quiz-card">
                <h3>üéÆ Environments & Benchmarks</h3>
                <div class="resource-links">
                    <a href="https://gym.openai.com/" class="resource-link" target="_blank">üèãÔ∏è OpenAI Gym / Gymnasium</a>
                    <a href="https://github.com/openai/procgen" class="resource-link" target="_blank">üéÆ OpenAI Procgen</a>
                    <a href="https://github.com/danijar/crafter" class="resource-link" target="_blank">üéÆ Crafter Environment</a>
                    <a href="https://github.com/deepmind/dm_control" class="resource-link" target="_blank">ü§ñ DeepMind Control Suite</a>
                    <a href="https://www.kaggle.com/competitions?sortBy=date&search=reinforcement+learning" class="resource-link" target="_blank">üèÖ Kaggle RL Competitions</a>
                </div>
            </div>
        </div>
    </div>

    <div id="checklist" class="content-section">
        <div class="progress-tracker">
            <h2>üìã Learning Checklist</h2>
            <p>Mark topics as complete to track your overall progress across all learning areas.</p>
            <div class="progress-bar">
                <div class="progress-fill" id="checklistProgressFill"></div>
            </div>
            <p id="checklistProgressText">0% Complete</p>
            <div class="checklist-items-container">
            </div>
        </div>
    </div>

    <div class="floating-progress">
        <span id="progress-text">0%</span>
    </div>
</div>

<nav class="floating-nav">
    <a href="#overview" class="nav-item">Overview</a>
    <a href="#phase1-math" class="nav-item">Phase 1</a>
    <a href="#phase2-ml-dl" class="nav-item">Phase 2</a>
    <a href="#phase3-rl" class="nav-item">Phase 3</a>
    <a href="#phase4-research" class="nav-item">Phase 4</a>
    <a href="#tools" class="nav-item">Tools</a>
    <a href="#resources" class="nav-item">Resources</a>
    <a href="#checklist" class="nav-item">Checklist</a>
</nav>

<div class="study-timer-container">
    <div class="timer-title">Study Timer</div>
    <div id="timerDisplay">00:00:00</div>
    <button id="timerBtn">Start</button>
    <button id="resetBtn">Reset</button>
</div>


<script>
    // Data for subtopic summaries
    const subtopicSummaries = {
        "distributions": "In probability, distributions describe the probabilities of different outcomes in a sample space, e.g., Gaussian (normal), Bernoulli, Poisson, etc.",
        "expectation-variance": "Expectation is the weighted average of possible outcomes (mean), while variance measures how far values in a data set are from the mean (spread).",
        "bayes-rule": "Bayes' Rule relates conditional probabilities. It's fundamental for updating beliefs about a hypothesis ($H$) given new evidence ($E$): $P(H|E) = [P(E|H) \\cdot P(H)] / P(E)$.",
        "kl-divergence": "Kullback-Leibler (KL) Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. It's often used in RL for measuring policy changes.",

        "vectors-matrices": "Vectors are ordered lists of numbers (1D arrays), representing points or directions. Matrices are 2D arrays of numbers, used for linear transformations and representing data.",
        "eigenvalues": "Eigenvalues and eigenvectors characterize linear transformations. An eigenvector's direction isn't changed by the transformation, only scaled by its eigenvalue.",
        "svd-pca": "Singular Value Decomposition (SVD) is a factorization of a matrix. Principal Component Analysis (PCA) is a dimensionality reduction technique based on SVD, finding orthogonal components that capture most variance.",

        "derivatives": "Derivatives measure the rate at which a function's output changes with respect to a change in its input. They represent the slope of a tangent line to a curve.",
        "chain-rule": "The chain rule is a formula to compute the derivative of a composite function. It's crucial for backpropagation in neural networks.",
        "partial-derivatives": "Partial derivatives describe how a multi-variable function's output changes with respect to one variable, holding others constant. Used for gradients.",
        "jacobians": "The Jacobian matrix is the matrix of all first-order partial derivatives of a vector-valued function. It's a generalization of the gradient for vector functions.",

        "gradient-descent": "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. It moves in the direction opposite to the gradient of the function.",
        "sgd-adam": "Stochastic Gradient Descent (SGD) and Adam are popular optimization algorithms (variants of gradient descent) used to train neural networks more efficiently by updating weights based on small batches of data.",
        "convex-optimization": "Convex optimization deals with minimizing convex functions over convex sets. Problems in this field have desirable properties, like any local minimum being a global minimum.",

        "supervised-learning": "Supervised learning is a machine learning paradigm where a model learns from labeled data (input-output pairs) to make predictions on new, unseen data.",
        "bias-variance": "Bias is the error from erroneous assumptions in the learning algorithm (underfitting). Variance is the error from sensitivity to small fluctuations in the training set (overfitting). The trade-off balances these.",
        "cross-validation": "Cross-validation is a technique to assess how the results of a statistical analysis will generalize to an independent dataset. It involves partitioning the dataset into subsets for training and validation, cycling through which subset is used for validation.",
        "ml-regularization": "Regularization techniques (e.g., L1, L2) are used in machine learning to prevent overfitting by adding a penalty to the loss function based on the magnitude of the model's weights, encouraging simpler models.",

        "nn-basics": "Neural Networks are computational models inspired by biological neural networks. They consist of layers of interconnected nodes (neurons) that process information through activation functions.",
        "backpropagation": "Backpropagation is the algorithm used to efficiently calculate the gradients of the loss function with respect to the weights of a neural network, enabling weight updates during training via gradient descent.",
        "cnns-rnns": "Convolutional Neural Networks (CNNs) are specialized for processing grid-like data (e.g., images) using convolutional filters. Recurrent Neural Networks (RNNs) are designed for sequential data (e.g., text, time series) by maintaining an internal state.",
        "transformers-intro": "Transformers are a neural network architecture, predominantly used in natural language processing (NLP), that rely on self-attention mechanisms to weigh the importance of different parts of the input sequence, allowing for parallel processing of sequences.",

        "autoencoders": "Autoencoders are neural networks trained to reconstruct their input. They learn a compressed, lower-dimensional representation (encoding) of the input data in the hidden layer, useful for dimensionality reduction and feature learning.",
        "vaes": "Variational Autoencoders (VAEs) are generative models that learn a probabilistic mapping from input to a latent space, allowing for sampling new, similar data by drawing from this learned distribution.",
        "contrastive-learning": "Contrastive learning is a self-supervised learning approach where the model learns representations by pulling similar (positive) samples closer and pushing dissimilar (negative) samples farther apart in the embedding space.",
        "self-supervised-learning": "Self-supervised learning is a technique where the model generates its own labels from the input data (e.g., predicting missing parts, ordering shuffled data) to learn meaningful representations without human-annotated labels, often serving as a pre-training step.",

        "mdps": "Markov Decision Processes (MDPs) are a mathematical framework for modeling sequential decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. Key components are states, actions, transitions, and rewards.",
        "value-functions": "Value functions estimate how good it is for an agent to be in a given state ($V^\\pi$) or to perform a given action in a given state ($Q^\\pi$), in terms of expected cumulative future rewards, under a given policy $\\pi$.",
        "bellman-equations": "Bellman equations define relationships between the value of a state (or state-action pair) and the values of its successor states under a given policy, forming the fundamental recursive relationships for solving MDPs.",
        "value-iteration": "Value Iteration is an algorithm that iteratively updates the estimated value function ($V$ or $Q$) until it converges to the optimal value function, from which an optimal policy can be directly derived.",
        "policy-iteration": "Policy Iteration is an iterative algorithm that alternates between two steps: Policy Evaluation (calculating the value function for the current policy) and Policy Improvement (updating the policy to be greedy with respect to the evaluated value function).",

        "q-learning-deep": "Deep Q-learning applies Q-learning updates to neural networks as function approximators, enabling it to handle large or continuous state spaces where tabular Q-learning is impractical.",
        "dqn": "Deep Q-Network (DQN) is a landmark Deep RL algorithm that uses a neural network to approximate the Q-value function. It incorporates key improvements like experience replay and a target network for stability.",
        "experience-replay": "Experience Replay is a technique used in Deep RL where past experiences (state, action, reward, next state) are stored in a buffer and randomly sampled during training. This breaks correlations in the data and improves sample efficiency.",
        "target-networks": "Target Networks are separate, periodically updated copies of the main Q-network. They provide stable Q-targets for the learning algorithm, reducing oscillation and improving training stability in DQN and similar methods.",
        "double-dueling-dqn": "Double DQN addresses the overestimation of Q-values by decoupling the selection of the next action from its evaluation. Dueling DQN is an architecture that separates the estimation of state-value function and advantage function, improving performance and stability.",

        "policy-gradients": "Policy Gradient methods directly optimize a parameterized policy by computing the gradient of the expected return with respect to the policy parameters and performing gradient ascent, aiming to increase the probability of actions that lead to higher rewards.",
        "actor-critic": "Actor-Critic methods combine the strengths of policy gradient (the 'actor' learns the policy) and value-based methods (the 'critic' learns a value function to critique the actor's actions). This often leads to more stable and efficient learning than pure policy gradients.",
        "a2c-a3c": "A2C (Advantage Actor-Critic) and A3C (Asynchronous Advantage Actor-Critic) are popular actor-critic algorithms. A3C uses asynchronous parallel agents for training stability and efficiency, while A2C is its synchronous counterpart.",
        "ppo": "Proximal Policy Optimization (PPO) is a family of policy gradient methods that optimize a 'clipped' surrogate objective function, leading to stable and efficient policy updates with fewer hyperparameter tunes. It's widely used due to its simplicity and robust performance.",
        "sac": "Soft Actor-Critic (SAC) is an off-policy actor-critic algorithm that aims to maximize expected return while also maximizing policy entropy. This dual objective promotes both reward maximization and effective exploration and robustness.",

        "environment-randomization": "Environment Randomization (or Domain Randomization) is a technique to improve generalization by training an agent in a simulator where certain aspects of the environment (e.g., textures, object positions, physics parameters) are varied randomly. This forces the agent to learn robust policies that transfer to new unseen variations in the real world or new simulated environments.",
        "procgen-benchmark": "Procgen Benchmark is a set of 16 procedurally generated reinforcement learning environments designed to test how well agents generalize to unseen environments. Unlike standard benchmarks, agents must adapt to new level layouts, object positions, etc., pushing the boundaries of generalization research.",
        "crafter-environment": "Crafter is a 2D open-world survival game environment designed to evaluate an agent's ability to learn and generalize across a diverse set of tasks and skills in a complex, procedurally generated world. It offers a broad spectrum of objectives and interactions.",
        "data-augmentation-rl": "Data Augmentation in RL refers to techniques used to increase the diversity of training data (observations, transitions) without collecting new experiences. This can involve applying transformations to images (e.g., cropping, rotating), adding noise, or re-sampling, to improve robustness and generalization by exposing the agent to varied inputs.",
        "policy-embeddings-transfer": "Policy Embeddings involve learning a low-dimensional representation for different policies or tasks. These embeddings can facilitate transfer learning by identifying similar policies for new tasks, or by using the embedding space to guide the exploration and learning process in novel environments.",

        "paper-reading": "The process of critically analyzing and understanding research papers. This involves identifying the problem addressed, the proposed solution, the methodology, experimental setup, key results, and limitations. It's crucial for staying updated and informing new research.",
        "algorithm-implementation": "Translating an algorithm's mathematical or pseudo-code description into working code. This often involves careful attention to details, handling edge cases, ensuring computational efficiency, and debugging.",
        "ablation-studies": "Experiments where a component or feature of a system (e.g., a specific loss term, a network module) is systematically removed or altered to understand its isolated contribution to the overall performance or behavior. Used to analyze the importance of different parts of an algorithm.",
        "benchmarking-hyperparameter-tuning": "Benchmarking involves evaluating an algorithm's performance on standard tasks and comparing it against established baselines or other methods. Hyperparameter tuning is the process of finding the optimal configuration of parameters (e.g., learning rate, network size) that maximize performance.",

        "define-project-scope": "Clearly outlining the goals, objectives, deliverables, and constraints of a project before starting implementation. This helps in managing expectations, staying focused, and breaking down a large problem into manageable parts.",
        "custom-environment-design": "Creating a new simulation or game environment for an RL agent to interact with. This involves defining the state space, action space, reward function, and transition dynamics that dictate how the agent perceives and interacts with the world.",
        "generalization-analysis-new-tasks": "Evaluating how well a trained RL agent performs on new, unseen tasks, environments, or variations of the original task that were not part of its training data. This assesses the agent's ability to generalize beyond its specific training experiences.",
        "policy-clustering-visualization": "Techniques used to group similar learned policies together (clustering) and visually represent them, often using dimensionality reduction methods like t-SNE or UMAP. This helps in understanding the diversity of behaviors an agent can learn or how different training runs lead to different policies.",
        "communication-results": "Effectively presenting and explaining research findings through various mediums such as written reports, academic papers, blog posts, presentations, or video demonstrations. The goal is to clearly convey complex technical information and its implications to different audiences."
    };

    // Checklist data (Expanded to include all detailed topics for comprehensive tracking)
    const checklistItems = [
        // Phase 1: Mathematics Foundations
        "Probability & Statistics: Distributions",
        "Probability & Statistics: Expectation & Variance",
        "Probability & Statistics: Bayes Rule",
        "Probability & Statistics: KL Divergence",
        "Linear Algebra: Vectors & Matrices",
        "Linear Algebra: Eigenvalues",
        "Linear Algebra: SVD & PCA",
        "Calculus: Derivatives",
        "Calculus: Chain Rule",
        "Calculus: Partial Derivatives",
        "Calculus: Jacobians",
        "Optimization: Gradient Descent",
        "Optimization: SGD & Adam",
        "Optimization: Convex Optimization",

        // Phase 2: Machine Learning & Deep Learning
        "ML Fundamentals: Supervised Learning",
        "ML Fundamentals: Bias-Variance",
        "ML Fundamentals: Cross-validation",
        "ML Fundamentals: Regularization",
        "Deep Learning: Neural Network Basics",
        "Deep Learning: Backpropagation",
        "Deep Learning: CNNs & RNNs",
        "Deep Learning: Transformers Intro",
        "Representation Learning: Autoencoders",
        "Representation Learning: Variational Autoencoders (VAEs)",
        "Representation Learning: Contrastive Learning",
        "Representation Learning: Self-supervised Learning",

        // Phase 3: Reinforcement Learning
        "Core RL: Markov Decision Processes (MDPs)",
        "Core RL: Value Functions ($V^\pi, Q^\pi$)",
        "Core RL: Bellman Equations",
        "Core RL: Value Iteration",
        "Core RL: Policy Iteration",
        "Deep RL: Q-learning (deep)",
        "Deep RL: Deep Q-Networks (DQN)",
        "Deep RL: Experience Replay",
        "Deep RL: Target Networks",
        "Deep RL: Double DQN, Dueling DQN",
        "Policy Gradient: REINFORCE",
        "Policy Gradient: Actor-Critic Methods",
        "Policy Gradient: A2C/A3C",
        "Policy Gradient: Proximal Policy Optimization (PPO)",
        "Policy Gradient: Soft Actor-Critic (SAC)",
        "Generalization in RL: Environment Randomization",
        "Generalization in RL: Procgen Benchmark",
        "Generalization in RL: Crafter Environment",
        "Generalization in RL: Data Augmentation for RL",
        "Generalization in RL: Policy Embeddings / Transfer Learning",

        // Phase 4: Research & Practice
        "Research: Paper Reading & Understanding",
        "Research: Algorithm Implementation from Scratch",
        "Research: Ablation Studies",
        "Research: Benchmarking & Hyperparameter Tuning",
        "Project: Define Project Scope",
        "Project: Custom Environment Design",
        "Project: Generalization Analysis on New Tasks",
        "Project: Policy Clustering / Visualization of Embeddings",
        "Project: Effective Communication of Results"
    ];

    // Initialize checklist
    function initializeChecklist() {
        const checklistContainer = document.querySelector('#checklist .checklist-items-container');
        checklistContainer.innerHTML = ''; // Clear existing items to prevent duplicates
        checklistItems.forEach((item, index) => {
            const checklistItem = document.createElement('div');
            checklistItem.className = 'checklist-item';
            checklistItem.innerHTML = `
                    <div class="checkbox" data-index="${index}">
                        <span class="checkmark" style="display: none;">‚úì</span>
                    </div>
                    <span>${item}</span>
                `;
            checklistItem.addEventListener('click', handleChecklistItemClick); // Add event listener to the item itself
            checklistContainer.appendChild(checklistItem);
        });
    }

    // Handle checkbox clicks (delegated from checklist item click)
    function handleChecklistItemClick(event) {
        const checkbox = event.currentTarget.querySelector('.checkbox'); // Get the checkbox within the clicked item
        const checkmark = checkbox.querySelector('.checkmark');
        const isChecked = checkbox.classList.contains('checked');

        if (isChecked) {
            checkbox.classList.remove('checked');
            checkmark.style.display = 'none';
        } else {
            checkbox.classList.add('checked');
            checkmark.style.display = 'block';
        }

        updateProgress();
        saveProgress();
    }

    // Update progress bars (both overall and checklist-specific)
    function updateProgress() {
        const checkboxes = document.querySelectorAll('#checklist .checkbox');
        const checkedBoxes = document.querySelectorAll('#checklist .checkbox.checked');
        const progress = (checkboxes.length > 0) ? (checkedBoxes.length / checkboxes.length) * 100 : 0;

        // Overall Progress (header and floating)
        const overallProgressDisplay = document.getElementById('overall-progress');
        const floatingProgressText = document.getElementById('progress-text');
        const completedTopicsDisplay = document.getElementById('completed-topics');

        overallProgressDisplay.textContent = `${progress.toFixed(0)}%`;
        floatingProgressText.textContent = `${progress.toFixed(0)}%`;
        completedTopicsDisplay.textContent = checkedBoxes.length; // Count completed items from checklist

        // Roadmap Progress (roadmap tab specific)
        const roadmapProgressFill = document.getElementById('roadmapProgressFill');
        const roadmapProgressText = document.getElementById('roadmapProgressText');

        if (roadmapProgressFill) { // Check if element exists (might not be on the current tab)
            roadmapProgressFill.style.width = progress + '%';

            if (progress === 0) {
                roadmapProgressText.textContent = "0% Complete - Let's get started!";
            } else if (progress < 25) {
                roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Great start! Keep building that foundation.`;
            } else if (progress < 50) {
                roadmapProgressText.textContent = `${Math.round(progress)}% Complete - You're making solid progress!`;
            } else if (progress < 75) {
                roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Over halfway there, excellent work!`;
            } else if (progress < 100) {
                roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Almost done, you're crushing it!`;
            } else {
                roadmapProgressText.textContent = "100% Complete - Congratulations! You're ready for RL research! üéâ";
            }
        }


        // Checklist Progress (checklist tab specific)
        const checklistProgressFill = document.getElementById('checklistProgressFill');
        const checklistProgressText = document.getElementById('checklistProgressText');

        if (checklistProgressFill) { // Check if element exists
            checklistProgressFill.style.width = progress + '%';
            checklistProgressText.textContent = `${Math.round(progress)}% Complete`;
        }
    }

    // Save progress to localStorage
    function saveProgress() {
        const checkedItems = [];
        document.querySelectorAll('#checklist .checkbox.checked').forEach(checkbox => {
            checkedItems.push(parseInt(checkbox.dataset.index));
        });
        localStorage.setItem('rl_roadmap_progress_checklist', JSON.stringify(checkedItems)); // Unique key
    }

    // Load progress from localStorage
    function loadProgress() {
        const saved = localStorage.getItem('rl_roadmap_progress_checklist');
        if (saved) {
            const checkedItems = JSON.parse(saved);
            checkedItems.forEach(index => {
                const checkbox = document.querySelector(`#checklist [data-index="${index}"]`);
                if (checkbox) {
                    checkbox.classList.add('checked');
                    checkbox.querySelector('.checkmark').style.display = 'block';
                }
            });
        }
        updateProgress(); // Ensure progress is updated after loading
    }

    // --- Tab Switching (from Deep RL Mastery) ---
    function showSection(sectionId) {
        const sections = document.querySelectorAll('.content-section');
        sections.forEach(section => section.classList.remove('active'));

        const tabs = document.querySelectorAll('.tab');
        tabs.forEach(tab => tab.classList.remove('active'));

        const selectedSection = document.getElementById(sectionId);
        selectedSection.classList.add('active');

        // Find the tab that matches the sectionId and activate it
        const selectedTab = Array.from(document.querySelectorAll('.tab')).find(tab => {
            const tabText = tab.textContent.trim().toLowerCase();
            const targetText = sectionId.toLowerCase();
            // Handle cases where tab text is longer (e.g., "Overview" vs "overview")
            return tabText.includes(targetText);
        });
        if (selectedTab) {
            selectedTab.classList.add('active');
        }
        // If switching to checklist or roadmap, ensure progress bars are visible
        if (sectionId === 'checklist' || sectionId === 'roadmap') {
            updateProgress();
        }
        // Trigger animation on elements within the newly active section
        animateSectionElements(selectedSection);
    }

    // --- Phase Toggling (adapted for new structure) ---
    function togglePhase(headerElement) {
        const phase = headerElement.closest('.phase');
        phase.classList.toggle('expanded');
    }

    // --- Subtopic Summary Functionality ---
    let currentOpenSummaryElement = null; // To track the currently open summary
    let currentActiveSubtopicButton = null; // To track the currently active button

    function showSubtopicSummary(buttonElement) {
        const topicId = buttonElement.dataset.topicId;
        const summaryText = subtopicSummaries[topicId];
        const weekSection = buttonElement.closest('.week-section');
        // Ensure each week-section has its own summary display area
        let summaryDisplayElement = weekSection.querySelector('.subtopic-summary');
        if (!summaryDisplayElement) {
            summaryDisplayElement = document.createElement('div');
            summaryDisplayElement.classList.add('subtopic-summary');
            weekSection.appendChild(summaryDisplayElement);
        }

        // Deactivate all other active subtopic buttons globally
        if (currentActiveSubtopicButton && currentActiveSubtopicButton !== buttonElement) {
            currentActiveSubtopicButton.classList.remove('active-summary');
        }
        // Close any currently open summary if it's different
        if (currentOpenSummaryElement && currentOpenSummaryElement !== summaryDisplayElement) {
            currentOpenSummaryElement.classList.remove('active');
            currentOpenSummaryElement.innerHTML = '';
        }


        // Toggle active class on the clicked button
        buttonElement.classList.toggle('active-summary');

        if (summaryText && buttonElement.classList.contains('active-summary')) {
            summaryDisplayElement.innerHTML = `<p>${summaryText}</p>`;
            summaryDisplayElement.classList.add('active');
            currentOpenSummaryElement = summaryDisplayElement; // Set current open summary
            currentActiveSubtopicButton = buttonElement; // Set current active button
        } else {
            summaryDisplayElement.classList.remove('active');
            summaryDisplayElement.innerHTML = '';
            currentOpenSummaryElement = null;
            currentActiveSubtopicButton = null;
        }

        // Re-render MathJax if present in the new content
        if (window.MathJax) {
            MathJax.typesetPromise([summaryDisplayElement]).then(() => {
                // typeset is complete
            }).catch((err) => console.log('MathJax typesetting failed: ' + err.message));
        }
    }


    // --- Quiz Functionality (from Deep RL Mastery) ---
    function selectOption(optionElement, isCorrect) {
        const quizCard = optionElement.closest('.quiz-card');
        const options = quizCard.querySelectorAll('.quiz-option');
        options.forEach(opt => {
            opt.classList.remove('selected', 'correct', 'incorrect');
        });

        optionElement.classList.add('selected');

        const feedback = quizCard.querySelector('.quiz-feedback');
        feedback.style.display = 'block';

        if (isCorrect) {
            optionElement.classList.add('correct');
            feedback.classList.add('correct');
            feedback.textContent = 'Correct! ' + getCorrectFeedback(quizCard.querySelector('.quiz-question').textContent.trim());
        } else {
            optionElement.classList.add('incorrect');
            feedback.classList.add('incorrect');
            feedback.textContent = 'Incorrect. Try again. ' + getCorrectFeedback(quizCard.querySelector('.quiz-question').textContent.trim());
        }

        // Re-render MathJax if present in the new content
        if (window.MathJax) {
            MathJax.typesetPromise([feedback]).then(() => {
                // typeset is complete
            }).catch((err) => console.log('MathJax typesetting failed: ' + err.message));
        }
    }

    // Helper function to provide specific feedback for correct answers
    function getCorrectFeedback(question) {
        const trimmedQuestion = question.replace(/^\d+\.\s*\(([^)]+)\)\s*|\d+\.\s*/, '').trim();

        switch (trimmedQuestion) {
            // Math Foundations
            case 'Which of the following best describes the union of two sets A and B?':
                return 'The union of two sets A and B, denoted $A \\cup B$, is the set containing all elements that are in A, or in B, or in both.';
            case 'If P(A) = 0.5, P(B) = 0.4, and A and B are independent events, what is P(A and B)?':
                return 'For independent events, $P(A \\text{ and } B) = P(A) \\cdot P(B) = 0.2$.';
            case 'What is the result of multiplying a 2x3 matrix by a 3x2 matrix?':
                return 'When multiplying matrices $A_{m \\times n}$ and $B_{n \\times p}$, the resulting matrix $C$ will have dimensions $m \\times p$. So, a 2x3 matrix times a 3x2 matrix results in a 2x2 matrix.';
            case 'What is the derivative of $f(x) = x^2 + 3x - 5$?':
                return 'Using the power rule for derivatives, the derivative of $x^n$ is $nx^{n-1}$. So, the derivative of $x^2$ is $2x$, the derivative of $3x$ is $3$, and the derivative of a constant $-5$ is $0$. Thus, $f\'(x) = 2x + 3$.';
            case 'Gradient Descent aims to find the minimum of a function by iteratively moving in which direction?':
                return 'Gradient descent moves in the direction opposite to the gradient because the gradient points towards the steepest ascent, and we want to find the minimum.';

            // Deep Learning Foundations
            case 'Which of the following is NOT a common activation function used in neural networks?':
                return 'Fourier Transform is a mathematical operation used for signal processing, not a typical activation function in neural networks. ReLU, Sigmoid, and Tanh are common activation functions.';
            case 'What is the primary role of backpropagation in neural network training?':
                return 'Backpropagation is an algorithm used to efficiently calculate the gradients of the loss function with respect to the network\'s weights. These gradients are then used by optimization algorithms (like gradient descent) to update the weights.';
            case 'Which technique helps prevent overfitting by randomly setting a fraction of neurons to zero during training?':
                return 'Dropout is a regularization technique where, during training, randomly selected neurons are ignored (or "dropped out") along with their connections. This prevents neurons from co-adapting too much and forces the network to learn more robust features.';

            // Core RL
            case 'What is the Bellman equation used for in reinforcement learning?':
                return 'The Bellman equation expresses the recursive relationship between the value of a state and the values of its successor states, allowing us to break down a complex problem into simpler subproblems.';
            case 'What is the main difference between Q-learning and SARSA?':
                return 'Q-learning is off-policy because it learns the optimal Q-value based on the greedy action in the next state, irrespective of the current policy being followed. SARSA is on-policy because it learns based on the action actually taken in the next state according to the current policy.';
            case 'What problem does the Deep Q-Network (DQN) address compared to traditional Q-learning?':
                return 'Traditional Q-learning uses tabular methods, which are impractical for large or continuous state spaces. DQN uses deep neural networks as function approximators to generalize across a high-dimensional state space.';
            case 'In the context of policy gradient methods, what does the gradient ascent update?':
                return 'Policy gradient methods directly optimize the policy parameters by using gradient ascent to maximize the expected cumulative reward, without necessarily learning a value function first (though actor-critic methods combine both).';
            case 'What is the purpose of experience replay in DQN?':
                return 'Experience replay stores past experiences and samples them randomly for training. This breaks the correlation between consecutive samples (which can be problematic for neural network training) and allows for better sample efficiency by reusing data.';

            // Representation & Generalization
            case 'What is the main goal of contrastive learning in the context of state representation?':
                return 'In contrastive learning for state representation, the goal is to learn an embedding space where representations of "similar" states (e.g., temporally close, or semantically related) are pulled closer together, while "dissimilar" states are pushed further apart.';
            case 'Unlike traditional RL which estimates expected returns, what does distributional RL estimate?':
                return 'Distributional RL aims to estimate the full distribution of returns (e.g., a histogram or a set of atoms representing the probability of different return values) rather than just a single expected value. This provides a richer understanding of the uncertainty in returns.';
            case 'What is the primary characteristic of offline (or batch) reinforcement learning?':
                return 'Offline RL learns a policy solely from a fixed, pre-collected dataset of environmental interactions, without any further online interaction with the environment during training. This is crucial for applications where online interaction is costly or dangerous.';

            // Projects & Portfolio
            case 'When implementing DQN, why is a target network often used?':
                return 'A target network in DQN helps stabilize training by providing a fixed, less frequently updated Q-value target. This reduces the instability caused by using the same network for both predicting Q-values and generating targets, which would lead to constantly chasing a moving target.';
            case 'Which dimensionality reduction technique is commonly used to visualize high-dimensional embeddings in 2D or 3D?':
                return 't-SNE (t-Distributed Stochastic Neighbor Embedding) is particularly popular for visualizing high-dimensional data, including embeddings, by projecting them into a lower-dimensional space (typically 2D or 3D) while trying to preserve local neighborhoods.';
            default:
                return '';
        }
    }

    // --- Mark Complete (from Deep RL Mastery - adjusted for new topic structure, but mainly checklist will be used) ---
    // This function will likely become less critical with the new checklist,
    // but keeping it for individual topic completion if desired.
    function markComplete(buttonElement) {
        const weekSection = buttonElement.closest('.week-section'); // Find the parent week-section
        if (!weekSection) return; // Exit if not found

        // Simple visual indication for marking the whole week section
        weekSection.style.opacity = 0.6; // Fade out the section
        buttonElement.disabled = true;
        buttonElement.textContent = 'Week Completed!';
        buttonElement.style.background = '#4CAF50'; // Green color for completed

        // Potentially, you could loop through the subtopics in this week section
        // and mark them as checked in the global checklistItems array and update localStorage.
        // For now, this just visually marks the week-section.
    }


    // Study Timer variables and functions (Moved outside DOMContentLoaded and into global scope for clarity and proper closure)
    let studyTime = 0;
    let timerInterval = null; // Initialize to null
    let isStudying = false;

    function addStudyTimer() {
        // Get references to elements, assuming they are in the HTML
        const timerContainer = document.querySelector('.study-timer-container'); // Select existing container
        if (!timerContainer) { // If container doesn't exist, create it (fallback, ideally in HTML)
            const newTimerContainer = document.createElement('div');
            newTimerContainer.className = 'study-timer-container';
            newTimerContainer.innerHTML = `
                    <div class="timer-title">Study Timer</div>
                    <div id="timerDisplay">00:00:00</div>
                    <button id="timerBtn">Start</button>
                    <button id="resetBtn">Reset</button>
                `;
            document.body.appendChild(newTimerContainer);
        }

        const timerDisplay = document.getElementById('timerDisplay');
        const timerBtn = document.getElementById('timerBtn');
        const resetBtn = document.getElementById('resetBtn');

        function updateTimerDisplay() {
            const hours = Math.floor(studyTime / 3600);
            const minutes = Math.floor((studyTime % 3600) / 60);
            const seconds = studyTime % 60;
            timerDisplay.textContent = `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
        }

        timerBtn.onclick = function() { // Use onclick directly for simplicity
            if (isStudying) {
                clearInterval(timerInterval);
                timerBtn.textContent = 'Start';
                isStudying = false;
                localStorage.setItem('study_timer_time', studyTime.toString()); // Save on pause
            } else {
                timerInterval = setInterval(() => {
                    studyTime++;
                    updateTimerDisplay();
                }, 1000);
                timerBtn.textContent = 'Pause';
                isStudying = true;
            }
        };

        resetBtn.onclick = function() { // Use onclick directly
            clearInterval(timerInterval);
            timerInterval = null; // Clear interval ID
            studyTime = 0;
            updateTimerDisplay();
            timerBtn.textContent = 'Start';
            isStudying = false;
            localStorage.removeItem('study_timer_time'); // Clear saved time on reset
        };

        // Load saved time on startup
        const savedTime = localStorage.getItem('study_timer_time');
        if (savedTime) {
            studyTime = parseInt(savedTime, 10);
            updateTimerDisplay();
        }

        // Save time before closing/leaving page (only if timer was running)
        window.addEventListener('beforeunload', () => {
            if (isStudying) { // Only save if timer was running
                localStorage.setItem('study_timer_time', studyTime.toString());
            }
        });
    }


    // Smooth scrolling for navigation
    function smoothScroll() {
        document.querySelectorAll('.floating-nav .nav-item, .tabs .tab').forEach(link => {
            link.addEventListener('click', function(e) {
                const href = this.getAttribute('href');
                const sectionId = this.dataset.sectionId || (href && href.startsWith('#') ? href.substring(1) : null);

                if (sectionId) {
                    e.preventDefault();
                    const targetElement = document.getElementById(sectionId);
                    if (targetElement) {
                        targetElement.scrollIntoView({
                            behavior: 'smooth',
                            block: 'start'
                        });
                    }
                }
            });
        });
    }

    // Animate elements within a section
    function animateSectionElements(section) {
        // Reset and animate direct children of the section if they are cards/phases/etc.
        section.querySelectorAll('.phase, .week-section, .overview-card, .tools-section, .quiz-section, .progress-tracker, .competency-card, .tool-item, .checklist-item').forEach(el => {
            el.style.opacity = '0'; // Reset opacity for re-animation
            el.style.transform = 'translateY(20px)'; // Reset position
            setTimeout(() => { // Small delay for sequential animation
                el.style.opacity = '1';
                el.style.transform = 'translateY(0)';
            }, Math.random() * 200); // Randomize delay slightly for a staggered effect
        });
    }

    // Initialize everything when page loads
    document.addEventListener('DOMContentLoaded', function() {
        initializeChecklist();
        loadProgress();
        smoothScroll();
        addStudyTimer(); // Call the function to set up the timer

        // Ensure correct tab is active on load
        showSection('roadmap'); // Default to roadmap view

        // Add MathJax configuration and re-typeset
        if (window.MathJax) {
            MathJax = {
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']]
                },
                startup: {
                    typeset: true // Ensure initial typeset
                }
            };
        }
    });

</script>
<script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</body>
</html>