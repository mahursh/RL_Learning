<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Reinforcement Learning Mastery & Generalization Roadmap</title>
  <style>
    /* Base styles from Deep RL Mastery */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      color: #333;
      line-height: 1.6; /* Added for readability */
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }

    /* Header (from Deep RL Mastery) */
    .header {
      text-align: center;
      margin-bottom: 40px;
      background: rgba(255, 255, 255, 0.95);
      padding: 30px;
      border-radius: 20px;
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
      backdrop-filter: blur(10px);
    }

    .header h1 {
      font-size: 3em;
      background: linear-gradient(135deg, #667eea, #764ba2);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 10px;
    }

    .header p {
      font-size: 1.2em;
      color: #666;
      margin-bottom: 20px;
    }

    .progress-overview {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin-bottom: 30px;
    }

    .progress-card {
      background: rgba(255, 255, 255, 0.9);
      padding: 20px;
      border-radius: 15px;
      text-align: center;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease;
    }

    .progress-card:hover {
      transform: translateY(-5px);
    }

    .progress-number {
      font-size: 2.5em;
      font-weight: bold;
      color: #667eea;
    }

    /* Tabs (from Deep RL Mastery) */
    .tabs {
      display: flex;
      background: rgba(255, 255, 255, 0.2);
      border-radius: 15px;
      padding: 5px;
      margin-bottom: 30px;
      backdrop-filter: blur(10px);
    }

    .tab {
      flex: 1;
      padding: 15px 20px;
      text-align: center;
      background: transparent;
      border: none;
      color: white;
      font-size: 1.1em;
      font-weight: 600;
      border-radius: 10px;
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .tab.active {
      background: rgba(255, 255, 255, 0.9);
      color: #333;
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }

    .content-section {
      display: none;
      animation: fadeIn 0.5s ease;
    }

    .content-section.active {
      display: block;
    }

    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(20px); }
      to { opacity: 1; transform: translateY(0); }
    }

    /* Phase Structure (from Deep RL Mastery, adapted for Generalization Roadmap style) */
    .phase {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 15px;
      margin-bottom: 30px;
      overflow: hidden;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }

    .phase-header {
      background: linear-gradient(135deg, #4c51bf, #667eea);
      color: white;
      padding: 25px 30px;
      cursor: pointer;
      transition: all 0.3s ease;
      display: flex;
      flex-direction: column;
      align-items: flex-start;
    }
    .phase-header h2 {
      font-size: 1.8rem;
      margin-bottom: 5px;
    }
    .phase-header p {
      font-size: 0.9em;
      margin-top: 5px;
      color: rgba(255, 255, 255, 0.8);
    }

    .phase-content {
      padding: 30px;
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.3s ease-out;
    }

    .phase.expanded .phase-content {
      max-height: 2000px;
      transition: max-height 0.5s ease-in;
    }

    /* Week Section (from Generalization Roadmap) */
    .week-section {
      margin-bottom: 30px;
      padding: 20px;
      background: #f8fafc;
      border-radius: 12px;
      border-left: 4px solid #38b2ac;
      transition: all 0.3s ease;
    }
    .week-section:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    }

    .week-section h4 {
      color: #2d3748;
      font-size: 1.3rem;
      margin-bottom: 15px;
    }

    .topics {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-bottom: 15px;
    }

    .topic-tag {
      background: linear-gradient(135deg, #38b2ac, #319795);
      color: white;
      padding: 5px 12px;
      border-radius: 20px;
      font-size: 0.85rem;
      font-weight: 500;
    }

    /* Resources (from Deep RL Mastery, combined with Generalization Roadmap styles) */
    .resources {
      margin-top: 15px;
    }

    .resources h5 {
      color: #4a5568;
      margin-bottom: 10px;
      font-weight: 600;
    }

    .resource-links {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
    }

    .resource-link {
      background: #667eea;
      color: white;
      padding: 8px 15px;
      border-radius: 20px;
      text-decoration: none;
      font-size: 0.9em;
      transition: all 0.3s ease;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    .resource-link:hover {
      background: #5a6fd8;
      transform: translateY(-2px);
    }

    /* Quiz Section (from Deep RL Mastery) */
    .quiz-section {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
    }
    .quiz-section h3 {
      font-size: 1.6em;
      color: #667eea;
      margin-bottom: 20px;
      text-align: center;
    }

    .quiz-card {
      background: #f8f9ff;
      border-radius: 15px;
      padding: 25px;
      margin-bottom: 20px;
      border-left: 4px solid #667eea;
    }

    .quiz-question {
      font-size: 1.1em;
      font-weight: 600;
      margin-bottom: 15px;
      color: #333;
    }

    .quiz-options {
      display: grid;
      gap: 10px;
      margin-bottom: 15px;
    }

    .quiz-option {
      padding: 12px 20px;
      background: white;
      border: 2px solid #e0e0e0;
      border-radius: 10px;
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .quiz-option:hover {
      border-color: #667eea;
      background: #f0f4ff;
    }

    .quiz-option.selected {
      border-color: #667eea;
      background: #667eea;
      color: white;
    }

    .quiz-option.correct {
      border-color: #28a745;
      background: #28a745;
      color: white;
    }

    .quiz-option.incorrect {
      border-color: #dc3545;
      background: #dc3545;
      color: white;
    }

    .quiz-feedback {
      padding: 15px;
      border-radius: 10px;
      margin-top: 15px;
      display: none;
    }

    .quiz-feedback.correct {
      background: #d4edda;
      color: #155724;
      border: 1px solid #c3e6cb;
    }

    .quiz-feedback.incorrect {
      background: #f8d7da;
      color: #721c24;
      border: 1px solid #f5c6cb;
    }

    /* Schedule Section (from Deep RL Mastery) */
    .week-schedule {
      display: grid;
      gap: 20px;
    }

    .week-card {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 15px;
      padding: 25px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
      border-left: 5px solid #667eea;
    }

    .week-number {
      font-size: 1.2em;
      font-weight: bold;
      color: #667eea;
      margin-bottom: 10px;
    }

    .week-focus {
      font-size: 1.1em;
      font-weight: 600;
      margin-bottom: 10px;
      color: #333;
    }

    .week-goals {
      color: #666;
      line-height: 1.6;
    }

    .btn {
      background: linear-gradient(135deg, #667eea, #764ba2);
      color: white;
      padding: 12px 25px;
      border: none;
      border-radius: 25px;
      cursor: pointer;
      font-size: 1em;
      font-weight: 600;
      transition: all 0.3s ease;
      margin-top: 15px;
    }

    .btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
    }

    /* New elements from RL Generalization Roadmap */
    .hero {
      text-align: center;
      padding: 40px 0;
      background: rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(10px);
      border-radius: 20px;
      margin-bottom: 30px;
      border: 1px solid rgba(255, 255, 255, 0.2);
      color: white;
    }

    .hero h1 {
      font-size: 3rem;
      font-weight: 800;
      background: linear-gradient(45deg, #fff, #e0e7ff);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 10px;
    }

    .hero p {
      color: #e0e7ff;
      font-size: 1.2rem;
      max-width: 600px;
      margin: 0 auto;
    }

    .overview-card {
      background: white;
      border-radius: 15px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }

    .overview-card h2 {
      color: #4c51bf;
      font-size: 2rem;
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .competencies-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 20px;
      margin-top: 20px;
    }

    .competency-card {
      background: linear-gradient(135deg, #f8fafc, #e2e8f0);
      padding: 20px;
      border-radius: 12px;
      border-left: 4px solid #4c51bf;
      transition: all 0.3s ease;
    }

    .competency-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 8px 25px rgba(76, 81, 191, 0.15);
    }

    .competency-card h4 {
      color: #2d3748;
      margin-bottom: 10px;
      font-size: 1.1rem;
    }

    .competency-card ul {
      list-style: none;
      color: #4a5568;
    }

    .competency-card li {
      padding: 2px 0;
      position: relative;
      padding-left: 15px;
    }

    .competency-card li:before {
      content: "‚ñ∏";
      position: absolute;
      left: 0;
      color: #4c51bf;
    }

    .tools-section {
      background: white;
      border-radius: 15px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }

    .tools-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 15px;
      margin-top: 20px;
    }

    .tool-item {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 15px;
      background: linear-gradient(135deg, #f7fafc, #edf2f7);
      border-radius: 10px;
      border-left: 4px solid #ed8936;
      transition: all 0.3s ease;
    }
    .tool-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }

    .tool-name {
      font-weight: 600;
      color: #2d3748;
    }

    .tool-desc {
      color: #4a5568;
      font-size: 0.9rem;
    }

    .progress-tracker {
      background: white;
      border-radius: 15px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }

    .progress-bar {
      width: 100%;
      height: 20px;
      background: #e2e8f0;
      border-radius: 10px;
      overflow: hidden;
      margin: 20px 0;
    }

    .progress-fill {
      height: 100%;
      background: linear-gradient(90deg, #48bb78, #38a169);
      width: 0%;
      transition: width 0.5s ease;
      border-radius: 10px;
    }

    .checklist-items-container {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 15px;
      margin-top: 20px;
    }

    .checklist-item {
      display: flex;
      align-items: center;
      gap: 10px;
      padding: 10px;
      background: #f8fafc;
      border-radius: 8px;
      cursor: pointer;
      transition: background 0.3s ease;
    }

    .checklist-item:hover {
      background: #e2e8f0;
    }

    .checkbox {
      width: 20px;
      height: 20px;
      border: 2px solid #4c51bf;
      border-radius: 4px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      transition: all 0.2s ease;
    }
    .checkbox:hover {
      transform: scale(1.1);
      border-color: #667eea;
    }

    .checkbox.checked {
      background: #4c51bf;
      color: white;
    }

    .floating-nav {
      position: fixed;
      right: 20px;
      top: 50%;
      transform: translateY(-50%);
      background: rgba(255, 255, 255, 0.9);
      backdrop-filter: blur(10px);
      border-radius: 12px;
      padding: 15px;
      box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
      z-index: 1000;
    }

    .nav-item {
      display: block;
      color: #4c51bf;
      text-decoration: none;
      padding: 8px 12px;
      border-radius: 8px;
      margin-bottom: 5px;
      font-size: 0.9rem;
      transition: background 0.3s ease;
    }

    .nav-item:hover {
      background: rgba(76, 81, 191, 0.1);
    }

    /* Floating Progress (from Deep RL Mastery) - keep for overall page progress */
    .floating-progress {
      position: fixed;
      bottom: 30px;
      right: 30px;
      background: rgba(255, 255, 255, 0.95);
      padding: 20px;
      border-radius: 50%;
      width: 80px;
      height: 80px;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
      backdrop-filter: blur(10px);
      font-weight: bold;
      color: #667eea;
      font-size: 1.1em;
    }

    /* Study Timer (New feature) */
    .study-timer-container {
      position: fixed;
      bottom: 20px;
      left: 20px;
      background: white;
      padding: 15px;
      border-radius: 12px;
      box-shadow: 0 5px 20px rgba(0,0,0,0.1);
      z-index: 1000;
    }
    .study-timer-container .timer-title {
      font-weight: 600;
      color: #4c51bf;
      margin-bottom: 10px;
    }
    .study-timer-container #timerDisplay {
      font-size: 1.2rem;
      font-weight: 600;
      color: #2d3748;
      margin-bottom: 10px;
    }
    .study-timer-container button {
      border: none;
      padding: 8px 16px;
      border-radius: 6px;
      cursor: pointer;
      margin-right: 8px;
      font-weight: 500;
      transition: background 0.3s ease;
    }
    .study-timer-container #timerBtn {
      background: #4c51bf;
      color: white;
    }
    .study-timer-container #timerBtn:hover {
      background: #5a6fd8;
    }
    .study-timer-container #resetBtn {
      background: #e53e3e;
      color: white;
    }
    .study-timer-container #resetBtn:hover {
      background: #c53030;
    }

    /* Subtopic Summary Styles */
    .subtopic-button {
      background: white;
      padding: 5px 12px;
      border-radius: 15px;
      font-size: 0.9em;
      border: 1px solid #e0e0e0;
      cursor: pointer;
      transition: all 0.2s ease;
      color: #333;
    }
    .subtopic-button:hover {
      background: #e6e9f0;
      border-color: #667eea;
    }
    .subtopic-button.active-summary {
      background: #667eea;
      color: white;
      border-color: #667eea;
    }
    .subtopic-summary {
      background: #e6f0ff;
      border-left: 3px solid #4c51bf;
      padding: 15px;
      border-radius: 8px;
      margin-top: 15px;
      font-size: 0.95em;
      color: #2d3748;
      display: none;
      animation: fadeIn 0.3s ease-out;
    }
    .subtopic-summary.active {
      display: block;
    }

    /* Exercise Section Styles */
    .exercise-section {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
    }
    .exercise-section h2 {
      font-size: 2em;
      color: #4c51bf;
      margin-bottom: 20px;
      text-align: center;
    }
    .exercise-section h3 { /* For sub-sections like "Probability Exercises" */
      font-size: 1.5em;
      color: #38b2ac;
      margin-top: 30px;
      margin-bottom: 15px;
      border-bottom: 2px solid #e0e0e0;
      padding-bottom: 5px;
    }
    .exercise-card {
      background: #f8f9ff;
      border-radius: 15px;
      padding: 25px;
      margin-bottom: 20px;
      border-left: 4px solid #667eea;
    }
    .exercise-question {
      font-size: 1.1em;
      font-weight: 600;
      margin-bottom: 15px;
      color: #333;
    }
    .solution-button {
      background: #38b2ac;
      color: white;
      padding: 10px 20px;
      border: none;
      border-radius: 20px;
      cursor: pointer;
      font-size: 0.95em;
      font-weight: 600;
      transition: all 0.3s ease;
      margin-top: 10px;
    }
    .solution-button:hover {
      background: #319795;
      transform: translateY(-1px);
    }
    .exercise-solution {
      background: #e0f2f7;
      border-left: 3px solid #319795;
      padding: 15px;
      border-radius: 8px;
      margin-top: 15px;
      font-size: 0.95em;
      color: #0c5460;
      display: none;
      animation: fadeIn 0.3s ease-out;
    }
    .exercise-solution.active {
      display: block;
    }
    .exercise-solution p {
      margin-bottom: 8px;
    }
    .exercise-solution strong {
      color: #4c51bf;
    }


    /* Responsive adjustments */
    @media (max-width: 768px) {
      .container {
        padding: 10px;
      }

      .header h1 {
        font-size: 2em;
      }
      .hero h1 {
        font-size: 2.2rem;
      }

      .tabs {
        flex-direction: column;
      }

      .progress-overview {
        grid-template-columns: 1fr 1fr;
      }

      .floating-progress {
        bottom: 20px;
        right: 20px;
        width: 60px;
        height: 60px;
        font-size: 0.9em;
      }
      .floating-nav {
        display: none;
      }
      .competencies-grid, .tools-grid, .checklist-items-container {
        grid-template-columns: 1fr;
      }
      .study-timer-container {
        bottom: 10px;
        left: 10px;
        padding: 10px;
      }
      .study-timer-container #timerDisplay {
        font-size: 1rem;
      }
      .study-timer-container button {
        padding: 6px 12px;
        font-size: 0.8rem;
      }
    }

    /* CSS animations for new elements */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .resource-link {
      transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    }

    .week-section {
      transition: all 0.3s ease;
    }

    .week-section:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    }

    .phase {
      transition: all 0.3s ease;
    }

    .checkbox {
      transition: all 0.2s ease;
    }

    .checkbox:hover {
      transform: scale(1.1);
      border-color: #667eea;
    }

    .competency-card {
      transition: all 0.3s ease;
    }

    .competency-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 8px 25px rgba(76, 81, 191, 0.15);
    }

    .tool-item {
      transition: all 0.3s ease;
    }

    .tool-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }
  </style>
  <script type="text/javascript" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']] // This handles inline math like $x^2$
      },
      startup: {
        typeset: true // Ensure initial typeset of the entire document
      }
    };
  </script>
</head>
<body>
<div class="container">
  <div class="hero">
    <h1>üß† RL Generalization Roadmap</h1>
    <p>Your comprehensive 6-month journey to mastering Reinforcement Learning Generalization</p>
  </div>

  <div class="header" style="background:none; box-shadow:none; backdrop-filter:none; padding:0; margin-bottom: 20px;">
    <div class="progress-overview">
      <div class="progress-card">
        <div class="progress-number" id="overall-progress">0%</div>
        <div>Overall Progress</div>
      </div>
      <div class="progress-card">
        <div class="progress-number" id="completed-topics">0</div>
        <div>Topics Completed</div>
      </div>
      <div class="progress-card">
        <div class="progress-number" id="current-week">Week 1</div>
        <div>Current Focus</div>
      </div>
      <div class="progress-card">
        <div class="progress-number" id="study-streak">0</div>
        <div>Study Streak</div>
      </div>
    </div>
  </div>

  <div class="tabs">
    <button class="tab active" onclick="showSection('roadmap')">üìö Roadmap</button>
    <button class="tab" onclick="showSection('overview')">üí° Overview</button>
    <button class="tab" onclick="showSection('quiz')">üß© Quizzes</button>
    <button class="tab" onclick="showSection('exercises')">‚úçÔ∏è Exercises</button> <button class="tab" onclick="showSection('schedule')">üìÖ Schedule</button>
    <button class="tab" onclick="showSection('tools')">üîß Tools</button>
    <button class="tab" onclick="showSection('resources')">üîó Resources</button>
    <button class="tab" onclick="showSection('checklist')">üìã Checklist</button>
  </div>

  <div id="overview" class="content-section">
    <div class="overview-card">
      <h2>‚úÖ Overview: Core Competencies</h2>
      <p>These are the fundamental areas you'll master throughout this roadmap.</p>
      <div class="competencies-grid">
        <div class="competency-card">
          <h4>üìä Mathematics</h4>
          <ul>
            <li>Probability & Statistics</li>
            <li>Linear Algebra</li>
            <li>Calculus</li>
            <li>Optimization</li>
          </ul>
        </div>
        <div class="competency-card">
          <h4>ü§ñ Machine Learning</h4>
          <ul>
            <li>Classical ML algorithms</li>
            <li>Bias-variance trade-off</li>
            <li>Overfitting concepts</li>
          </ul>
        </div>
        <div class="competency-card">
          <h4>üß† Deep Learning</h4>
          <ul>
            <li>Neural Networks</li>
            <li>CNNs, RNNs, Transformers</li>
            <li>Representation Learning</li>
          </ul>
        </div>
        <div class="competency-card">
          <h4>üéÆ Reinforcement Learning</h4>
          <ul>
            <li>MDPs, Bellman equations</li>
            <li>Value & Policy methods</li>
            <li>Generalization in RL</li>
          </ul>
        </div>
        <div class="competency-card">
          <h4>üî¨ Research Skills</h4>
          <ul>
            <li>Paper reading</li>
            <li>Algorithm implementation</li>
            <li>Experiment design</li>
          </ul>
        </div>
        <div class="competency-card">
          <h4>üíª Software</h4>
          <ul>
            <li>Python, PyTorch, JAX</li>
            <li>RL Environments</li>
            <li>Experiment tracking</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div id="roadmap" class="content-section active">
    <div class="progress-tracker">
      <h2>üìà Overall Roadmap Progress</h2>
      <div class="progress-bar">
        <div class="progress-fill" id="roadmapProgressFill"></div>
      </div>
      <p id="roadmapProgressText">0% Complete - Let's get started!</p>
    </div>

    <div class="phase" id="phase1-math">
      <div class="phase-header" onclick="togglePhase(this)">
        <div>
          <h2>üü¶ Phase 1: Mathematics Foundations</h2>
          <p>Weeks 1-6 ‚Ä¢ Building the essential mathematical foundation for ML and RL.</p>
        </div>
      </div>
      <div class="phase-content">
        <div class="week-section">
          <h4>üìÖ Week 1-2: Probability & Statistics</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="distributions">Distributions</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="expectation-variance">Expectation & Variance</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bayes-rule">Bayes Rule</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="kl-divergence">KL Divergence</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://www.khanacademy.org/math/statistics-probability" class="resource-link" target="_blank">üìä Khan Academy</a>
              <a href="https://greenteapress.com/thinkstats/" class="resource-link" target="_blank">üìñ Think Stats</a>
              <a href="https://www.youtube.com/playlist?list=PLblh5JKNUPxVjGyf6S3zFh_f4s-QYf6Pj" class="resource-link" target="_blank">üì∫ StatQuest (YouTube)</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 3: Linear Algebra</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="vectors-matrices">Vectors & Matrices</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="eigenvalues">Eigenvalues</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="svd-pca">SVD & PCA</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" class="resource-link" target="_blank">üé• 3Blue1Brown (YouTube)</a>
              <a href="https://web.stanford.edu/class/cs229/section/cs229-linalg.pdf" class="resource-link" target="_blank">üìÑ Stanford CS229 Notes</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 4: Calculus</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="derivatives">Derivatives</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="chain-rule">Chain Rule</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="partial-derivatives">Partial Derivatives</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="jacobians">Jacobians</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://www.khanacademy.org/math/calculus-1" class="resource-link" target="_blank">üìä Khan Academy Calculus</a>
              <a href="https://www.coursera.org/learn/machine-learning-linear-algebra" class="resource-link" target="_blank">üéì DeepLearning.ai Linear Algebra (Coursera)</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 5-6: Optimization</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="gradient-descent">Gradient Descent</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="sgd-adam">SGD & Adam</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="convex-optimization">Convex Optimization</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229 Lectures</a>
              <a href="https://web.stanford.edu/~boyd/cvxbook/" class="resource-link" target="_blank">üìñ Convex Optimization (Boyd & Vandenberghe)</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="phase" id="phase2-ml-dl">
      <div class="phase-header" onclick="togglePhase(this)">
        <div>
          <h2>üü¶ Phase 2: Machine Learning & Deep Learning</h2>
          <p>Weeks 7-12 ‚Ä¢ Core ML algorithms and neural network foundations.</p>
        </div>
      </div>
      <div class="phase-content">
        <div class="week-section">
          <h4>üìÖ Week 7-8: ML Fundamentals</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="supervised-learning">Supervised Learning</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bias-variance">Bias-Variance</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="cross-validation">Cross-validation</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ml-regularization">Regularization</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229</a>
              <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book" class="resource-link" target="_blank">üìñ Bishop: Pattern Recognition and Machine Learning</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 9-10: Deep Learning Foundations</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="nn-basics">Neural Network Basics</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="backpropagation">Backpropagation</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="cnns-rnns">CNNs & RNNs</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="transformers-intro">Transformers Intro</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://cs231n.github.io/" class="resource-link" target="_blank">üéì CS231n Stanford (CNNs)</a>
              <a href="https://www.coursera.org/specializations/deep-learning" class="resource-link" target="_blank">üéì DeepLearning.ai Specialization</a>
              <a href="https://nnfs.io/" class="resource-link" target="_blank">üìñ Neural Networks from Scratch</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 11-12: Representation Learning</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="autoencoders">Autoencoders</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="vaes">Variational Autoencoders (VAEs)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="contrastive-learning">Contrastive Learning</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="self-supervised-learning">Self-supervised Learning</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://arxiv.org/abs/1312.6114" class="resource-link" target="_blank">üìÑ Auto-Encoding Variational Bayes (Paper)</a>
              <a href="https://www.youtube.com/playlist?list=PLtk_G4NlB8j49yJ-K24Qp2WnI1uFp252P" class="resource-link" target="_blank">üì∫ Yannic Kilcher (YouTube - VAEs/GANs)</a>
              <a href="https://lilianweng.github.io/posts/2021-01-29-self-supervised-learning/" class="resource-link" target="_blank">üìù Lil'Log Self-Supervised Learning</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="phase" id="phase3-rl">
      <div class="phase-header" onclick="togglePhase(this)">
        <div>
          <h2>üü¶ Phase 3: Reinforcement Learning</h2>
          <p>Weeks 13-20 ‚Ä¢ Deep dive into core RL algorithms and generalization concepts.</p>
        </div>
      </div>
      <div class="phase-content">
        <div class="week-section">
          <h4>üìÖ Week 13-14: Tabular RL & MDPs</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="mdps">Markov Decision Processes (MDPs)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="value-functions">Value Functions ($V^\pi, Q^\pi$)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="bellman-equations">Bellman Equations</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="value-iteration">Value Iteration</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-iteration">Policy Iteration</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="http://incompleteideas.net/book/the-book-2nd.html" class="resource-link" target="_blank">üìñ Sutton & Barto (Full Book)</a>
              <a href="https://www.davidsilver.uk/teaching/" class="resource-link" target="_blank">üé• David Silver's RL Course</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 15-16: Deep RL Foundations (Value-Based)</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="q-learning-deep">Q-learning (deep)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="dqn">Deep Q-Networks (DQN)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="experience-replay">Experience Replay</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="target-networks">Target Networks</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="double-dueling-dqn">Double DQN, Dueling DQN</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://spinningup.openai.com/en/latest/" class="resource-link" target="_blank">üöÄ OpenAI Spinning Up</a>
              <a href="https://github.com/vwxyzjn/cleanrl" class="resource-link" target="_blank">üíª CleanRL Implementations</a>
              <a href="https://arxiv.org/abs/1312.5602" class="resource-link" target="_blank">üìÑ DQN Paper</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 17-18: Policy Gradient + Actor-Critic Methods</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-gradients">Policy Gradients (REINFORCE)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="actor-critic">Actor-Critic Methods</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="a2c-a3c">A2C/A3C</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ppo">Proximal Policy Optimization (PPO)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="sac">Soft Actor-Critic (SAC)</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Resources:</h5>
            <div class="resource-links">
              <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" class="resource-link" target="_blank">üìù Lil'Log: Policy Gradient Algorithms</a>
              <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction" class="resource-link" target="_blank">ü§ó HuggingFace Deep RL Course</a>
              <a href="https://arxiv.org/abs/1707.06347" class="resource-link" target="_blank">üìÑ PPO Paper</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 19-20: Generalization in RL</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="environment-randomization">Environment Randomization (Domain Randomization)</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="procgen-benchmark">Procgen Benchmark</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="crafter-environment">Crafter Environment</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="data-augmentation-rl">Data Augmentation for RL</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-embeddings-transfer">Policy Embeddings / Transfer Learning</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Key Papers & Environments:</h5>
            <div class="resource-links">
              <a href="https://arxiv.org/abs/2306.05483" class="resource-link" target="_blank">üìÑ Generalization via Adversarial Regularization (Paper)</a>
              <a href="https://openai.com/research/procgen" class="resource-link" target="_blank">üéÆ OpenAI Procgen</a>
              <a href="https://github.com/danijar/crafter" class="resource-link" target="_blank">üéÆ Crafter Environment</a>
              <a href="https://arxiv.org/abs/1911.05722" class="resource-link" target="_blank">üìÑ CURL: Contrastive Unsupervised Representations for RL (Paper)</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="phase" id="phase4-research">
      <div class="phase-header" onclick="togglePhase(this)">
        <div>
          <h2>üü¶ Phase 4: Research & Practice</h2>
          <p>Weeks 21-24 ‚Ä¢ Hands-on research experience and project development.</p>
        </div>
      </div>
      <div class="phase-content">
        <div class="week-section">
          <h4>üìÖ Week 21-22: Reproduce a Paper</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="paper-reading">Paper Reading & Understanding</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="algorithm-implementation">Algorithm Implementation from Scratch</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="ablation-studies">Ablation Studies</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="benchmarking-hyperparameter-tuning">Benchmarking & Hyperparameter Tuning</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Tools & Platforms:</h5>
            <div class="resource-links">
              <a href="https://pytorch.org/" class="resource-link" target="_blank">üî• PyTorch</a>
              <a href="https://jax.readthedocs.io/en/latest/" class="resource-link" target="_blank">üêç JAX</a>
              <a href="https://hydra.cc/" class="resource-link" target="_blank">‚öôÔ∏è Hydra (Config Management)</a>
              <a href="https://wandb.ai/" class="resource-link" target="_blank">üìä Weights & Biases (Experiment Tracking)</a>
              <a href="https://paperswithcode.com/" class="resource-link" target="_blank">üî¨ Papers With Code</a>
            </div>
          </div>
        </div>

        <div class="week-section">
          <h4>üìÖ Week 23-24: Mini Project & Portfolio Building</h4>
          <div class="topics">
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="define-project-scope">Define Project Scope</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="custom-environment-design">Custom Environment Design</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="generalization-analysis-new-tasks">Generalization Analysis on New Tasks</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="policy-clustering-visualization">Policy Clustering / Visualization of Embeddings</button>
            <button class="subtopic-button" onclick="showSubtopicSummary(this)" data-topic-id="communication-results">Effective Communication of Results (Report/Blog/Video)</button>
          </div>
          <div class="subtopic-summary"></div>
          <div class="resources">
            <h5>üìö Environments & Project Ideas:</h5>
            <div class="resource-links">
              <a href="https://gym.openai.com/" class="resource-link" target="_blank">üèãÔ∏è OpenAI Gym (Basic environments)</a>
              <a href="https://github.com/openai/procgen" class="resource-link" target="_blank">üéÆ Procgen (Procedural Generation)</a>
              <a href="https://github.com/deepmind/dm_control" class="resource-link" target="_blank">ü§ñ DeepMind Control Suite</a>
              <a href="https://www.kaggle.com/competitions?sortBy=date&search=reinforcement+learning" class="resource-link" target="_blank">üèÖ Kaggle RL Competitions</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div id="quiz" class="content-section">
    <h2>üß© Knowledge Quizzes</h2>
    <p>Test your understanding with these interactive quizzes, organized by phase!</p>

    <div class="quiz-section">
      <h3>üìê Math Foundations Quizzes</h3>

      <div class="quiz-card">
        <div class="quiz-question">1. (Set Theory) Which of the following best describes the union of two sets A and B?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) Elements common to both A and B.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) All elements that are in A, or in B, or in both.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) Elements in A but not in B.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) Elements not in A and not in B.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">2. (Probability) If P(A) = 0.5, P(B) = 0.4, and A and B are independent events, what is P(A and B)?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) 0.9</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) 0.2</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) 0.1</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) 0.0</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">3. (Linear Algebra) What is the result of multiplying a 2x3 matrix by a 3x2 matrix?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) A 3x3 matrix</div>
          <div class="quiz-option" onclick="selectOption(this, false)">B) A 2x3 matrix</div>
          <div class="quiz-option" onclick="selectOption(this, true)">C) A 2x2 matrix</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) A 3x2 matrix</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">4. (Calculus) What is the derivative of $f(x) = x^2 + 3x - 5$?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) $2x - 5$</div>
          <div class="quiz-option" onclick="selectOption(this, false)">B) $x + 3$</div>
          <div class="quiz-option" onclick="selectOption(this, true)">C) $2x + 3$</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) $x^3/3 + 3x^2/2 - 5x$</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">5. (Optimization) Gradient Descent aims to find the minimum of a function by iteratively moving in which direction?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) The direction of the steepest ascent</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) The direction opposite to the gradient</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) A random direction</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) The direction parallel to the gradient</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>
    </div>

    <div class="quiz-section">
      <h3>üß† Deep Learning Foundations Quizzes</h3>

      <div class="quiz-card">
        <div class="quiz-question">1. (Neural Network Basics) Which of the following is NOT a common activation function used in neural networks?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) ReLU</div>
          <div class="quiz-option" onclick="selectOption(this, false)">B) Sigmoid</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) Tanh</div>
          <div class="quiz-option" onclick="selectOption(this, true)">D) Fourier Transform</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">2. (Backpropagation) What is the primary role of backpropagation in neural network training?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) To perform the forward pass through the network.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) To compute the gradients of the loss function with respect to the network's weights.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) To initialize the weights of the network.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) To select the optimal network architecture.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">3. (Training Techniques) Which technique helps prevent overfitting by randomly setting a fraction of neurons to zero during training?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) Batch Normalization</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) Dropout</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) L1 Regularization</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) Early Stopping</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>
    </div>

    <div class="quiz-section">
      <h3>üéØ Core Reinforcement Learning Quizzes</h3>

      <div class="quiz-card">
        <div class="quiz-question">1. What is the Bellman equation used for in reinforcement learning?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) To calculate the gradient of the policy</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) To express the relationship between value functions at different time steps</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) To define the action space</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) To update neural network weights</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">2. What is the main difference between Q-learning and SARSA?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) Q-learning uses neural networks, SARSA doesn't</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) Q-learning is off-policy, SARSA is on-policy</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) Q-learning is for continuous actions, SARSA for discrete</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) There is no significant difference</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">3. What problem does the Deep Q-Network (DQN) address compared to traditional Q-learning?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) DQN works only with continuous action spaces</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) DQN can handle large state spaces through function approximation</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) DQN eliminates the need for exploration</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) DQN works faster than Q-learning</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">4. In the context of policy gradient methods, what does the gradient ascent update?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) The value function parameters</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) The policy parameters to maximize expected return</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) The environment dynamics</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) The reward function</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">5. What is the purpose of experience replay in DQN?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) To speed up training by using multiple GPUs</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) To break correlation between consecutive samples and improve sample efficiency</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) To generate new experiences from existing ones</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) To visualize the learning process</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>
    </div>

    <div class="quiz-section">
      <h3>üîÑ Representation & Generalization Quizzes</h3>

      <div class="quiz-card">
        <div class="quiz-question">1. (Contrastive Embeddings) What is the main goal of contrastive learning in the context of state representation?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) To learn a policy directly from raw pixels.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) To learn a representation where similar states are close and dissimilar states are far apart.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) To predict the next state given the current state and action.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) To minimize the number of actions required to reach a goal.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">2. (Distributional RL) Unlike traditional RL which estimates expected returns, what does distributional RL estimate?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) The optimal policy directly.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">B) The uncertainty in the environment.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">C) The full distribution of returns.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) The variance of the returns.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">3. (Offline RL) What is the primary characteristic of offline (or batch) reinforcement learning?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) The agent interacts with the environment in real-time.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) The agent learns from a fixed dataset of previously collected transitions without further environment interaction.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) The agent learns only from expert demonstrations.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) The agent is trained using only positive rewards.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>
    </div>

    <div class="quiz-section">
      <h3>üöÄ Projects & Portfolio Quizzes</h3>

      <div class="quiz-card">
        <div class="quiz-question">1. (Implementing Algorithms) When implementing DQN, why is a target network often used?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) To speed up the forward pass.</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) To stabilize the training process by providing a fixed Q-value target for a period.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) To generate more diverse experiences for replay.</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) To handle continuous action spaces.</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>

      <div class="quiz-card">
        <div class="quiz-question">2. (Embedding Visualizations) Which dimensionality reduction technique is commonly used to visualize high-dimensional embeddings in 2D or 3D?</div>
        <div class="quiz-options">
          <div class="quiz-option" onclick="selectOption(this, false)">A) Principal Component Analysis (PCA)</div>
          <div class="quiz-option" onclick="selectOption(this, true)">B) t-Distributed Stochastic Neighbor Embedding (t-SNE)</div>
          <div class="quiz-option" onclick="selectOption(this, false)">C) Linear Discriminant Analysis (LDA)</div>
          <div class="quiz-option" onclick="selectOption(this, false)">D) Independent Component Analysis (ICA)</div>
        </div>
        <div class="quiz-feedback"></div>
      </div>
    </div>
  </div>

  <div id="exercises" class="content-section">
    <div class="exercise-section">
      <h2>‚úçÔ∏è Math Exercises</h2>
      <p>Practice your math foundations with these problems. Click "Show Solution" to see the step-by-step answer.</p>

      <h3>Probability & Statistics Exercises</h3>

      <h4>Easy Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          1. You have a bag containing 5 red balls and 3 blue balls. If you draw two balls randomly without replacement, what is the probability that both balls are red?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Probability of drawing the first red ball.</strong></p>
          <p>There are 5 red balls out of a total of $5 + 3 = 8$ balls.</p>
          <p>$P(\\text{1st ball is red}) = \\frac{5}{8}$</p>
          <p><strong>Step 2: Probability of drawing the second red ball (given the first was red and not replaced).</strong></p>
          <p>After drawing one red ball, there are now 4 red balls left and a total of 7 balls remaining in the bag.</p>
          <p>$P(\\text{2nd ball is red | 1st is red}) = \\frac{4}{7}$</p>
          <p><strong>Step 3: Calculate the joint probability.</strong></p>
          <p>The probability that both balls are red is the product of the probabilities from Step 1 and Step 2:</p>
          <p>$P(\\text{both balls are red}) = P(\\text{1st is red}) \\times P(\\text{2nd is red | 1st is red})$</p>
          <p>$P(\\text{both balls are red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}$</p>
          <p>Therefore, the probability that both balls are red is $\\frac{5}{14}$ (approximately 0.357).</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          2. A new reinforcement learning agent receives rewards: +10 with probability 0.3, -5 with probability 0.5, and +2 with probability 0.2. What is the expected reward for this agent?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for expected value.</strong></p>
          <p>For a discrete random variable $X$ with outcomes $x_i$ and probabilities $P(X=x_i)$, the expected value $E[X]$ is given by:</p>
          <p>$E[X] = \\sum x_i P(X=x_i)$</p>
          <p><strong>Step 2: Identify the outcomes and their probabilities.</strong></p>
          <ul>
            <li>$x_1 = 10$, $P(X=10) = 0.3$</li>
            <li>$x_2 = -5$, $P(X=-5) = 0.5$</li>
            <li>$x_3 = 2$, $P(X=2) = 0.2$</li>
          </ul>
          <p><strong>Step 3: Calculate the expected value.</strong></p>
          <p>$E[\\text{Reward}] = (10)(0.3) + (-5)(0.5) + (2)(0.2)$</p>
          <p>$E[\\text{Reward}] = 3.0 - 2.5 + 0.4$</p>
          <p>$E[\\text{Reward}] = 0.9$</p>
          <p>The expected reward for this agent is 0.9.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          3. If $P(A) = 0.4$, $P(B) = 0.5$, and $P(A \cap B) = 0.2$, what is $P(A \cup B)$?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for the probability of the union of two events.</strong></p>
          <p>$P(A \cup B) = P(A) + P(B) - P(A \cap B)$</p>
          <p><strong>Step 2: Substitute the given probabilities into the formula.</strong></p>
          <p>$P(A \cup B) = 0.4 + 0.5 - 0.2$</p>
          <p>$P(A \cup B) = 0.9 - 0.2$</p>
          <p>$P(A \cup B) = 0.7$</p>
          <p>The probability of $A \\cup B$ is 0.7.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          4. If the probability of event A occurring is $P(A) = 0.7$, what is the probability that event A does NOT occur, $P(A^c)$?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the complement rule of probability.</strong></p>
          <p>$P(A^c) = 1 - P(A)$</p>
          <p><strong>Step 2: Substitute the given probability.</strong></p>
          <p>$P(A^c) = 1 - 0.7$</p>
          <p>$P(A^c) = 0.3$</p>
          <p>The probability that event A does NOT occur is 0.3.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          5. A spinner has 4 equal sections: Red, Blue, Green, Yellow. What is the probability of landing on Blue?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify favorable outcomes and total possible outcomes.</strong></p>
          <p>Favorable outcomes (landing on Blue): 1</p>
          <p>Total possible outcomes: 4 (Red, Blue, Green, Yellow)</p>
          <p><strong>Step 2: Calculate the probability.</strong></p>
          <p>$P(\\text{Blue}) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of outcomes}} = \\frac{1}{4}$</p>
          <p>The probability of landing on Blue is $\\frac{1}{4}$ or 0.25.</p>
        </div>
      </div>

      <h4>Medium Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          6. Using the previous exercise's data (rewards: +10 (0.3), -5 (0.5), +2 (0.2), and expected reward $E[X]=0.9$), calculate the variance of the rewards.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for variance.</strong></p>
          <p>The variance $Var(X)$ is given by $Var(X) = E[(X - E[X])^2] = \\sum (x_i - E[X])^2 P(X=x_i)$.</p>
          <p>We already have $E[X] = 0.9$.</p>
          <p><strong>Step 2: Calculate $(x_i - E[X])^2$ for each outcome.</strong></p>
          <ul>
            <li>For $x_1 = 10$: $(10 - 0.9)^2 = (9.1)^2 = 82.81$</li>
            <li>For $x_2 = -5$: $(-5 - 0.9)^2 = (-5.9)^2 = 34.81$</li>
            <li>For $x_3 = 2$: $(2 - 0.9)^2 = (1.1)^2 = 1.21$</li>
          </ul>
          <p><strong>Step 3: Calculate the variance.</strong></p>
          <p>$Var(X) = (82.81)(0.3) + (34.81)(0.5) + (1.21)(0.2)$</p>
          <p>$Var(X) = 24.843 + 17.405 + 0.242$</p>
          <p>$Var(X) = 42.49$</p>
          <p>The variance of the rewards is 42.49.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          7. You roll a fair six-sided die. If you roll an even number, you win 3 points. If you roll an odd number, you lose 2 points. What is the expected value of rolling the die?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify possible outcomes and their probabilities.</strong></p>
          <p>Even numbers: {2, 4, 6}. Probability $P(\\text{Even}) = 3/6 = 0.5$. Reward = +3.</p>
          <p>Odd numbers: {1, 3, 5}. Probability $P(\\text{Odd}) = 3/6 = 0.5$. Reward = -2.</p>
          <p><strong>Step 2: Apply the expected value formula.</strong></p>
          <p>$E[X] = \\sum x_i P(X=x_i)$</p>
          <p>$E[\\text{Points}] = (3 \\cdot 0.5) + (-2 \\cdot 0.5)$</p>
          <p>$E[\\text{Points}] = 1.5 - 1.0$</p>
          <p>$E[\\text{Points}] = 0.5$</p>
          <p>The expected value of rolling the die is 0.5 points.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          8. In a certain city, 30% of citizens own a dog. 40% own a cat. 15% own both a dog and a cat. What percentage of citizens own neither a dog nor a cat?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Define events and given probabilities.</strong></p>
          <p>Let $D$ be the event that a citizen owns a dog, and $C$ be the event that a citizen owns a cat.</p>
          <p>$P(D) = 0.30$</p>
          <p>$P(C) = 0.40$</p>
          <p>$P(D \\cap C) = 0.15$ (probability of owning both)</p>
          <p>We want to find $P(D^c \\cap C^c)$, which is the probability of owning neither.</p>
          <p><strong>Step 2: Use the formula for the union of two events.</strong></p>
          <p>$P(D \\cup C) = P(D) + P(C) - P(D \\cap C)$</p>
          <p>$P(D \\cup C) = 0.30 + 0.40 - 0.15 = 0.70 - 0.15 = 0.55$</p>
          <p>This is the probability that a citizen owns at least one pet (dog or cat or both).</p>
          <p><strong>Step 3: Use the complement rule to find the probability of owning neither.</strong></p>
          <p>$P(\\text{neither}) = P((D \\cup C)^c) = 1 - P(D \\cup C)$</p>
          <p>$P(\\text{neither}) = 1 - 0.55 = 0.45$</p>
          <p>Therefore, 45% of citizens own neither a dog nor a cat.</p>
        </div>
      </div>

      <h4>Hard Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          9. A diagnostic test has a 95% true positive rate (sensitivity) and a 1% false positive rate. If 0.5% of the population has the disease, what is the probability that a person who tests positive actually has the disease? (Use Bayes' Theorem).
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Define events and probabilities.</strong></p>
          <ul>
            <li>$D$: Has the disease. $P(D) = 0.005$</li>
            <li>$D^c$: Does not have the disease. $P(D^c) = 1 - P(D) = 0.995$</li>
            <li>$T$: Tests positive.</li>
            <li>$T^c$: Tests negative.</li>
          </ul>
          <p>Given probabilities:</p>
          <ul>
            <li>$P(T|D) = 0.95$ (True positive rate)</li>
            <li>$P(T|D^c) = 0.01$ (False positive rate)</li>
          </ul>
          <p>We want to find $P(D|T)$, the probability a person who tests positive actually has the disease.</p>
          <p><strong>Step 2: Apply Bayes' Theorem.</strong></p>
          <p>$P(D|T) = \\frac{P(T|D) \\cdot P(D)}{P(T)}$</p>
          <p><strong>Step 3: Calculate $P(T)$ using the law of total probability.</strong></p>
          <p>$P(T) = P(T|D)P(D) + P(T|D^c)P(D^c)$</p>
          <p>$P(T) = (0.95)(0.005) + (0.01)(0.995)$</p>
          <p>$P(T) = 0.00475 + 0.00995 = 0.0147$</p>
          <p><strong>Step 4: Substitute values into Bayes' Theorem.</strong></p>
          <p>$P(D|T) = \\frac{0.00475}{0.0147} \\approx 0.3238$</p>
          <p>Therefore, if a person tests positive, there is approximately a 32.38% chance they actually have the disease.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          10. Calculate the KL divergence $D_{KL}(P \\|\\| Q)$ for two discrete probability distributions: $P = [0.2, 0.3, 0.5]$ and $Q = [0.1, 0.4, 0.5]$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for KL Divergence.</strong></p>
          <p>For discrete distributions $P$ and $Q$, the KL divergence is:</p>
          <p>$D_{KL}(P \\|\\| Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)$</p>
          <p>Using natural logarithm (ln).</p>
          <p><strong>Step 2: Calculate each term of the sum.</strong></p>
          <ul>
            <li>Term 1: $0.2 \\cdot \\log(0.2 / 0.1) = 0.2 \\cdot \\log(2) \\approx 0.2 \\cdot 0.693 = 0.1386$</li>
            <li>Term 2: $0.3 \\cdot \\log(0.3 / 0.4) = 0.3 \\cdot \\log(0.75) \\approx 0.3 \\cdot (-0.287) = -0.0861$</li>
            <li>Term 3: $0.5 \\cdot \\log(0.5 / 0.5) = 0.5 \\cdot \\log(1) = 0.5 \\cdot 0 = 0$</li>
          </ul>
          <p><strong>Step 3: Sum the terms.</strong></p>
          <p>$D_{KL}(P \\|\\| Q) = 0.1386 - 0.0861 + 0 = 0.0525$</p>
          <p>The KL divergence $D_{KL}(P \\|\\| Q)$ is approximately 0.0525.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          11. A new COVID-19 test has a 99% accuracy if you have the disease (true positive rate) and 98% accuracy if you don't have it (true negative rate). If 1% of the population has COVID-19, what is the probability that a randomly selected person who tests negative actually has the disease?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Define events and probabilities.</strong></p>
          <ul>
            <li>$D$: Has disease. $P(D) = 0.01$</li>
            <li>$D^c$: Does not have disease. $P(D^c) = 1 - 0.01 = 0.99$</li>
            <li>$T$: Tests positive.</li>
            <li>$T^c$: Tests negative.</li>
          </ul>
          <p>Given rates:</p>
          <ul>
            <li>$P(T|D) = 0.99$ (true positive)</li>
            <li>$P(T^c|D^c) = 0.98$ (true negative)</li>
          </ul>
          <p>From true negative rate, we can find false positive rate:</p>
          <ul>
            <li>$P(T|D^c) = 1 - P(T^c|D^c) = 1 - 0.98 = 0.02$</li>
          </ul>
          <p>We want to find $P(D|T^c)$.</p>
          <p><strong>Step 2: Apply Bayes' Theorem.</strong></p>
          <p>$P(D|T^c) = \\frac{P(T^c|D) \\cdot P(D)}{P(T^c)}$</p>
          <p><strong>Step 3: Calculate $P(T^c|D)$.</strong></p>
          <p>$P(T^c|D) = 1 - P(T|D) = 1 - 0.99 = 0.01$ (False negative rate)</p>
          <p><strong>Step 4: Calculate $P(T^c)$ using the law of total probability.</strong></p>
          <p>$P(T^c) = P(T^c|D)P(D) + P(T^c|D^c)P(D^c)$</p>
          <p>$P(T^c) = (0.01)(0.01) + (0.98)(0.99)$</p>
          <p>$P(T^c) = 0.0001 + 0.9702 = 0.9703$</p>
          <p><strong>Step 5: Substitute values into Bayes' Theorem.</strong></p>
          <p>$P(D|T^c) = \\frac{0.01 \\cdot 0.01}{0.9703} = \\frac{0.0001}{0.9703} \\approx 0.000103$</p>
          <p>The probability that a person who tests negative actually has the disease is extremely low, approximately 0.0103%.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          12. A continuous random variable $X$ has a probability density function $f(x) = \frac{1}{2}e^{-x/2}$ for $x \\ge 0$, and $f(x) = 0$ for $x < 0$. Find the expected value $E[X]$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for expected value of a continuous random variable.</strong></p>
          <p>$E[X] = \\int_{-\\infty}^{\\infty} x f(x) dx$</p>
          <p>Since $f(x)=0$ for $x < 0$, the integral limits become from $0$ to $\\infty$.</p>
          <p>$E[X] = \\frac{1}{2} \\int_{0}^{\\infty} x e^{-x/2} dx$</p>
          <p><strong>Step 2: Use integration by parts formula: $\\int u \\, dv = uv - \\int v \\, du$.</strong></p>
          <p>Let $u = x$, so $du = dx$.</p>
          <p>Let $dv = e^{-x/2} dx$, so $v = -2e^{-x/2}$.</p>
          <p><strong>Step 3: Apply integration by parts.</strong></p>
          <p>$\\int_{0}^{\\infty} x e^{-x/2} dx = [-2x e^{-x/2}]_{0}^{\\infty} - \\int_{0}^{\\infty} (-2e^{-x/2}) dx$</p>
          <p>Evaluate the first term: $\\lim_{x\\to\\infty}(-2x e^{-x/2}) - (-2 \\cdot 0 \\cdot e^0) = 0 - 0 = 0$. (Using L'Hopital's rule for the limit if needed)</p>
          <p>Evaluate the second integral:</p>
          <p>$ -\\int_{0}^{\\infty} (-2e^{-x/2}) dx = 2 \\int_{0}^{\\infty} e^{-x/2} dx = 2 [-2e^{-x/2}]_{0}^{\\infty}$</p>
          <p>$= 2 [(\\lim_{x\\to\\infty}(-2e^{-x/2})) - (-2e^0)] = 2 [0 - (-2)] = 2(2) = 4$</p>
          <p><strong>Step 4: Combine the results.</strong></p>
          <p>$E[X] = \\frac{1}{2} \\cdot 4 = 2$</p>
          <p>The expected value $E[X]$ is 2. (This is the expected value of an exponential distribution with rate $\\lambda = 1/2$, where $E[X]=1/\\lambda=2$).</p>
        </div>
      </div>

      <h3>Linear Algebra Exercises</h3>

      <h4>Easy Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          13. Given two vectors $\mathbf{u} = \begin{pmatrix} 1 \\\\\\\\ 2 \end{pmatrix}$ and $\mathbf{v} = \begin{pmatrix} 3 \\\\\\\\ -1 \end{pmatrix}$, calculate their dot product.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the definition of the dot product for 2D vectors.</strong></p>
          <p>For two vectors $\mathbf{u} = \begin{pmatrix} u_1 \\\\\\\\ u_2 \end{pmatrix}$ and $\mathbf{v} = \begin{pmatrix} v_1 \\\\\\\\ v_2 \end{pmatrix}$, their dot product is given by:</p>
          <p>$\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2$</p>
          <p><strong>Step 2: Substitute the given values into the formula.</strong></p>
          <p>$\mathbf{u} = \begin{pmatrix} 1 \\\\\\\\ 2 \end{pmatrix}$, so $u_1 = 1$ and $u_2 = 2$.</p>
          <p>$\mathbf{v} = \begin{pmatrix} 3 \\\\\\\\ -1 \end{pmatrix}$, so $v_1 = 3$ and $v_2 = -1$.</p>
          <p>$\mathbf{u} \cdot \mathbf{v} = (1)(3) + (2)(-1)$</p>
          <p><strong>Step 3: Perform the multiplication and addition.</strong></p>
          <p>$\mathbf{u} \cdot \mathbf{v} = 3 - 2$</p>
          <p>$\mathbf{u} \cdot \mathbf{v} = 1$</p>
          <p>Therefore, the dot product of vectors $\mathbf{u}$ and $\mathbf{v}$ is 1.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          14. Given vector $\mathbf{w} = \begin{pmatrix} 3 \\\\\\\\ -4 \\\\\\\\ 0 \end{pmatrix}$, calculate its Euclidean (L2) norm, denoted $||\mathbf{w}||_2$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for the L2 norm of a vector.</strong></p>
          <p>For a vector $\mathbf{w} = \begin{pmatrix} w_1 \\\\\\\\ w_2 \\\\\\\\ \vdots \\\\\\\\ w_n \end{pmatrix}$, its L2 norm is $||\mathbf{w}||_2 = \sqrt{w_1^2 + w_2^2 + \\dots + w_n^2}$.</p>
          <p><strong>Step 2: Substitute the components of $\mathbf{w}$ into the formula.</strong></p>
          <p>$||\mathbf{w}||_2 = \sqrt{3^2 + (-4)^2 + 0^2}$</p>
          <p><strong>Step 3: Calculate the squares and sum them.</strong></p>
          <p>$||\mathbf{w}||_2 = \sqrt{9 + 16 + 0}$</p>
          <p>$||\mathbf{w}||_2 = \sqrt{25}$</p>
          <p><strong>Step 4: Take the square root.</strong></p>
          <p>$||\mathbf{w}||_2 = 5$</p>
          <p>The Euclidean norm of vector $\mathbf{w}$ is 5.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          15. Find the transpose of the matrix $M = \begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \end{pmatrix}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the definition of a matrix transpose.</strong></p>
          <p>The transpose of a matrix is obtained by flipping the matrix over its diagonal; that is, it switches the row and column indices of the matrix $M_{ij}$ becomes $M^T_{ji}$. Rows become columns, and columns become rows.</p>
          <p><strong>Step 2: Apply the transpose operation.</strong></p>
          <p>Given $M = \begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \end{pmatrix}$</p>
          <p>The first row of $M$ becomes the first column of $M^T$.</p>
          <p>The second row of $M$ becomes the second column of $M^T$.</p>
          <p>$M^T = \begin{pmatrix} 1 & 4 \\\\\\\\ 2 & 5 \\\\\\\\ 3 & 6 \end{pmatrix}$</p>
          <p>The transpose of $M$ is $\begin{pmatrix} 1 & 4 \\\\\\\\ 2 & 5 \\\\\\\\ 3 & 6 \end{pmatrix}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          16. Given matrices $A = \begin{pmatrix} 1 & 0 \\\\\\\\ 2 & 3 \end{pmatrix}$ and $B = \\begin{pmatrix} 4 & 5 \\\\\\\\ 6 & 7 \\end{pmatrix}$, calculate $A + B$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall matrix addition rule.</strong></p>
          <p>To add two matrices of the same dimensions, you add their corresponding elements.</p>
          <p><strong>Step 2: Add the corresponding elements.</strong></p>
          <p>$A + B = \\begin{pmatrix} 1+4 & 0+5 \\\\\\\\ 2+6 & 3+7 \\end{pmatrix}$</p>
          <p><strong>Step 3: Perform the additions.</strong></p>
          <p>$A + B = \\begin{pmatrix} 5 & 5 \\\\\\\\ 8 & 10 \\end{pmatrix}$</p>
          <p>The sum of matrices $A$ and $B$ is $\\begin{pmatrix} 5 & 5 \\\\\\\\ 8 & 10 \\end{pmatrix}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          17. What is the dimension of the vector $\\mathbf{v} = \\begin{pmatrix} 7 \\\\\\\\ -2 \\\\\\\\ 5 \\end{pmatrix}$?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Count the number of components in the vector.</strong></p>
          <p>The vector $\\mathbf{v}$ has 3 components: 7, -2, and 5.</p>
          <p><strong>Step 2: Determine the dimension.</strong></p>
          <p>The dimension of a vector is the number of its components.</p>
          <p>Therefore, the dimension of $\\mathbf{v}$ is 3.</p>
        </div>
      </div>

      <h4>Medium Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          18. Given matrices $A = \\begin{pmatrix} 1 & 2 \\\\\\\\ 3 & 4 \\end{pmatrix}$ and $B = \\begin{pmatrix} 5 & 6 \\\\\\\\ 7 & 8 \\end{pmatrix}$, calculate the product $AB$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall matrix multiplication rule.</strong></p>
          <p>For $C = AB$, the element $C_{ij}$ is the dot product of row $i$ of $A$ and column $j$ of $B$.</p>
          <p><strong>Step 2: Calculate $C_{11}$.</strong></p>
          <p>Row 1 of A: $[1, 2]$, Column 1 of B: $[5, 7]^T$</p>
          <p>$C_{11} = (1)(5) + (2)(7) = 5 + 14 = 19$</p>
          <p><strong>Step 3: Calculate $C_{12}$.</strong></p>
          <p>Row 1 of A: $[1, 2]$, Column 2 of B: $[6, 8]^T$</p>
          <p>$C_{12} = (1)(6) + (2)(8) = 6 + 16 = 22$</p>
          <p><strong>Step 4: Calculate $C_{21}$.</strong></p>
          <p>Row 2 of A: $[3, 4]$, Column 1 of B: $[5, 7]^T$</p>
          <p>$C_{21} = (3)(5) + (4)(7) = 15 + 28 = 43$</p>
          <p><strong>Step 5: Calculate $C_{22}$.</strong></p>
          <p>Row 2 of A: $[3, 4]$, Column 2 of B: $[6, 8]^T$</p>
          <p>$C_{22} = (3)(6) + (4)(8) = 18 + 32 = 50$</p>
          <p><strong>Step 6: Form the resulting matrix.</strong></p>
          <p>$AB = \\begin{pmatrix} 19 & 22 \\\\\\\\ 43 & 50 \\end{pmatrix}$</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          19. Given vectors $\\mathbf{a} = \\begin{pmatrix} 5 \\\\\\\\ 1 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 2 \\\\\\\\ 3 \\end{pmatrix}$, calculate the magnitude of their difference, $||\\mathbf{a} - \\mathbf{b}||_2$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Calculate the difference vector $\\mathbf{a} - \\mathbf{b}$.</strong></p>
          <p>$\\mathbf{a} - \\mathbf{b} = \\begin{pmatrix} 5 - 2 \\\\\\\\ 1 - 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\\\\\ -2 \\end{pmatrix}$</p>
          <p><strong>Step 2: Calculate the Euclidean (L2) norm of the resulting vector.</strong></p>
          <p>$||\\mathbf{a} - \\mathbf{b}||_2 = \\sqrt{3^2 + (-2)^2}$</p>
          <p>$||\\mathbf{a} - \\mathbf{b}||_2 = \\sqrt{9 + 4}$</p>
          <p>$||\\mathbf{a} - \\mathbf{b}||_2 = \\sqrt{13}$</p>
          <p>The magnitude of the difference is $\\sqrt{13}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          20. Calculate the product of matrix $A = \\begin{pmatrix} 2 & 1 \\\\\\\\ 0 & 3 \\end{pmatrix}$ and vector $\\mathbf{x} = \\begin{pmatrix} 4 \\\\\\\\ 5 \\end{pmatrix}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the rule for matrix-vector multiplication.</strong></p>
          <p>If $A$ is an $m \\times n$ matrix and $\\mathbf{x}$ is an $n \\times 1$ column vector, then $A\\mathbf{x}$ is an $m \\times 1$ column vector. Each element of the resulting vector is the dot product of the corresponding row of $A$ and the vector $\\mathbf{x}$.</p>
          <p><strong>Step 2: Perform the dot product for each row.</strong></p>
          <p>First row: $(2 \\cdot 4) + (1 \\cdot 5) = 8 + 5 = 13$</p>
          <p>Second row: $(0 \\cdot 4) + (3 \\cdot 5) = 0 + 15 = 15$</p>
          <p><strong>Step 3: Form the resulting vector.</strong></p>
          <p>$A\\mathbf{x} = \\begin{pmatrix} 13 \\\\\\\\ 15 \\end{pmatrix}$</p>
          <p>The product is $\\begin{pmatrix} 13 \\\\\\\\ 15 \\end{pmatrix}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          21. Find the trace of the matrix $M = \\begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\\\\\ 7 & 8 & 9 \\end{pmatrix}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the definition of the trace of a square matrix.</strong></p>
          <p>The trace of a square matrix is the sum of the elements on its main diagonal (from the upper left to the lower right).</p>
          <p><strong>Step 2: Identify the diagonal elements.</strong></p>
          <p>For $M = \\begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\\\\\ 7 & 8 & 9 \\end{pmatrix}$, the diagonal elements are 1, 5, and 9.</p>
          <p><strong>Step 3: Sum the diagonal elements.</strong></p>
          <p>$Trace(M) = 1 + 5 + 9 = 15$</p>
          <p>The trace of matrix $M$ is 15.</p>
        </div>
      </div>

      <h4>Hard Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          22. Find the inverse of the matrix $A = \\begin{pmatrix} 4 & 7 \\\\\\\\ 2 & 6 \\end{pmatrix}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for the inverse of a 2x2 matrix.</strong></p>
          <p>For a matrix $A = \\begin{pmatrix} a & b \\\\\\\\ c & d \\end{pmatrix}$, its inverse $A^{-1}$ is given by:</p>
          <p>$A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\\\\\ -c & a \\end{pmatrix}$</p>
          <p>The term $ad - bc$ is the determinant of $A$, denoted $det(A)$.</p>
          <p><strong>Step 2: Calculate the determinant of $A$.</strong></p>
          <p>$det(A) = (4)(6) - (7)(2) = 24 - 14 = 10$</p>
          <p><strong>Step 3: Apply the inverse formula.</strong></p>
          <p>$A^{-1} = \\frac{1}{10} \\begin{pmatrix} 6 & -7 \\\\\\\\ -2 & 4 \\end{pmatrix}$</p>
          <p><strong>Step 4: Multiply by the scalar $\\frac{1}{10}$.</strong></p>
          <p>$A^{-1} = \\begin{pmatrix} 6/10 & -7/10 \\\\\\\\ -2/10 & 4/10 \\end{pmatrix} = \\begin{pmatrix} 0.6 & -0.7 \\\\\\\\ -0.2 & 0.4 \\end{pmatrix}$</p>
          <p>The inverse of matrix $A$ is $\\begin{pmatrix} 0.6 & -0.7 \\\\\\\\ -0.2 & 0.4 \\end{pmatrix}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          23. For a matrix $A = \\begin{pmatrix} 2 & 1 \\\\\\\\ 1 & 2 \\end{pmatrix}$, set up the characteristic equation to find its eigenvalues. (No need to solve for eigenvalues)
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the characteristic equation for eigenvalues.</strong></p>
          <p>For a square matrix $A$, its eigenvalues $\\lambda$ are found by solving the characteristic equation:</p>
          <p>$det(A - \\lambda I) = 0$</p>
          <p>where $I$ is the identity matrix of the same dimension as $A$.</p>
          <p><strong>Step 2: Form the matrix $(A - \\lambda I)$.</strong></p>
          <p>$A - \\lambda I = \\begin{pmatrix} 2 & 1 \\\\\\\\ 1 & 2 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\\\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\lambda & 1 \\\\\\\\ 1 & 2 - \\lambda \\end{pmatrix}$</p>
          <p><strong>Step 3: Calculate the determinant of $(A - \\lambda I)$.</strong></p>
          <p>$det(A - \\lambda I) = (2 - \\lambda)(2 - \\lambda) - (1)(1)$</p>
          <p>$det(A - \\lambda I) = (2 - \\lambda)^2 - 1$</p>
          <p><strong>Step 4: Set the determinant to zero to form the characteristic equation.</strong></p>
          <p>$(2 - \\lambda)^2 - 1 = 0$</p>
          <p>This is the characteristic equation used to find the eigenvalues of matrix $A$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          24. Given two vectors $\\mathbf{a} = \\begin{pmatrix} 1 \\\\\\\\ 2 \\\\\\\\ 3 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 4 \\\\\\\\ 5 \\\\\\\\ 6 \\end{pmatrix}$, calculate their cross product $\\mathbf{a} \\times \\mathbf{b}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the formula for the cross product of 3D vectors.</strong></p>
          <p>For $\\mathbf{a} = \\begin{pmatrix} a_1 \\\\\\\\ a_2 \\\\\\\\ a_3 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} b_1 \\\\\\\\ b_2 \\\\\\\\ b_3 \\end{pmatrix}$, their cross product is:</p>
          <p>$\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} a_2 b_3 - a_3 b_2 \\\\\\\\ a_3 b_1 - a_1 b_3 \\\\\\\\ a_1 b_2 - a_2 b_1 \\end{pmatrix}$</p>
          <p><strong>Step 2: Substitute the components of $\\mathbf{a}$ and $\\mathbf{b}$.</strong></p>
          <ul>
            <li>$a_1=1, a_2=2, a_3=3$</li>
            <li>$b_1=4, b_2=5, b_3=6$</li>
          </ul>
          <p><strong>Step 3: Calculate each component of the resulting vector.</strong></p>
          <ul>
            <li>Component 1: $(2)(6) - (3)(5) = 12 - 15 = -3$</li>
            <li>Component 2: $(3)(4) - (1)(6) = 12 - 6 = 6$</li>
            <li>Component 3: $(1)(5) - (2)(4) = 5 - 8 = -3$</li>
          </ul>
          <p><strong>Step 4: Form the resulting cross product vector.</strong></p>
          <p>$\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} -3 \\\\\\\\ 6 \\\\\\\\ -3 \\end{pmatrix}$</p>
          <p>The cross product of $\\mathbf{a}$ and $\\mathbf{b}$ is $\\begin{pmatrix} -3 \\\\\\\\ 6 \\\\\\\\ -3 \\end{pmatrix}$.</p>
        </div>
      </div>

      <h3>Calculus Exercises</h3>

      <h4>Easy Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          25. Find the derivative of the function $f(x) = (x^2 + 1)^3$ with respect to $x$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify the outer and inner functions for the Chain Rule.</strong></p>
          <p>Let $u = x^2 + 1$ (inner function)</p>
          <p>Let $f(u) = u^3$ (outer function)</p>
          <p><strong>Step 2: Find the derivative of the outer function with respect to $u$.</strong></p>
          <p>$\\frac{df}{du} = \\frac{d}{du}(u^3) = 3u^2$</p>
          <p><strong>Step 3: Find the derivative of the inner function with respect to $x$.</strong></p>
          <p>$\\frac{du}{dx} = \\frac{d}{dx}(x^2 + 1) = 2x + 0 = 2x$</p>
          <p><strong>Step 4: Apply the Chain Rule: $\\frac{df}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx}$.</strong></p>
          <p>Substitute the derivatives found in Step 2 and Step 3:</p>
          <p>$\\frac{df}{dx} = (3u^2) \\cdot (2x)$</p>
          <p><strong>Step 5: Substitute $u$ back in terms of $x$.</strong></p>
          <p>Recall $u = x^2 + 1$. Substitute this back into the expression:</p>
          <p>$\\frac{df}{dx} = 3(x^2 + 1)^2 \\cdot (2x)$</p>
          <p>$\\frac{df}{dx} = 6x(x^2 + 1)^2$</p>
          <p>Therefore, the derivative of $f(x) = (x^2 + 1)^3$ is $f'(x) = 6x(x^2 + 1)^2$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          26. Find the derivative of $f(x) = 5x^3 - 2x + 7$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Apply the power rule and sum/difference rule.</strong></p>
          <p>Recall that $\\frac{d}{dx}(ax^n) = anx^{n-1}$ and $\\frac{d}{dx}(c) = 0$ for a constant $c$.</p>
          <p><strong>Step 2: Differentiate each term.</strong></p>
          <ul>
            <li>$\\frac{d}{dx}(5x^3) = 5 \\cdot 3x^{3-1} = 15x^2$</li>
            <li>$\\frac{d}{dx}(-2x) = -2 \\cdot 1x^{1-1} = -2x^0 = -2$</li>
            <li>$\\frac{d}{dx}(7) = 0$</li>
          </ul>
          <p><strong>Step 3: Combine the derivatives.</strong></p>
          <p>$f'(x) = 15x^2 - 2 + 0$</p>
          <p>$f'(x) = 15x^2 - 2$</p>
          <p>The derivative of $f(x)$ is $15x^2 - 2$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          27. Find the derivative of $f(x) = \\ln(x^2 + 5)$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the derivative of $\\ln(u)$ and apply the Chain Rule.</strong></p>
          <p>The derivative of $\\ln(u)$ with respect to $u$ is $\\frac{1}{u}$.</p>
          <p>Let $u = x^2 + 5$. Then $\\frac{du}{dx} = 2x$.</p>
          <p><strong>Step 2: Apply the Chain Rule.</strong></p>
          <p>$f'(x) = \\frac{1}{u} \\cdot \\frac{du}{dx}$</p>
          <p>$f'(x) = \\frac{1}{x^2 + 5} \\cdot (2x)$</p>
          <p><strong>Step 3: Simplify.</strong></p>
          <p>$f'(x) = \\frac{2x}{x^2 + 5}$</p>
          <p>The derivative is $\\frac{2x}{x^2 + 5}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          28. Evaluate $\\int x^5 dx$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Apply the power rule for integration.</strong></p>
          <p>$\\int x^n dx = \\frac{x^{n+1}}{n+1} + C$ for $n \\neq -1$.</p>
          <p><strong>Step 2: Apply the rule with $n=5$.</strong></p>
          <p>$\\int x^5 dx = \\frac{x^{5+1}}{5+1} + C$</p>
          <p>$\\int x^5 dx = \\frac{x^6}{6} + C$</p>
          <p>The integral is $\\frac{x^6}{6} + C$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          29. Find the derivative of $f(x) = \\sin(x) + \\cos(x)$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the derivatives of $\\sin(x)$ and $\\cos(x)$.</strong></p>
          <p>$\\frac{d}{dx}(\\sin(x)) = \\cos(x)$</p>
          <p>$\\frac{d}{dx}(\\cos(x)) = -\\sin(x)$</p>
          <p><strong>Step 2: Apply the sum rule for differentiation.</strong></p>
          <p>$f'(x) = \\frac{d}{dx}(\\sin(x)) + \\frac{d}{dx}(\\cos(x))$</p>
          <p>$f'(x) = \\cos(x) - \\sin(x)$</p>
          <p>The derivative is $\\cos(x) - \\sin(x)$.</p>
        </div>
      </div>

      <h4>Medium Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          30. Find the derivative of $g(x) = (x^2)(e^x)$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify $u$ and $v$ for the Product Rule.</strong></p>
          <p>Let $u = x^2$ and $v = e^x$.</p>
          <p><strong>Step 2: Find $u'$ and $v'$.</strong></p>
          <p>$u' = \\frac{d}{dx}(x^2) = 2x$</p>
          <p>$v' = \\frac{d}{dx}(e^x) = e^x$</p>
          <p><strong>Step 3: Apply the Product Rule: $(uv)' = u'v + uv'$.</strong></p>
          <p>$g'(x) = (2x)(e^x) + (x^2)(e^x)$</p>
          <p><strong>Step 4: Factor out common terms (optional, but good practice).</strong></p>
          <p>$g'(x) = e^x(2x + x^2)$</p>
          <p>$g'(x) = x e^x (2 + x)$</p>
          <p>The derivative of $g(x)$ is $x e^x (2 + x)$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          31. Find the derivative of $f(x) = e^{x^2 + 3x}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify outer and inner functions.</strong></p>
          <p>Outer function: $g(u) = e^u$</p>
          <p>Inner function: $u(x) = x^2 + 3x$</p>
          <p><strong>Step 2: Find derivatives of outer and inner functions.</strong></p>
          <p>$g'(u) = e^u$</p>
          <p>$u'(x) = 2x + 3$</p>
          <p><strong>Step 3: Apply the Chain Rule: $f'(x) = g'(u(x)) \\cdot u'(x)$.</strong></p>
          <p>$f'(x) = e^{x^2 + 3x} \\cdot (2x + 3)$</p>
          <p><strong>Step 4: Simplify.</strong></p>
          <p>$f'(x) = (2x + 3)e^{x^2 + 3x}$</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          32. Find the second derivative of $f(x) = x^4 - 2x^2 + 5$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Find the first derivative ($f'(x)$).</strong></p>
          <p>$f'(x) = \\frac{d}{dx}(x^4 - 2x^2 + 5)$</p>
          <p>$f'(x) = 4x^3 - 4x$</p>
          <p><strong>Step 2: Find the second derivative ($f''(x)$) by differentiating $f'(x)$.</strong></p>
          <p>$f''(x) = \\frac{d}{dx}(4x^3 - 4x)$</p>
          <p>$f''(x) = 4 \\cdot 3x^2 - 4 \\cdot 1$</p>
          <p>$f''(x) = 12x^2 - 4$</p>
          <p>The second derivative of $f(x)$ is $12x^2 - 4$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          33. Find the derivative of $f(x) = \\frac{e^x}{x^2}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify $u$ and $v$ for the Quotient Rule.</strong></p>
          <p>Let $u = e^x$ and $v = x^2$.</p>
          <p><strong>Step 2: Find $u'$ and $v'$.</strong></p>
          <p>$u' = \\frac{d}{dx}(e^x) = e^x$</p>
          <p>$v' = \\frac{d}{dx}(x^2) = 2x$</p>
          <p><strong>Step 3: Apply the Quotient Rule: $\\left(\\frac{u}{v}\\right)' = \\frac{u'v - uv'}{v^2}$.</strong></p>
          <p>$f'(x) = \\frac{(e^x)(x^2) - (e^x)(2x)}{(x^2)^2}$</p>
          <p><strong>Step 4: Simplify the expression.</strong></p>
          <p>$f'(x) = \\frac{x^2 e^x - 2x e^x}{x^4}$</p>
          <p>$f'(x) = \\frac{x e^x (x - 2)}{x^4}$</p>
          <p>$f'(x) = \\frac{e^x (x - 2)}{x^3}$</p>
          <p>The derivative is $\\frac{e^x (x - 2)}{x^3}$.</p>
        </div>
      </div>

      <h4>Hard Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          34. Find the derivative of $h(x) = \\frac{\\sin(x)}{x}$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify $u$ and $v$ for the Quotient Rule.</strong></p>
          <p>Let $u = \\sin(x)$ and $v = x$.</p>
          <p><strong>Step 2: Find $u'$ and $v'$.</strong></p>
          <p>$u' = \\frac{d}{dx}(\\sin(x)) = \\cos(x)$</p>
          <p>$v' = \\frac{d}{dx}(x) = 1$</p>
          <p><strong>Step 3: Apply the Quotient Rule: $\\left(\\frac{u}{v}\\right)' = \\frac{u'v - uv'}{v^2}$.</strong></p>
          <p>$h'(x) = \\frac{(\\cos(x))(x) - (\\sin(x))(1)}{x^2}$</p>
          <p><strong>Step 4: Simplify the expression.</strong></p>
          <p>$h'(x) = \\frac{x\\cos(x) - \\sin(x)}{x^2}$</p>
          <p>The derivative of $h(x)$ is $\\frac{x\\cos(x) - \\sin(x)}{x^2}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          35. Find the partial derivative of $f(x, y) = 3x^2y + 2xy^3 - 5x$ with respect to $x$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Treat $y$ as a constant and differentiate with respect to $x$.</strong></p>
          <p>Recall that $\\frac{\\partial}{\\partial x}(c \\cdot g(x)) = c \\cdot g'(x)$ and $\\frac{\\partial}{\\partial x}(h(y)) = 0$.</p>
          <p><strong>Step 2: Differentiate each term with respect to $x$.</strong></p>
          <ul>
            <li>For $3x^2y$: Treat $3y$ as a constant. $\\frac{\\partial}{\\partial x}(3x^2y) = 3y \\cdot \\frac{d}{dx}(x^2) = 3y(2x) = 6xy$</li>
            <li>For $2xy^3$: Treat $2y^3$ as a constant. $\\frac{\\partial}{\\partial x}(2xy^3) = 2y^3 \\cdot \\frac{d}{dx}(x) = 2y^3(1) = 2y^3$</li>
            <li>For $-5x$: $\\frac{\\partial}{\\partial x}(-5x) = -5$</li>
          </ul>
          <p><strong>Step 3: Combine the partial derivatives.</strong></p>
          <p>$\\frac{\\partial f}{\\partial x} = 6xy + 2y^3 - 5$</p>
          <p>The partial derivative of $f(x, y)$ with respect to $x$ is $6xy + 2y^3 - 5$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          36. Find $\\frac{\\partial}{\\partial y} f(x, y, z)$ for $f(x, y, z) = x^2 \\sin(y) + z^3 y^2 - 7x z$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Treat $x$ and $z$ as constants and differentiate with respect to $y$.</strong></p>
          <p><strong>Step 2: Differentiate each term with respect to $y$.</strong></p>
          <ul>
            <li>For $x^2 \\sin(y)$: Treat $x^2$ as constant. $\\frac{\\partial}{\\partial y}(x^2 \\sin(y)) = x^2 \\cos(y)$</li>
            <li>For $z^3 y^2$: Treat $z^3$ as constant. $\\frac{\\partial}{\\partial y}(z^3 y^2) = z^3 (2y) = 2yz^3$</li>
            <li>For $-7xz$: This term does not contain $y$, so its partial derivative with respect to $y$ is 0.</li>
          </ul>
          <p><strong>Step 3: Combine the partial derivatives.</strong></p>
          <p>$\\frac{\\partial f}{\\partial y} = x^2 \\cos(y) + 2yz^3$</p>
          <p>The partial derivative is $x^2 \\cos(y) + 2yz^3$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          37. Find $\\frac{\\partial^2 f}{\\partial x^2}$ for $f(x, y) = x^3y^2 - 2x^2 + 5y$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Find the first partial derivative with respect to $x$.</strong></p>
          <p>$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^3y^2 - 2x^2 + 5y)$</p>
          <p>$\\frac{\\partial f}{\\partial x} = 3x^2y^2 - 4x$</p>
          <p><strong>Step 2: Find the second partial derivative with respect to $x$ by differentiating the first partial derivative (from Step 1) with respect to $x$ again.</strong></p>
          <p>$\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(3x^2y^2 - 4x)$</p>
          <p>$\\frac{\\partial^2 f}{\\partial x^2} = 3y^2 \\cdot 2x - 4$</p>
          <p>$\\frac{\\partial^2 f}{\\partial x^2} = 6xy^2 - 4$</p>
          <p>The second partial derivative is $6xy^2 - 4$.</p>
        </div>
      </div>

      <h3>Optimization Exercises</h3>

      <h4>Easy Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          38. Perform one step of gradient descent for the function $f(x) = x^2$ starting at $x_0 = 3$ with a learning rate $\\alpha = 0.1$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Find the derivative (gradient) of the function.</strong></p>
          <p>$f'(x) = \\frac{d}{dx}(x^2) = 2x$</p>
          <p><strong>Step 2: Calculate the gradient at the initial point $x_0$.</strong></p>
          <p>$f'(3) = 2(3) = 6$</p>
          <p><strong>Step 3: Apply the gradient descent update rule.</strong></p>
          <p>$x_{new} = x_{old} - \\alpha \\cdot f'(x_{old})$</p>
          <p>$x_1 = 3 - (0.1)(6)$</p>
          <p>$x_1 = 3 - 0.6$</p>
          <p>$x_1 = 2.4$</p>
          <p>After one step of gradient descent, the new value of $x$ is 2.4.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          39. A model parameter $\\theta$ is at 2.0. Its gradient with respect to the loss is 0.5. If the learning rate $\\alpha = 0.01$, what is the new value of $\\theta$ after one gradient descent step?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the gradient descent update rule.</strong></p>
          <p>$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla L(\\theta_{old})$</p>
          <p>where $\\nabla L(\\theta_{old})$ is the gradient of the loss with respect to $\\theta$ at the current $\\theta_{old}$.</p>
          <p><strong>Step 2: Substitute the given values.</strong></p>
          <ul>
            <li>$\\theta_{old} = 2.0$</li>
            <li>$\\nabla L(\\theta_{old}) = 0.5$</li>
            <li>$\\alpha = 0.01$</li>
          </ul>
          <p>$\\theta_{new} = 2.0 - (0.01)(0.5)$</p>
          <p>$\\theta_{new} = 2.0 - 0.005$</p>
          <p>$\\theta_{new} = 1.995$</p>
          <p>The new value of $\\theta$ is 1.995.</p>
        </div>
      </div>

      <h4>Medium Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          40. Find the gradient of the function $f(x, y) = x^2 + 2y^2$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall the definition of the gradient.</strong></p>
          <p>For a function $f(x, y)$, the gradient $\\nabla f(x,y)$ is the vector of its partial derivatives:</p>
          <p>$\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\\\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}$</p>
          <p><strong>Step 2: Calculate the partial derivative with respect to $x$.</strong></p>
          <p>Treat $y$ as a constant: $\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + 2y^2) = 2x + 0 = 2x$</p>
          <p><strong>Step 3: Calculate the partial derivative with respect to $y$.</strong></p>
          <p>Treat $x$ as a constant: $\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + 2y^2) = 0 + 4y = 4y$</p>
          <p><strong>Step 4: Form the gradient vector.</strong></p>
          <p>$\\nabla f(x,y) = \\begin{pmatrix} 2x \\\\\\\\ 4y \\end{pmatrix}$</p>
          <p>The gradient of $f(x, y)$ is $\\begin{pmatrix} 2x \\\\\\\\ 4y \\end{pmatrix}$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          41. For the function $f(x) = (x-2)^2$, at what point is its gradient equal to zero?
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Find the derivative (gradient) of the function.</strong></p>
          <p>$f'(x) = \\frac{d}{dx}((x-2)^2) = 2(x-2) \\cdot 1 = 2x - 4$ (using Chain Rule)</p>
          <p><strong>Step 2: Set the gradient to zero and solve for $x$.</strong></p>
          <p>$2x - 4 = 0$</p>
          <p>$2x = 4$</p>
          <p>$x = 2$</p>
          <p>The gradient of the function is equal to zero at $x = 2$. This is the minimum point of the parabola.</p>
        </div>
      </div>

      <h4>Hard Exercises</h4>
      <div class="exercise-card">
        <div class="exercise-question">
          42. Find the gradient of $f(x, y, z) = x^2y + y^3z + z^2$.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Recall gradient definition.</strong></p>
          <p>$\\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\\\\\ \\frac{\\partial f}{\\partial y} \\\\\\\\ \\frac{\\partial f}{\\partial z} \\end{pmatrix}$</p>
          <p><strong>Step 2: Calculate partial derivative with respect to $x$.</strong></p>
          <p>$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2y + y^3z + z^2) = 2xy$</p>
          <p><strong>Step 3: Calculate partial derivative with respect to $y$.</strong></p>
          <p>$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2y + y^3z + z^2) = x^2 + 3y^2z$</p>
          <p><strong>Step 4: Calculate partial derivative with respect to $z$.</strong></p>
          <p>$\\frac{\\partial f}{\\partial z} = \\frac{\\partial}{\\partial z}(x^2y + y^3z + z^2) = y^3 + 2z$</p>
          <p><strong>Step 5: Form the gradient vector.</strong></p>
          <p>$\\nabla f(x, y, z) = \\begin{pmatrix} 2xy \\\\\\\\ x^2 + 3y^2z \\\\\\\\ y^3 + 2z \\end{pmatrix}$</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          43. Set up the Lagrangian for minimizing $f(x, y) = x^2 + y^2$ subject to the constraint $g(x, y) = x + y - 1 = 0$. (No need to solve)
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify the objective function and the constraint function.</strong></p>
          <p>Objective function: $f(x, y) = x^2 + y^2$</p>
          <p>Constraint function: $g(x, y) = x + y - 1 = 0$</p>
          <p><strong>Step 2: Introduce the Lagrange multiplier $\\lambda$.</strong></p>
          <p>The Lagrangian $\\mathcal{L}(x, y, \\lambda)$ is formed by combining the objective function and the constraint function with the Lagrange multiplier:</p>
          <p>$\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda g(x, y)$</p>
          <p><strong>Step 3: Substitute the given functions into the Lagrangian formula.</strong></p>
          <p>$\\mathcal{L}(x, y, \\lambda) = (x^2 + y^2) - \\lambda (x + y - 1)$</p>
          <p>This is the Lagrangian for the given constrained optimization problem.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          44. Set up the Lagrangian for maximizing $f(x, y, z) = xyz$ subject to $x + y + z = 1$. (No need to solve)
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Step 1: Identify objective function and constraint.</strong></p>
          <p>Objective function: $f(x, y, z) = xyz$</p>
          <p>Constraint function: $g(x, y, z) = x + y + z - 1 = 0$</p>
          <p><strong>Step 2: Form the Lagrangian.</strong></p>
          <p>$\\mathcal{L}(x, y, z, \\lambda) = f(x, y, z) - \\lambda g(x, y, z)$</p>
          <p>$\\mathcal{L}(x, y, z, \\lambda) = xyz - \\lambda(x + y + z - 1)$</p>
          <p>This is the Lagrangian for the problem.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          45. Is the function $f(x) = |x|$ convex? Explain why or why not.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Yes, the function $f(x) = |x|$ is convex.</strong></p>
          <p><strong>Explanation (Geometric):</strong></p>
          <p>A function is convex if, for any two points on its graph, the line segment connecting these two points lies entirely on or above the graph of the function. If you draw the graph of $f(x) = |x|$, which is a V-shape with its vertex at the origin, you'll observe that any line segment connecting two points on this V-shape will always be above or on the V-shape itself.</p>
          <p><strong>Explanation (Mathematical):</strong></p>
          <p>A continuously differentiable function is convex if its second derivative is non-negative ($f''(x) \\ge 0$). For $f(x) = |x|$:</p>
          <ul>
            <li>For $x > 0$, $f(x) = x$, so $f'(x) = 1$ and $f''(x) = 0$.</li>
            <li>For $x < 0$, $f(x) = -x$, so $f'(x) = -1$ and $f''(x) = 0$.</li>
          </ul>
          <p>At $x=0$, the function is not differentiable in the classical sense (it has a sharp corner). However, the definition of convexity extends to non-differentiable functions. A more general definition states that for a function $f$ to be convex, for any $x, y$ in its domain and any $\\alpha \\in [0, 1]$:</p>
          <p>$f(\\alpha x + (1 - \\alpha)y) \\le \\alpha f(x) + (1 - \\alpha)f(y)$</p>
          <p>This property holds true for $f(x) = |x|$, which can be proven using the triangle inequality. The subgradient for $f(x)=|x|$ at $x=0$ is any value in $[-1, 1]$, and the function satisfies the first-order convexity condition for all $x$.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          46. Describe the potential issues of using a learning rate that is too large in gradient descent.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p>If the learning rate (step size) in gradient descent is too large, it can lead to several problems:</p>
          <ul>
            <li><strong>Oscillation:</strong> The optimization algorithm might repeatedly overshoot the minimum, oscillating back and forth across it without converging.</li>
            <li><strong>Divergence:</strong> The algorithm might take increasingly large steps, moving further and further away from the minimum, causing the loss to increase instead of decrease.</li>
            <li><strong>Missing the minimum:</strong> Even if it doesn't diverge, a very large learning rate can cause the optimizer to jump over the true minimum entirely, settling in a suboptimal region or never truly converging.</li>
            <li><strong>Numerical instability:</strong> Extremely large steps can lead to parameters becoming very large (exploding gradients), causing numerical overflow issues.</li>
          </ul>
          <p>Effectively, a large learning rate can prevent the optimization process from converging to a good solution or from converging at all.</p>
        </div>
      </div>

      <div class="exercise-card">
        <div class="exercise-question">
          47. Explain the concept of "local minima" and "global minimum" in the context of optimization, and why it's a challenge for gradient descent in non-convex functions.
        </div>
        <button class="solution-button" onclick="toggleSolution(this)">Show Solution</button>
        <div class="exercise-solution">
          <p><strong>Global Minimum:</strong> The global minimum of a function is the point where the function's value is the lowest across its entire domain. It's the absolute lowest point on the function's landscape.</p>
          <p><strong>Local Minimum:</strong> A local minimum is a point where the function's value is lower than at any nearby points within a specific region or neighborhood of the function's domain, but it might not be the absolute lowest point globally.</p>
          <p><strong>Challenge for Gradient Descent in Non-Convex Functions:</strong></p>
          <p>Gradient descent is a greedy algorithm; it always moves in the direction of the steepest descent. In **convex functions**, this is guaranteed to lead to the global minimum because there's only one minimum and the function curves upwards everywhere. However, in **non-convex functions** (common in deep learning loss landscapes), there can be multiple local minima.</p>
          <p>The challenge is that gradient descent can get "stuck" in a local minimum. Once it reaches a point where the gradient is zero (or very close to zero) in a local minimum, it will stop or slow down significantly, even if there's a much lower global minimum elsewhere on the loss landscape. It lacks a mechanism to explore beyond the immediate vicinity of its current position once a local optimum is found. This is a primary reason why techniques like momentum, Adam, or even different initialization strategies are used to help optimizers escape shallow local minima or saddle points.</p>
        </div>
      </div>
    </div>
  </div>

  <div id="schedule" class="content-section">
    <div class="week-schedule">
      <h2>üìÖ 15-Week Learning Schedule (Deep RL Core)</h2>
      <p>This schedule outlines a systematic approach to mastering core Deep RL concepts.</p>

      <div class="week-card">
        <div class="week-number">Week 1</div>
        <div class="week-focus">Set Theory & Logic</div>
        <div class="week-goals">Understand sets, functions, and logical operations. Practice with exercises from Khan Academy and complete basic proofs.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Weeks 2-3</div>
        <div class="week-focus">Probability & Statistics</div>
        <div class="week-goals">Learn distributions, expectation, variance. Complete Khan Academy statistics course and work through Think Stats problems.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Weeks 4-5</div>
        <div class="week-focus">Linear Algebra & Calculus</div>
        <div class="week-goals">Finish 3Blue1Brown & Khan courses; solve practice problems focusing on derivatives and chain rule.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 6</div>
        <div class="week-focus">Optimization Basics</div>
        <div class="week-goals">Cover gradient descent fundamentals, convexity concepts. Implement a simple optimizer from scratch.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 7</div>
        <div class="week-focus">Neural Network Basics</div>
        <div class="week-goals">Build perceptron from scratch, understand forward pass and common loss functions. Start with simple classification tasks.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 8</div>
        <div class="week-focus">Backpropagation & Gradient Descent</div>
        <div class="week-goals">Implement backpropagation algorithm step by step. Train small networks on toy datasets to solidify understanding.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 9</div>
        <div class="week-focus">Activation Functions & Architectures</div>
        <div class="week-goals">Explore different activation functions (ReLU, Sigmoid, Tanh). Introduction to CNNs with simple image classification.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 10</div>
        <div class="week-focus">Training Deep Networks</div>
        <div class="week-goals">Apply regularization techniques (dropout, batch norm). Train a complete network end-to-end on a real dataset.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 11</div>
        <div class="week-focus">Core RL - MDPs, Value Functions, Bellman Equations</div>
        <div class="week-goals">Study Sutton & Barto Chapters 1-3. Implement value iteration and policy iteration on grid world examples.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 12</div>
        <div class="week-focus">TD, Monte Carlo, Q-Learning, SARSA</div>
        <div class="week-goals">Implement tabular Q-Learning and SARSA agents. Test on simple environments like FrozenLake or Taxi.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 13</div>
        <div class="week-focus">Deep RL - DQN, Policy Gradients, Actor-Critic</div>
        <div class="week-goals">Implement DQN from scratch. Try OpenAI Gym examples with Atari games. Experiment with PPO using Stable-Baselines3.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 14</div>
        <div class="week-focus">Representation & Exploration</div>
        <div class="week-goals">Learn about contrastive embeddings and representation learning. Run experiments on Procgen environments.</div>
      </div>

      <div class="week-card">
        <div class="week-number">Week 15</div>
        <div class="week-focus">Projects & Research Portfolio</div>
        <div class="week-goals">Reproduce a research paper (PSE or EDE). Create visualizations of your results and build a portfolio showcasing your implementations.</div>
      </div>
    </div>
  </div>

  <div id="tools" class="content-section">
    <div class="tools-section">
      <h2>üîß Essential Tools & Libraries</h2>
      <p>These are key software and libraries you'll use throughout your Deep RL journey.</p>
      <div class="tools-grid">
        <div class="tool-item">
          <div>
            <div class="tool-name">Python</div>
            <div class="tool-desc">Primary language for ML/RL development and research.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">PyTorch</div>
            <div class="tool-desc">Flexible deep learning framework, widely used in academia and research.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">JAX</div>
            <div class="tool-desc">High-performance numerical computing library, gaining popularity in Google Research.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">OpenAI Gym / Gymnasium</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">DeepMind Control Suite</div>
            <div class="tool-desc">Set of physics-based reinforcement learning environments, great for continuous control.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">Weights & Biases (W&B)</div>
            <div class="tool-desc">Experiment tracking, visualization, and collaboration platform.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">Stable-Baselines3</div>
            <div class="tool-desc">Set of reliable implementations of RL algorithms in PyTorch.</div>
          </div>
        </div>
        <div class="tool-item">
          <div>
            <div class="tool-name">TensorFlow</div>
            <div class="tool-desc">End-to-end open source machine learning platform.</div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div id="resources" class="content-section">
    <div class="quiz-section">
      <h2>üîó Essential Resources</h2>
      <p>Curated collection of the best learning materials, now including key papers and environments.</p>

      <div class="quiz-card">
        <h3>üìö Books & Textbooks</h3>
        <div class="resource-links">
          <a href="http://incompleteideas.net/book/the-book-2nd.html" class="resource-link" target="_blank">üìñ Sutton & Barto - Reinforcement Learning: An Introduction</a>
          <a href="https://www.deeplearningbook.org/" class="resource-link" target="_blank">üìñ Deep Learning by Ian Goodfellow et al.</a>
          <a href="https://nnfs.io/" class="resource-link" target="_blank">üìñ Neural Networks from Scratch</a>
          <a href="https://think-bayes.github.io/think-bayes/" class="resource-link" target="_blank">üìñ Think Bayes</a>
          <a href="https://web.stanford.edu/~boyd/cvxbook/" class="resource-link" target="_blank">üìñ Convex Optimization (Boyd & Vandenberghe)</a>
          <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book" class="resource-link" target="_blank">üìñ Pattern Recognition and Machine Learning (Bishop)</a>
        </div>
      </div>

      <div class="quiz-card">
        <h3>üéì Online Courses & Lectures</h3>
        <div class="resource-links">
          <a href="https://www.deeplearning.ai/" class="resource-link" target="_blank">üéì DeepLearning.AI Specialization</a>
          <a href="https://course.fast.ai/" class="resource-link" target="_blank">üéì Fast.ai Practical Deep Learning</a>
          <a href="https://www.davidsilver.uk/teaching/" class="resource-link" target="_blank">üé• David Silver's RL Course</a>
          <a href="https://spinningup.openai.com/" class="resource-link" target="_blank">üöÄ OpenAI Spinning Up</a>
          <a href="https://cs231n.github.io/" class="resource-link" target="_blank">üéì CS231n Stanford (CNNs)</a>
          <a href="https://web.stanford.edu/class/cs229/" class="resource-link" target="_blank">üéì Stanford CS229 (Machine Learning)</a>
          <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction" class="resource-link" target="_blank">ü§ó HuggingFace Deep RL Course</a>
          <a href="https://www.khanacademy.org/math/statistics-probability" class="resource-link" target="_blank">üìä Khan Academy (Math Foundations)</a>
          <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" class="resource-link" target="_blank">üé• 3Blue1Brown (Linear Algebra)</a>
        </div>
      </div>

      <div class="quiz-card">
        <h3>üìù Key Papers & Articles</h3>
        <div class="resource-links">
          <a href="https://arxiv.org/" class="resource-link" target="_blank">üî¨ ArXiv (Preprint server)</a>
          <a href="https://paperswithcode.com/" class="resource-link" target="_blank">üìä Papers with Code</a>
          <a href="https://www.assemblyai.com/blog/rl-newsletter/" class="resource-link" target="_blank">‚úâÔ∏è The Reinforcement Learning Newsletter</a>
          <a href="https://arxiv.org/abs/1312.5602" class="resource-link" target="_blank">üìÑ DQN Paper</a>
          <a href="https://arxiv.org/abs/1707.06347" class="resource-link" target="_blank">üìÑ PPO Paper</a>
          <a href="https://arxiv.org/abs/1312.6114" class="resource-link" target="_blank">üìÑ Auto-Encoding Variational Bayes (VAE)</a>
          <a href="https://arxiv.org/abs/1911.05722" class="resource-link" target="_blank">üìÑ CURL: Contrastive Unsupervised Representations for RL</a>
          <a href="https://arxiv.org/abs/2306.05483" class="resource-link" target="_blank">üìÑ Generalization via Adversarial Regularization</a>
          <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" class="resource-link" target="_blank">üìù Lil'Log: Policy Gradient Algorithms</a>
          <a href="https://lilianweng.github.io/posts/2021-01-29-self-supervised-learning/" class="resource-link" target="_blank">üìù Lil'Log: Self-Supervised Learning</a>
        </div>
      </div>

      <div class="quiz-card">
        <h3>üéÆ Environments & Benchmarks</h3>
        <div class="resource-links">
          <a href="https://gym.openai.com/" class="resource-link" target="_blank">üèãÔ∏è OpenAI Gym / Gymnasium</a>
          <a href="https://github.com/openai/procgen" class="resource-link" target="_blank">üéÆ OpenAI Procgen</a>
          <a href="https://github.com/danijar/crafter" class="resource-link" target="_blank">üéÆ Crafter Environment</a>
          <a href="https://github.com/deepmind/dm_control" class="resource-link" target="_blank">ü§ñ DeepMind Control Suite</a>
          <a href="https://www.kaggle.com/competitions?sortBy=date&search=reinforcement+learning" class="resource-link" target="_blank">üèÖ Kaggle RL Competitions</a>
        </div>
      </div>
    </div>
  </div>

  <div id="checklist" class="content-section">
    <div class="progress-tracker">
      <h2>üìã Learning Checklist</h2>
      <p>Mark topics as complete to track your overall progress across all learning areas.</p>
      <div class="progress-bar">
        <div class="progress-fill" id="checklistProgressFill"></div>
      </div>
      <p id="checklistProgressText">0% Complete</p>
      <div class="checklist-items-container">
      </div>
    </div>
  </div>

  <div class="floating-progress">
    <span id="progress-text">0%</span>
  </div>
</div>

<nav class="floating-nav">
  <a href="#overview" class="nav-item">Overview</a>
  <a href="#phase1-math" class="nav-item">Phase 1</a>
  <a href="#phase2-ml-dl" class="nav-item">Phase 2</a>
  <a href="#phase3-rl" class="nav-item">Phase 3</a>
  <a href="#phase4-research" class="nav-item">Phase 4</a>
  <a href="#tools" class="nav-item">Tools</a>
  <a href="#resources" class="nav-item">Resources</a>
  <a href="#checklist" class="nav-item">Checklist</a>
</nav>

<div class="study-timer-container">
  <div class="timer-title">Study Timer</div>
  <div id="timerDisplay">00:00:00</div>
  <button id="timerBtn">Start</button>
  <button id="resetBtn">Reset</button>
</div>


<script>
  // Data for subtopic summaries
  const subtopicSummaries = {
    "distributions": "In probability, distributions describe the probabilities of different outcomes in a sample space, e.g., Gaussian (normal), Bernoulli, Poisson, etc.",
    "expectation-variance": "Expectation is the weighted average of possible outcomes (mean), while variance measures how far values in a data set are from the mean (spread).",
    "bayes-rule": "Bayes' Rule relates conditional probabilities. It's fundamental for updating beliefs about a hypothesis ($H$) given new evidence ($E$): $P(H|E) = [P(E|H) \\cdot P(H)] / P(E)$.",
    "kl-divergence": "Kullback-Leibler (KL) Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. It's often used in RL for measuring policy changes.",

    "vectors-matrices": "Vectors are ordered lists of numbers (1D arrays), representing points or directions. Matrices are 2D arrays of numbers, used for linear transformations and representing data.",
    "eigenvalues": "Eigenvalues and eigenvectors characterize linear transformations. An eigenvector's direction isn't changed by the transformation, only scaled by its eigenvalue.",
    "svd-pca": "Singular Value Decomposition (SVD) is a factorization of a matrix. Principal Component Analysis (PCA) is a dimensionality reduction technique based on SVD, finding orthogonal components that capture most variance.",

    "derivatives": "Derivatives measure the rate at which a function's output changes with respect to a change in its input. They represent the slope of a tangent line to a curve.",
    "chain-rule": "The chain rule is a formula to compute the derivative of a composite function. It's crucial for backpropagation in neural networks.",
    "partial-derivatives": "Partial derivatives describe how a multi-variable function's output changes with respect to one variable, holding others constant. Used for gradients.",
    "jacobians": "The Jacobian matrix is the matrix of all first-order partial derivatives of a vector-valued function. It's a generalization of the gradient for vector functions.",

    "gradient-descent": "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. It moves in the direction opposite to the gradient of the function.",
    "sgd-adam": "Stochastic Gradient Descent (SGD) and Adam are popular optimization algorithms (variants of gradient descent) used to train neural networks more efficiently by updating weights based on small batches of data.",
    "convex-optimization": "Convex optimization deals with minimizing convex functions over convex sets. Problems in this field have desirable properties, like any local minimum being a global minimum.",

    "supervised-learning": "Supervised learning is a machine learning paradigm where a model learns from labeled data (input-output pairs) to make predictions on new, unseen data.",
    "bias-variance": "Bias is the error from erroneous assumptions in the learning algorithm (underfitting). Variance is the error from sensitivity to small fluctuations in the training set (overfitting). The trade-off balances these.",
    "cross-validation": "Cross-validation is a technique to assess how the results of a statistical analysis will generalize to an independent dataset. It involves partitioning the dataset into subsets for training and validation, cycling through which subset is used for validation.",
    "ml-regularization": "Regularization techniques (e.g., L1, L2) are used in machine learning to prevent overfitting by adding a penalty to the loss function based on the magnitude of the model's weights, encouraging simpler models.",

    "nn-basics": "Neural Networks are computational models inspired by biological neural networks. They consist of layers of interconnected nodes (neurons) that process information through activation functions.",
    "backpropagation": "Backpropagation is the algorithm used to efficiently calculate the gradients of the loss function with respect to the weights of a neural network, enabling weight updates during training via gradient descent.",
    "cnns-rnns": "Convolutional Neural Networks (CNNs) are specialized for processing grid-like data (e.g., images) using convolutional filters. Recurrent Neural Networks (RNNs) are designed for sequential data (e.g., text, time series) by maintaining an internal state.",
    "transformers-intro": "Transformers are a neural network architecture, predominantly used in natural language processing (NLP), that rely on self-attention mechanisms to weigh the importance of different parts of the input sequence, allowing for parallel processing of sequences.",

    "autoencoders": "Autoencoders are neural networks trained to reconstruct their input. They learn a compressed, lower-dimensional representation (encoding) of the input data in the hidden layer, useful for dimensionality reduction and feature learning.",
    "vaes": "Variational Autoencoders (VAEs) are generative models that learn a probabilistic mapping from input to a latent space, allowing for sampling new, similar data by drawing from this learned distribution.",
    "contrastive-learning": "Contrastive learning is a self-supervised learning approach where the model learns representations by pulling similar (positive) samples closer and pushing dissimilar (negative) samples farther apart in the embedding space.",
    "self-supervised-learning": "Self-supervised learning is a technique where the model generates its own labels from the input data (e.g., predicting missing parts, ordering shuffled data) to learn meaningful representations without human-annotated labels, often serving as a pre-training step.",

    "mdps": "Markov Decision Processes (MDPs) are a mathematical framework for modeling sequential decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. Key components are states, actions, transitions, and rewards.",
    "value-functions": "Value functions estimate how good it is for an agent to be in a given state ($V^\\pi$) or to perform a given action in a given state ($Q^\\pi$), in terms of expected cumulative future rewards, under a given policy $\\pi$.",
    "bellman-equations": "Bellman equations define relationships between the value of a state (or state-action pair) and the values of its successor states under a given policy, forming the fundamental recursive relationships for solving MDPs.",
    "value-iteration": "Value Iteration is an algorithm that iteratively updates the estimated value function ($V$ or $Q$) until it converges to the optimal value function, from which an optimal policy can be directly derived.",
    "policy-iteration": "Policy Iteration is an iterative algorithm that alternates between two steps: Policy Evaluation (calculating the value function for the current policy) and Policy Improvement (updating the policy to be greedy with respect to the evaluated value function).",

    "q-learning-deep": "Deep Q-learning applies Q-learning updates to neural networks as function approximators, enabling it to handle large or continuous state spaces where tabular Q-learning is impractical.",
    "dqn": "Deep Q-Network (DQN) is a landmark Deep RL algorithm that uses a neural network to approximate the Q-value function. It incorporates key improvements like experience replay and a target network for stability.",
    "experience-replay": "Experience Replay is a technique used in Deep RL where past experiences (state, action, reward, next state) are stored in a buffer and randomly sampled during training. This breaks correlations in the data and improves sample efficiency.",
    "target-networks": "Target Networks are separate, periodically updated copies of the main Q-network. They provide stable Q-targets for the learning algorithm, reducing oscillation and improving training stability in DQN and similar methods.",
    "double-dueling-dqn": "Double DQN addresses the overestimation of Q-values by decoupling the selection of the next action from its evaluation. Dueling DQN is an architecture that separates the estimation of state-value function and advantage function, improving performance and stability.",

    "policy-gradients": "Policy Gradient methods directly optimize a parameterized policy by computing the gradient of the expected return with respect to the policy parameters and performing gradient ascent, aiming to increase the probability of actions that lead to higher rewards.",
    "actor-critic": "Actor-Critic methods combine the strengths of policy gradient (the 'actor' learns the policy) and value-based methods (the 'critic' learns a value function to critique the actor's actions). This often leads to more stable and efficient learning than pure policy gradients.",
    "a2c-a3c": "A2C (Advantage Actor-Critic) and A3C (Asynchronous Advantage Actor-Critic) are popular actor-critic algorithms. A3C uses asynchronous parallel agents for training stability and efficiency, while A2C is its synchronous counterpart.",
    "ppo": "Proximal Policy Optimization (PPO) is a family of policy gradient methods that optimize a 'clipped' surrogate objective function, leading to stable and efficient policy updates with fewer hyperparameter tunes. It's widely used due to its simplicity and robust performance.",
    "sac": "Soft Actor-Critic (SAC) is an off-policy actor-critic algorithm that aims to maximize expected return while also maximizing policy entropy. This dual objective promotes both reward maximization and effective exploration and robustness.",

    "environment-randomization": "Environment Randomization (or Domain Randomization) is a technique to improve generalization by training an agent in a simulator where certain aspects of the environment (e.g., textures, object positions, physics parameters) are varied randomly. This forces the agent to learn robust policies that transfer to new unseen variations in the real world or new simulated environments.",
    "procgen-benchmark": "Procgen Benchmark is a set of 16 procedurally generated reinforcement learning environments designed to test how well agents generalize to unseen environments. Unlike standard benchmarks, agents must adapt to new level layouts, object positions, etc., pushing the boundaries of generalization research.",
    "crafter-environment": "Crafter is a 2D open-world survival game environment designed to evaluate an agent's ability to learn and generalize across a diverse set of tasks and skills in a complex, procedurally generated world. It offers a broad spectrum of objectives and interactions.",
    "data-augmentation-rl": "Data Augmentation in RL refers to techniques used to increase the diversity of training data (observations, transitions) without collecting new experiences. This can involve applying transformations to images (e.g., cropping, rotating), adding noise, or re-sampling, to improve robustness and generalization by exposing the agent to varied inputs.",
    "policy-embeddings-transfer": "Policy Embeddings involve learning a low-dimensional representation for different policies or tasks. These embeddings can facilitate transfer learning by identifying similar policies for new tasks, or by using the embedding space to guide the exploration and learning process in novel environments.",

    "paper-reading": "The process of critically analyzing and understanding research papers. This involves identifying the problem addressed, the proposed solution, the methodology, experimental setup, key results, and limitations. It's crucial for staying updated and informing new research.",
    "algorithm-implementation": "Translating an algorithm's mathematical or pseudo-code description into working code. This often involves careful attention to details, handling edge cases, ensuring computational efficiency, and debugging.",
    "ablation-studies": "Experiments where a component or feature of a system (e.g., a specific loss term, a network module) is systematically removed or altered to understand its isolated contribution to the overall performance or behavior. Used to analyze the importance of different parts of an algorithm.",
    "benchmarking-hyperparameter-tuning": "Benchmarking involves evaluating an algorithm's performance on standard tasks and comparing it against established baselines or other methods. Hyperparameter tuning is the process of finding the optimal configuration of parameters (e.g., learning rate, network size) that maximize performance.",

    "define-project-scope": "Clearly outlining the goals, objectives, deliverables, and constraints of a project before starting implementation. This helps in managing expectations, staying focused, and breaking down a large problem into manageable parts.",
    "custom-environment-design": "Creating a new simulation or game environment for an RL agent to interact with. This involves defining the state space, action space, reward function, and transition dynamics that dictate how the agent perceives and interacts with the world.",
    "generalization-analysis-new-tasks": "Evaluating how well a trained RL agent performs on new, unseen tasks, environments, or variations of the original task that were not part of its training data. This assesses the agent's ability to generalize beyond its specific training experiences.",
    "policy-clustering-visualization": "Techniques used to group similar learned policies together (clustering) and visually represent them, often using dimensionality reduction methods like t-SNE or UMAP. This helps in understanding the diversity of behaviors an agent can learn or how different training runs lead to different policies.",
    "communication-results": "Effecively presenting and explaining research findings through various mediums such as written reports, academic papers, blog posts, presentations, or video demonstrations. The goal is to clearly convey complex technical information and its implications to different audiences."
  };

  // Checklist data (Expanded to include all detailed topics for comprehensive tracking)
  const checklistItems = [
    // Phase 1: Mathematics Foundations
    "Probability & Statistics: Distributions",
    "Probability & Statistics: Expectation & Variance",
    "Probability & Statistics: Bayes Rule",
    "Probability & Statistics: KL Divergence",
    "Linear Algebra: Vectors & Matrices",
    "Linear Algebra: Eigenvalues",
    "Linear Algebra: SVD & PCA",
    "Calculus: Derivatives",
    "Calculus: Chain Rule",
    "Calculus: Partial Derivatives",
    "Calculus: Jacobians",
    "Optimization: Gradient Descent",
    "Optimization: SGD & Adam",
    "Optimization: Convex Optimization",

    // Phase 2: Machine Learning & Deep Learning
    "ML Fundamentals: Supervised Learning",
    "ML Fundamentals: Bias-Variance",
    "ML Fundamentals: Cross-validation",
    "ML Fundamentals: Regularization",
    "Deep Learning: Neural Network Basics",
    "Deep Learning: Backpropagation",
    "Deep Learning: CNNs & RNNs",
    "Deep Learning: Transformers Intro",
    "Representation Learning: Autoencoders",
    "Representation Learning: Variational Autoencoders (VAEs)",
    "Representation Learning: Contrastive Learning",
    "Representation Learning: Self-supervised Learning",

    // Phase 3: Reinforcement Learning
    "Core RL: Markov Decision Processes (MDPs)",
    "Core RL: Value Functions ($V^\\pi, Q^\\pi$)",
    "Core RL: Bellman Equations",
    "Core RL: Value Iteration",
    "Core RL: Policy Iteration",
    "Deep RL: Q-learning (deep)",
    "Deep RL: Deep Q-Networks (DQN)",
    "Deep RL: Experience Replay",
    "Deep RL: Target Networks",
    "Deep RL: Double DQN, Dueling DQN",
    "Policy Gradient: REINFORCE",
    "Policy Gradient: Actor-Critic Methods",
    "Policy Gradient: A2C/A3C",
    "Policy Gradient: Proximal Policy Optimization (PPO)",
    "Policy Gradient: Soft Actor-Critic (SAC)",
    "Generalization in RL: Environment Randomization",
    "Generalization in RL: Procgen Benchmark",
    "Generalization in RL: Crafter Environment",
    "Generalization in RL: Data Augmentation for RL",
    "Generalization in RL: Policy Embeddings / Transfer Learning",

    // Phase 4: Research & Practice
    "Research: Paper Reading & Understanding",
    "Research: Algorithm Implementation from Scratch",
    "Research: Ablation Studies",
    "Research: Benchmarking & Hyperparameter Tuning",
    "Project: Define Project Scope",
    "Project: Custom Environment Design",
    "Project: Generalization Analysis on New Tasks",
    "Project: Policy Clustering / Visualization of Embeddings",
    "Project: Effective Communication of Results"
  ];

  // Initialize checklist
  function initializeChecklist() {
    const checklistContainer = document.querySelector('#checklist .checklist-items-container');
    checklistContainer.innerHTML = ''; // Clear existing items to prevent duplicates
    checklistItems.forEach((item, index) => {
      const checklistItem = document.createElement('div');
      checklistItem.className = 'checklist-item';
      checklistItem.innerHTML = `
                    <div class="checkbox" data-index="${index}">
                        <span class="checkmark" style="display: none;">‚úì</span>
                    </div>
                    <span>${item}</span>
                `;
      checklistItem.addEventListener('click', handleChecklistItemClick); // Add event listener to the item itself
      checklistContainer.appendChild(checklistItem);
    });
  }

  // Handle checkbox clicks (delegated from checklist item click)
  function handleChecklistItemClick(event) {
    const checkbox = event.currentTarget.querySelector('.checkbox'); // Get the checkbox within the clicked item
    const checkmark = checkbox.querySelector('.checkmark');
    const isChecked = checkbox.classList.contains('checked');

    if (isChecked) {
      checkbox.classList.remove('checked');
      checkmark.style.display = 'none';
    } else {
      checkbox.classList.add('checked');
      checkmark.style.display = 'block';
    }

    updateProgress();
    saveProgress();
  }

  // Update progress bars (both overall and checklist-specific)
  function updateProgress() {
    const checkboxes = document.querySelectorAll('#checklist .checkbox');
    const checkedBoxes = document.querySelectorAll('#checklist .checkbox.checked');
    const progress = (checkboxes.length > 0) ? (checkedBoxes.length / checkboxes.length) * 100 : 0;

    // Overall Progress (header and floating)
    const overallProgressDisplay = document.getElementById('overall-progress');
    const floatingProgressText = document.getElementById('progress-text');
    const completedTopicsDisplay = document.getElementById('completed-topics');

    overallProgressDisplay.textContent = `${progress.toFixed(0)}%`;
    floatingProgressText.textContent = `${progress.toFixed(0)}%`;
    completedTopicsDisplay.textContent = checkedBoxes.length; // Count completed items from checklist

    // Roadmap Progress (roadmap tab specific)
    const roadmapProgressFill = document.getElementById('roadmapProgressFill');
    const roadmapProgressText = document.getElementById('roadmapProgressText');

    if (roadmapProgressFill) { // Check if element exists (might not be on the current tab)
      roadmapProgressFill.style.width = progress + '%';

      if (progress === 0) {
        roadmapProgressText.textContent = "0% Complete - Let's get started!";
      } else if (progress < 25) {
        roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Great start! Keep building that foundation.`;
      } else if (progress < 50) {
        roadmapProgressText.textContent = `${Math.round(progress)}% Complete - You're making solid progress!`;
      } else if (progress < 75) {
        roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Over halfway there, excellent work!`;
      } else if (progress < 100) {
        roadmapProgressText.textContent = `${Math.round(progress)}% Complete - Almost done, you're crushing it!`;
      } else {
        roadmapProgressText.textContent = "100% Complete - Congratulations! You're ready for RL research! üéâ";
      }
    }


    // Checklist Progress (checklist tab specific)
    const checklistProgressFill = document.getElementById('checklistProgressFill');
    const checklistProgressText = document.getElementById('checklistProgressText');

    if (checklistProgressFill) { // Check if element exists
      checklistProgressFill.style.width = progress + '%';
      checklistProgressText.textContent = `${Math.round(progress)}% Complete`;
    }
  }

  // Save progress to localStorage
  function saveProgress() {
    const checkedItems = [];
    document.querySelectorAll('#checklist .checkbox.checked').forEach(checkbox => {
      checkedItems.push(parseInt(checkbox.dataset.index));
    });
    localStorage.setItem('rl_roadmap_progress_checklist', JSON.stringify(checkedItems)); // Unique key
  }

  // Load progress from localStorage
  function loadProgress() {
    const saved = localStorage.getItem('rl_roadmap_progress_checklist');
    if (saved) {
      const checkedItems = JSON.parse(saved);
      checkedItems.forEach(index => {
        const checkbox = document.querySelector(`#checklist [data-index="${index}"]`);
        if (checkbox) {
          checkbox.classList.add('checked');
          checkbox.querySelector('.checkmark').style.display = 'block';
        }
      });
    }
    updateProgress(); // Ensure progress is updated after loading
  }

  // --- Tab Switching (from Deep RL Mastery) ---
  function showSection(sectionId) {
    const sections = document.querySelectorAll('.content-section');
    sections.forEach(section => section.classList.remove('active'));

    const tabs = document.querySelectorAll('.tab');
    tabs.forEach(tab => tab.classList.remove('active'));

    const selectedSection = document.getElementById(sectionId);
    selectedSection.classList.add('active');

    // Find the tab that matches the sectionId and activate it
    const selectedTab = Array.from(document.querySelectorAll('.tab')).find(tab => {
      const tabText = tab.textContent.trim().toLowerCase();
      const targetText = sectionId.toLowerCase();
      // Handle cases where tab text is longer (e.g., "Overview" vs "overview")
      return tabText.includes(targetText);
    });
    if (selectedTab) {
      selectedTab.classList.add('active');
    }
    // If switching to checklist or roadmap, ensure progress bars are visible
    if (sectionId === 'checklist' || sectionId === 'roadmap') {
      updateProgress();
    }
    // Trigger animation on elements within the newly active section
    animateSectionElements(selectedSection);
  }

  // --- Phase Toggling (adapted for new structure) ---
  function togglePhase(headerElement) {
    const phase = headerElement.closest('.phase');
    phase.classList.toggle('expanded');
  }

  // --- Subtopic Summary Functionality ---
  let currentOpenSummaryElement = null; // To track the currently open summary
  let currentActiveSubtopicButton = null; // To track the currently active button

  function showSubtopicSummary(buttonElement) {
    const topicId = buttonElement.dataset.topicId;
    const summaryText = subtopicSummaries[topicId];
    const weekSection = buttonElement.closest('.week-section');
    // Ensure each week-section has its own summary display area
    let summaryDisplayElement = weekSection.querySelector('.subtopic-summary');
    if (!summaryDisplayElement) {
      summaryDisplayElement = document.createElement('div');
      summaryDisplayElement.classList.add('subtopic-summary');
      weekSection.appendChild(summaryDisplayElement);
    }

    // Deactivate all other active subtopic buttons globally
    if (currentActiveSubtopicButton && currentActiveSubtopicButton !== buttonElement) {
      currentActiveSubtopicButton.classList.remove('active-summary');
    }
    // Close any currently open summary if it's different
    if (currentOpenSummaryElement && currentOpenSummaryElement !== summaryDisplayElement) {
      currentOpenSummaryElement.classList.remove('active');
      currentOpenSummaryElement.innerHTML = '';
    }


    // Toggle active class on the clicked button
    buttonElement.classList.toggle('active-summary');

    if (summaryText && buttonElement.classList.contains('active-summary')) {
      summaryDisplayElement.innerHTML = `<p>${summaryText}</p>`;
      summaryDisplayElement.classList.add('active');
      currentOpenSummaryElement = summaryDisplayElement; // Set current open summary
      currentActiveSubtopicButton = buttonElement; // Set current active button
    } else {
      summaryDisplayElement.classList.remove('active');
      summaryDisplayElement.innerHTML = '';
      currentOpenSummaryElement = null;
      currentActiveSubtopicButton = null;
    }

    // Re-render MathJax if present in the new content
    if (window.MathJax) {
      MathJax.typesetPromise([summaryDisplayElement]).then(() => {
        // typeset is complete
      }).catch((err) => console.log('MathJax typesetting failed: ' + err.message));
    }
  }


  // --- Quiz Functionality (from Deep RL Mastery) ---
  function selectOption(optionElement, isCorrect) {
    const quizCard = optionElement.closest('.quiz-card');
    const options = quizCard.querySelectorAll('.quiz-option');
    options.forEach(opt => {
      opt.classList.remove('selected', 'correct', 'incorrect');
    });

    optionElement.classList.add('selected');

    const feedback = quizCard.querySelector('.quiz-feedback');
    feedback.style.display = 'block';

    if (isCorrect) {
      optionElement.classList.add('correct');
      feedback.classList.add('correct');
      feedback.textContent = 'Correct! ' + getCorrectFeedback(quizCard.querySelector('.quiz-question').textContent.trim());
    } else {
      optionElement.classList.add('incorrect');
      feedback.classList.add('incorrect');
      feedback.textContent = 'Incorrect. Try again. ' + getCorrectFeedback(quizCard.querySelector('.quiz-question').textContent.trim());
    }

    // Re-render MathJax if present in the new content
    if (window.MathJax) {
      MathJax.typesetPromise([feedback]).then(() => {
        // typeset is complete
      }).catch((err) => console.log('MathJax typesetting failed: ' + err.message));
    }
  }

  // Helper function to provide specific feedback for correct answers
  function getCorrectFeedback(question) {
    const trimmedQuestion = question.replace(/^\d+\.\s*\(([^)]+)\)\s*|\d+\.\s*/, '').trim();

    switch (trimmedQuestion) {
            // Math Foundations
      case 'Which of the following best describes the union of two sets A and B?':
        return 'The union of two sets A and B, denoted $A \\cup B$, is the set containing all elements that are in A, or in B, or in both.';
      case 'If P(A) = 0.5, P(B) = 0.4, and A and B are independent events, what is P(A and B)?':
        return 'For independent events, $P(A \\text{ and } B) = P(A) \\cdot P(B) = 0.2$.';
      case 'What is the result of multiplying a 2x3 matrix by a 3x2 matrix?':
        return 'When multiplying matrices $A_{m \\times n}$ and $B_{n \\times p}$, the resulting matrix $C$ will have dimensions $m \\times p$. So, a 2x3 matrix times a 3x2 matrix results in a 2x2 matrix.';
      case 'What is the derivative of $f(x) = x^2 + 3x - 5$?':
        return 'Using the power rule for derivatives, the derivative of $x^n$ is $nx^{n-1}$. So, the derivative of $x^2$ is $2x$, the derivative of $3x$ is $3$, and the derivative of a constant $-5$ is $0$. Thus, $f\'(x) = 2x + 3$.';
      case 'Gradient Descent aims to find the minimum of a function by iteratively moving in which direction?':
        return 'Gradient descent moves in the direction opposite to the gradient because the gradient points towards the steepest ascent, and we want to find the minimum.';

            // Deep Learning Foundations
      case 'Which of the following is NOT a common activation function used in neural networks?':
        return 'Fourier Transform is a mathematical operation used for signal processing, not a typical activation function in neural networks. ReLU, Sigmoid, and Tanh are common activation functions.';
      case 'What is the primary role of backpropagation in neural network training?':
        return 'Backpropagation is an algorithm used to efficiently calculate the gradients of the loss function with respect to the network\'s weights. These gradients are then used by optimization algorithms (like gradient descent) to update the weights.';
      case 'Which technique helps prevent overfitting by randomly setting a fraction of neurons to zero during training?':
        return 'Dropout is a regularization technique where, during training, randomly selected neurons are ignored (or "dropped out") along with their connections. This prevents neurons from co-adapting too much and forces the network to learn more robust features.';

            // Core RL
      case 'What is the Bellman equation used for in reinforcement learning?':
        return 'The Bellman equation expresses the recursive relationship between the value of a state and the values of its successor states, allowing us to break down a complex problem into simpler subproblems.';
      case 'What is the main difference between Q-learning and SARSA?':
        return 'Q-learning is off-policy because it learns the optimal Q-value based on the greedy action in the next state, irrespective of the current policy being followed. SARSA is on-policy because it learns based on the action actually taken in the next state according to the current policy.';
      case 'What problem does the Deep Q-Network (DQN) address compared to traditional Q-learning?':
        return 'Traditional Q-learning uses tabular methods, which are impractical for large or continuous state spaces. DQN uses deep neural networks as function approximators to generalize across a high-dimensional state space.';
      case 'In the context of policy gradient methods, what does the gradient ascent update?':
        return 'Policy gradient methods directly optimize the policy parameters by using gradient ascent to maximize the expected cumulative reward, without necessarily learning a value function first (though actor-critic methods combine both).';
      case 'What is the purpose of experience replay in DQN?':
        return 'Experience replay stores past experiences and samples them randomly for training. This breaks the correlation between consecutive samples (which can be problematic for neural network training) and allows for better sample efficiency by reusing data.';

            // Representation & Generalization
      case 'What is the main goal of contrastive learning in the context of state representation?':
        return 'In contrastive learning for state representation, the goal is to learn an embedding space where representations of "similar" states (e.g., temporally close, or semantically related) are pulled closer together, while "dissimilar" states are pushed further apart.';
      case 'Unlike traditional RL which estimates expected returns, what does distributional RL estimate?':
        return 'Distributional RL aims to estimate the full distribution of returns (e.g., a histogram or a set of atoms representing the probability of different return values) rather than just a single expected value. This provides a richer understanding of the uncertainty in returns.';
      case 'What is the primary characteristic of offline (or batch) reinforcement learning?':
        return 'Offline RL learns a policy solely from a fixed, pre-collected dataset of environmental interactions, without any further online interaction with the environment during training. This is crucial for applications where online interaction is costly or dangerous.';

            // Projects & Portfolio
      case 'When implementing DQN, why is a target network often used?':
        return 'A target network in DQN helps stabilize training by providing a fixed, less frequently updated Q-value target. This reduces the instability caused by using the same network for both predicting Q-values and generating targets, which would lead to constantly chasing a moving target.';
      case 'Which dimensionality reduction technique is commonly used to visualize high-dimensional embeddings in 2D or 3D?':
        return 't-SNE (t-Distributed Stochastic Neighbor Embedding) is particularly popular for visualizing high-dimensional data, including embeddings, by projecting them into a lower-dimensional space (typically 2D or 3D) while trying to preserve local neighborhoods.';
      default:
        return '';
    }
  }

  // --- Mark Complete (from Deep RL Mastery - adjusted for new topic structure, but mainly checklist will be used) ---
  function markComplete(buttonElement) {
    const weekSection = buttonElement.closest('.week-section');
    if (!weekSection) return;

    weekSection.style.opacity = 0.6;
    buttonElement.disabled = true;
    buttonElement.textContent = 'Week Completed!';
    buttonElement.style.background = '#4CAF50';
  }


  // Study Timer variables and functions
  let studyTime = 0;
  let timerInterval = null;
  let isStudying = false;

  function addStudyTimer() {
    const timerContainer = document.querySelector('.study-timer-container');
    if (!timerContainer) {
      console.error("Study timer container not found. Make sure it's in the HTML.");
      return;
    }

    const timerDisplay = document.getElementById('timerDisplay');
    const timerBtn = document.getElementById('timerBtn');
    const resetBtn = document.getElementById('resetBtn');

    function updateTimerDisplay() {
      const hours = Math.floor(studyTime / 3600);
      const minutes = Math.floor((studyTime % 3600) / 60);
      const seconds = studyTime % 60;
      timerDisplay.textContent = `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }

    timerBtn.onclick = function() {
      if (isStudying) {
        clearInterval(timerInterval);
        timerBtn.textContent = 'Start';
        isStudying = false;
        localStorage.setItem('study_timer_time', studyTime.toString());
      } else {
        timerInterval = setInterval(() => {
          studyTime++;
          updateTimerDisplay();
        }, 1000);
        timerBtn.textContent = 'Pause';
        isStudying = true;
      }
    };

    resetBtn.onclick = function() {
      clearInterval(timerInterval);
      timerInterval = null;
      studyTime = 0;
      updateTimerDisplay();
      timerBtn.textContent = 'Start';
      isStudying = false;
      localStorage.removeItem('study_timer_time');
    };

    const savedTime = localStorage.getItem('study_timer_time');
    if (savedTime) {
      studyTime = parseInt(savedTime, 10);
      updateTimerDisplay();
    }

    window.addEventListener('beforeunload', () => {
      if (isStudying) {
        localStorage.setItem('study_timer_time', studyTime.toString());
      }
    });
  }

  // Smooth scrolling for navigation
  function smoothScroll() {
    document.querySelectorAll('.floating-nav .nav-item, .tabs .tab').forEach(link => {
      link.addEventListener('click', function(e) {
        const href = this.getAttribute('href');
        const sectionId = this.dataset.sectionId || (href && href.startsWith('#') ? href.substring(1) : null);

        if (sectionId) {
          e.preventDefault();
          const targetElement = document.getElementById(sectionId);
          if (targetElement) {
            targetElement.scrollIntoView({
              behavior: 'smooth',
              block: 'start'
            });
          }
        }
      });
    });
  }

  // Animate elements within a section
  function animateSectionElements(section) {
    section.querySelectorAll('.phase, .week-section, .overview-card, .tools-section, .quiz-section, .progress-tracker, .competency-card, .tool-item, .checklist-item, .exercise-card').forEach(el => {
      el.style.opacity = '0';
      el.style.transform = 'translateY(20px)';
      setTimeout(() => {
        el.style.opacity = '1';
        el.style.transform = 'translateY(0)';
      }, Math.random() * 200);
    });
  }

  // Function to toggle exercise solutions
  let currentOpenSolution = null; // Track currently open solution element

  function toggleSolution(buttonElement) {
    const exerciseCard = buttonElement.closest('.exercise-card');
    const solutionElement = exerciseCard.querySelector('.exercise-solution');

    // Close any currently open solution if it's different
    if (currentOpenSolution && currentOpenSolution !== solutionElement) {
      currentOpenSolution.classList.remove('active');
      currentOpenSolution.previousElementSibling.textContent = 'Show Solution'; // Reset text of its button
    }

    // Toggle the clicked solution
    solutionElement.classList.toggle('active');
    if (solutionElement.classList.contains('active')) {
      buttonElement.textContent = 'Hide Solution';
      currentOpenSolution = solutionElement; // Set as currently open
      // Typeset MathJax when showing the solution
      if (window.MathJax) {
        MathJax.typesetPromise([solutionElement]).then(() => {
          // console.log('MathJax typeset complete for solution');
        }).catch((err) => console.log('MathJax typesetting failed for solution: ' + err.message));
      }
    } else {
      buttonElement.textContent = 'Show Solution';
      currentOpenSolution = null; // No solution is open
    }
  }


  // Initialize everything when page loads
  document.addEventListener('DOMContentLoaded', function() {
    initializeChecklist();
    loadProgress();
    smoothScroll();
    addStudyTimer();

    showSection('roadmap');

    // MathJax configuration (moved here to ensure it's available for all dynamically loaded content)
    if (window.MathJax) {
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        startup: {
          typeset: true
        }
      };
    }
  });
</script>
</body>
</html>